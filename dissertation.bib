@article{spong_energy_1996,
	title        = {Energy {Based} {Control} of a {Class} of {Underactuated} {Mechanical} {Systems}},
	author       = {Spong, Mark W.},
	year         = 1996,
	month        = jun,
	journal      = {IFAC Proceedings Volumes},
	volume       = 29,
	number       = 1,
	pages        = {2828--2832},
	doi          = {10.1016/S1474-6670(17)58105-7},
	issn         = 14746670,
	url          = {https://linkinghub.elsevier.com/retrieve/pii/S1474667017581057},
	urldate      = {2020-03-15},
	abstract     = {In this paper we discuss some design m{\textasciitilde}thod8 for the co ntrol of a class of underactuated mechanical systems, This class of systems includes gymnastic robots , such as the Acrobat , as well th e classical cart- pole system. T he systems considered here arc nonminirnum phase whi ch gre1'Ltly complic.ates the control design issues. Our design techniques combine nonlinea.:r partial feed back linearization with Lyapunov methods based on saturation functions, switchin g a.nd energy sha.ping.},
	language     = {en}
}
@inproceedings{NIPS2012_c399862d,
	title        = {ImageNet Classification with Deep Convolutional Neural Networks},
	author       = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year         = 2012,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 25,
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	editor       = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger}
}
@misc{stable-baselines,
	title        = {Stable Baselines},
	author       = {Hill, Ashley and Raffin, Antonin and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Traore, Rene and Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
	year         = 2018,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/hill-a/stable-baselines}}
}
@article{spong_swing_1994,
	title        = {Swing up control of the acrobot using partial feedback linearization *},
	author       = {Spong, Mark W.},
	year         = 1994,
	month        = sep,
	journal      = {IFAC Proceedings Volumes},
	volume       = 27,
	number       = 14,
	pages        = {833--838},
	doi          = {10.1016/S1474-6670(17)47404-0},
	issn         = 14746670,
	url          = {https://linkinghub.elsevier.com/retrieve/pii/S1474667017474040},
	urldate      = {2020-03-13},
	abstract     = {In this paper we study the swing up control problem for the Acrobot using partial feedback linearization. We give conditions under which the response of either degree of freedom may be globally decoupled from the response of the other and linearized. This result can be used as a starting point to design swing up control algorithms. Analysis of the resulting zero dynamics shows interesting and rich behavior. Simulation results are presented showing the swing up motion resulting from the partial feedback linearization design.},
	language     = {en},
	file         = {1-s2.0-S1474667017474040-main.pdf:/Users/sgillen/Textbooks/Papers/1-s2.0-S1474667017474040-main.pdf:application/pdf}
}
@article{haarnoja_soft_2018,
	title        = {Soft {Actor}-{Critic}: {Off}-{Policy} {Maximum} {Entropy} {Deep} {Reinforcement} {Learning} with a {Stochastic} {Actor}},
	shorttitle   = {Soft {Actor}-{Critic}},
	author       = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	year         = 2018,
	month        = aug,
	journal      = {arXiv:1801.01290 [cs, stat]},
	url          = {http://arxiv.org/abs/1801.01290},
	urldate      = {2020-03-13},
	note         = {arXiv: 1801.01290},
	abstract     = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
	language     = {en},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file         = {1801.01290.pdf:/Users/sgillen/Textbooks/Papers/1801.01290.pdf:application/pdf}
}
@article{wiklendt_small_2009,
	title        = {A small spiking neural network with {LQR} control applied to the acrobot},
	author       = {Wiklendt, Lukasz and Chalup, Stephan and Middleton, Rick},
	year         = 2009,
	month        = may,
	journal      = {Neural Computing and Applications},
	volume       = 18,
	number       = 4,
	pages        = {369--375},
	doi          = {10.1007/s00521-008-0187-1},
	issn         = {0941-0643, 1433-3058},
	url          = {http://link.springer.com/10.1007/s00521-008-0187-1},
	urldate      = {2020-03-13},
	abstract     = {This paper presents the results of a computer simulation which, combined a small network of spiking neurons with linear quadratic regulator (LQR) control to solve the acrobot swing-up and balance task. To our knowledge, this task has not been previously solved with spiking neural networks. Input to the network was drawn from the state of the acrobot, and output was torque, either directly applied to the actuated joint, or via the switching of an LQR controller designed for balance. The neural network’s weights were tuned using a (l + k)-evolution strategy without recombination, and neurons’ parameters, were chosen to roughly approximate biological neurons.},
	language     = {en},
	file         = {10.1.1.160.330.pdf:/Users/sgillen/Textbooks/Papers/10.1.1.160.330.pdf:application/pdf}
}
@article{barth-maron_distributed_2018,
	title        = {Distributed {Distributional} {Deterministic} {Policy} {Gradients}},
	author       = {Barth-Maron, Gabriel and Hoffman, Matthew W. and Budden, David and Dabney, Will and Horgan, Dan and TB, Dhruva and Muldal, Alistair and Heess, Nicolas and Lillicrap, Timothy},
	year         = 2018,
	month        = apr,
	journal      = {arXiv:1804.08617 [cs, stat]},
	url          = {http://arxiv.org/abs/1804.08617},
	urldate      = {2020-03-13},
	note         = {arXiv: 1804.08617},
	abstract     = {This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of N -step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difﬁcult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.},
	language     = {en},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file         = {1804.08617.pdf:/Users/sgillen/Downloads/1804.08617.pdf:application/pdf}
}
@misc{garage,
	title        = {Garage: A toolkit for reproducible reinforcement learning research},
	author       = {The garage contributors},
	year         = 2019,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/rlworkgroup/garage}},
	commit       = {be070842071f736eb24f28e4b902a9f144f5c97b}
}
@misc{coumans2019,
	title        = {PyBullet, a Python module for physics simulation for games, robotics and machine learning},
	author       = {Erwin Coumans and Yunfei Bai},
	year         = {2016--2019},
	howpublished = {\url{http://pybullet.org}}
}
@misc{1606.01540,
	title        = {OpenAI Gym},
	author       = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
	year         = 2016,
	eprint       = {arXiv:1606.01540}
}
@techreport{deepmindcontrolsuite2018,
	title        = {Deep{Mind} Control Suite},
	author       = {Yuval Tassa and Yotam Doron and Alistair Muldal and Tom Erez and Yazhe Li and Diego de Las Casas and David Budden and Abbas Abdolmaleki and Josh Merel and Andrew Lefrancq and Timothy Lillicrap and Martin Riedmiller},
	year         = 2018,
	month        = jan,
	volume       = {abs/1504.04804},
	url          = {https://arxiv.org/abs/1801.00690},
	howpublished = {https://arxiv.org/abs/1801.00690},
	institution  = {DeepMind}
}
@article{schulman_trust_2015,
	title        = {Trust {Region} {Policy} {Optimization}},
	author       = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
	year         = 2015,
	month        = feb,
	journal      = {arXiv:1502.05477 [cs]},
	url          = {http://arxiv.org/abs/1502.05477},
	urldate      = {2019-03-01},
	note         = {arXiv: 1502.05477},
	abstract     = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justiï¬ed procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
	language     = {en},
	keywords     = {Computer Science - Machine Learning},
	file         = {Schulman et al. - 2015 - Trust Region Policy Optimization.pdf:/Users/sgillen/Zotero/storage/WB8DYQ5W/Schulman et al. - 2015 - Trust Region Policy Optimization.pdf:application/pdf}
}
@article{lillicrap_continuous_2015,
	title        = {Continuous control with deep reinforcement learning},
	author       = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	year         = 2015,
	month        = sep,
	journal      = {arXiv:1509.02971 [cs, stat]},
	url          = {http://arxiv.org/abs/1509.02971},
	urldate      = {2019-03-01},
	note         = {arXiv: 1509.02971},
	abstract     = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to ï¬nd policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies âend-to-endâ: directly from raw pixel inputs.},
	language     = {en},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file         = {Lillicrap et al. - 2015 - Continuous control with deep reinforcement learnin.pdf:/Users/sgillen/Zotero/storage/ZZREKUFP/Lillicrap et al. - 2015 - Continuous control with deep reinforcement learnin.pdf:application/pdf}
}
@article{bacon_option-critic_2016,
	title        = {The {Option}-{Critic} {Architecture}},
	author       = {Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
	year         = 2016,
	month        = sep,
	journal      = {arXiv:1609.05140 [cs]},
	url          = {http://arxiv.org/abs/1609.05140},
	urldate      = {2019-03-01},
	note         = {arXiv: 1609.05140},
	abstract     = {Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging. We tackle this problem in the framework of options [Sutton, Precup \& Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the ï¬exibility and efï¬ciency of the framework.},
	language     = {en},
	keywords     = {Computer Science - Artificial Intelligence},
	file         = {Bacon et al. - 2016 - The Option-Critic Architecture.pdf:/Users/sgillen/Zotero/storage/RCGSXZRY/Bacon et al. - 2016 - The Option-Critic Architecture.pdf:application/pdf}
}
@article{shazeer_outrageously_2017,
	title        = {Outrageously {Large} {Neural} {Networks}: {The} {Sparsely}-{Gated} {Mixture}-of-{Experts} {Layer}},
	shorttitle   = {Outrageously {Large} {Neural} {Networks}},
	author       = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
	year         = 2017,
	month        = jan,
	journal      = {arXiv:1701.06538 [cs, stat]},
	url          = {http://arxiv.org/abs/1701.06538},
	urldate      = {2019-03-01},
	note         = {arXiv: 1701.06538},
	abstract     = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are signiï¬cant algorithmic and performance challenges. In this work, we address these challenges and ï¬nally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efï¬ciency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve signiï¬cantly better results than state-of-the-art at lower computational cost.},
	language     = {en},
	keywords     = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file         = {Shazeer et al. - 2017 - Outrageously Large Neural Networks The Sparsely-G.pdf:/Users/sgillen/Zotero/storage/V6BYMJTZ/Shazeer et al. - 2017 - Outrageously Large Neural Networks The Sparsely-G.pdf:application/pdf}
}
@article{heess_emergence_2017,
	title        = {Emergence of {Locomotion} {Behaviours} in {Rich} {Environments}},
	author       = {Heess, Nicolas and TB, Dhruva and Sriram, Srinivasan and Lemmon, Jay and Merel, Josh and Wayne, Greg and Tassa, Yuval and Erez, Tom and Wang, Ziyu and Eslami, S. M. Ali and Riedmiller, Martin and Silver, David},
	year         = 2017,
	month        = jul,
	journal      = {arXiv:1707.02286 [cs]},
	url          = {http://arxiv.org/abs/1707.02286},
	urldate      = {2019-03-01},
	note         = {arXiv: 1707.02286},
	abstract     = {The reinforcement learning paradigm allows, in principle, for complex behaviours to be learned directly from simple reward signals. In practice, however, it is common to carefully hand-design the reward function to encourage a particular solution, or to derive it from demonstration data. In this paper explore how a rich environment can help to promote the learning of complex behavior. Speciï¬cally, we train agents in diverse environmental contexts, and ï¬nd that this encourages the emergence of robust behaviours that perform well across a suite of tasks. We demonstrate this principle for locomotion â behaviours that are known for their sensitivity to the choice of reward. We train several simulated bodies on a diverse set of challenging terrains and obstacles, using a simple reward function based on forward progress. Using a novel scalable variant of policy gradient reinforcement learning, our agents learn to run, jump, crouch and turn as required by the environment without explicit reward-based guidance. A visual depiction of highlights of the learned behavior can be viewed in this video.},
	language     = {en},
	keywords     = {Computer Science - Artificial Intelligence},
	file         = {Heess et al. - 2017 - Emergence of Locomotion Behaviours in Rich Environ.pdf:/Users/sgillen/Zotero/storage/SRJQ5NGU/Heess et al. - 2017 - Emergence of Locomotion Behaviours in Rich Environ.pdf:application/pdf}
}
@article{schulman_proximal_2017,
	title        = {Proximal {Policy} {Optimization} {Algorithms}},
	author       = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	year         = 2017,
	month        = jul,
	journal      = {arXiv:1707.06347 [cs]},
	url          = {http://arxiv.org/abs/1707.06347},
	urldate      = {2019-03-01},
	note         = {arXiv: 1707.06347},
	abstract     = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a âsurrogateâ objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the beneï¬ts of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	language     = {en},
	keywords     = {Computer Science - Machine Learning},
	file         = {Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:/Users/sgillen/Zotero/storage/3V433ET3/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf}
}
@article{2017Natur.550..354S,
	title        = {{Mastering the game of Go without human knowledge}},
	author       = {{Silver}, David and {Schrittwieser}, Julian and {Simonyan}, Karen and {Antonoglou}, Ioannis and {Huang}, Aja and {Guez}, Arthur and {Hubert}, Thomas and {Baker}, Lucas and {Lai}, Matthew and {Bolton}, Adrian and {Chen}, Yutian and {Lillicrap}, Timothy and {Hui}, Fan and {Sifre}, Laurent and {van den Driessche}, George and {Graepel}, Thore and {Hassabis}, Demis},
	year         = 2017,
	month        = oct,
	journal      = {\nat},
	volume       = 550,
	number       = 7676,
	pages        = {354--359},
	doi          = {10.1038/nature24270},
	adsurl       = {https://ui.adsabs.harvard.edu/abs/2017Natur.550..354S},
	adsnote      = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{anymal2022,
	title        = {Learning robust perceptive locomotion for quadrupedal robots in the wild},
	author       = {Miki, Takahiro and Lee, Joonho and Hwangbo, Jemin and Wellhausen, Lorenz and Koltun, Vladlen and Hutter, Marco},
	year         = 2022,
	month        = {Jan},
	journal      = {Science Robotics},
	publisher    = {American Association for the Advancement of Science (AAAS)},
	volume       = 7,
	number       = 62,
	doi          = {10.1126/scirobotics.abk2822},
	issn         = {2470-9476},
	url          = {http://dx.doi.org/10.1126/scirobotics.abk2822}
}
@article{muzero2020,
	title        = {Mastering Atari, Go, chess and shogi by planning with a learned model},
	author       = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
	year         = 2020,
	month        = {Dec},
	journal      = {Nature},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 588,
	number       = 7839,
	pages        = {604–609},
	doi          = {10.1038/s41586-020-03051-4},
	issn         = {1476-4687},
	url          = {http://dx.doi.org/10.1038/s41586-020-03051-4}
}
@article{openai_learning_2018,
	title        = {Learning {Dexterous} {In}-{Hand} {Manipulation}},
	author       = {OpenAI and Andrychowicz, Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and Schneider, Jonas and Sidor, Szymon and Tobin, Josh and Welinder, Peter and Weng, Lilian and Zaremba, Wojciech},
	year         = {2018f},
	month        = aug,
	journal      = {arXiv:1808.00177 [cs, stat]},
	url          = {http://arxiv.org/abs/1808.00177},
	urldate      = {2019-03-01},
	note         = {arXiv: 1808.00177},
	abstract     = {We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies which can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system like friction coefï¬cients and an objectâs appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including ï¬nger gaiting, multi-ï¬nger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five [43]. We also include a video of our results: https://youtu. be/jwSbzNHGflM.},
	language     = {en},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file         = {OpenAI et al. - 2018 - Learning Dexterous In-Hand Manipulation.pdf:/Users/sgillen/Zotero/storage/V3W3FTF9/OpenAI et al. - 2018 - Learning Dexterous In-Hand Manipulation.pdf:application/pdf}
}
@article{hwangbo_learning_2019,
	title        = {Learning agile and dynamic motor skills for legged robots},
	author       = {Hwangbo, Jemin and Lee, Joonho and Dosovitskiy, Alexey and Bellicoso, Dario and Tsounis, Vassilios and Koltun, Vladlen and Hutter, Marco},
	year         = 2019,
	month        = jan,
	journal      = {Science Robotics},
	volume       = 4,
	number       = 26,
	pages        = {eaau5872},
	doi          = {10.1126/scirobotics.aau5872},
	issn         = {2470-9476},
	url          = {http://robotics.sciencemag.org/lookup/doi/10.1126/scirobotics.aau5872},
	urldate      = {2019-03-01},
	language     = {en},
	file         = {Hwangbo et al. - 2019 - Learning agile and dynamic motor skills for legged.pdf:/Users/sgillen/Zotero/storage/QE9BMR6L/Hwangbo et al. - 2019 - Learning agile and dynamic motor skills for legged.pdf:application/pdf}
}
@article{yoshimoto_acrobot_2005,
	title        = {Acrobot control by learning the switching of multiple controllers},
	author       = {Yoshimoto, Junichiro and Nishimura, Masaya and Tokita, Yoichi and Ishii, Shin},
	year         = 2005,
	month        = may,
	journal      = {Artificial Life and Robotics},
	volume       = 9,
	number       = 2,
	pages        = {67--71},
	doi          = {10.1007/s10015-004-0340-6},
	issn         = {1433-5298, 1614-7456},
	url          = {http://link.springer.com/10.1007/s10015-004-0340-6},
	urldate      = {2019-03-01},
	abstract     = {Reinforcement learning (RL) has been applied to constructing controllers for nonlinear systems in recent years. Since RL methods do not require an exact dynamics model of the controlled object, they have a higher ï¬exibility and potential for adaptation to uncertain or nonstationary environments than methods based on traditional control theory. If the target system has a continuous state space whose dynamic characteristics are nonlinear, however, RL methods often suffer from unstable learning processes. For this reason, it is difï¬cult to apply RL methods to control tasks in the real world. In order to overcome the disadvantage of RL methods, we propose an RL scheme combining multiple controllers, each of which is constructed based on traditional control theory. We then apply it to a swinging-up and stabilizing task of an acrobot with a limited torque, which is a typical but difï¬cult task in the ï¬eld of nonlinear control theory. Our simulation result showed that our method was able to realize stable learning and to achieve fairly good control.},
	language     = {en},
	file         = {Yoshimoto et al. - 2005 - Acrobot control by learning the switching of multi.pdf:/Users/sgillen/Textbooks/Papers/Yoshimoto et al. - 2005 - Acrobot control by learning the switching of multi.pdf:application/pdf}
}
@inproceedings{biro_double_2010,
	title        = {Double inverted pendulum control by linear quadratic regulator and reinforcement learning},
	author       = {Biro, Sandor and Precup, Radu-Emil and Todinca, Doru},
	year         = 2010,
	booktitle    = {2010 {International} {Joint} {Conference} on {Computational} {Cybernetics} and {Technical} {Informatics}},
	publisher    = {IEEE},
	address      = {Timisoara, Romania},
	pages        = {159--164},
	doi          = {10.1109/ICCCYB.2010.5491309},
	isbn         = {978-1-4244-7432-5},
	url          = {http://ieeexplore.ieee.org/document/5491309/},
	urldate      = {2019-03-01},
	abstract     = {The paper gives an original combination of linear quadratic regulator and reinforcement learning dedicated to the position control of a double inverted pendulum system. An agent based on a modified Sarsa algorithm is applied to swing up the pendulum. The linear quadratic regulator is applied to the linearized mathematical model of the process in the vicinity of upright position. Digital simulation results show the performance of the new approach.},
	language     = {en},
	file         = {Biro et al. - 2010 - Double inverted pendulum control by linear quadrat.pdf:/Users/sgillen/Textbooks/Papers/Biro et al. - 2010 - Double inverted pendulum control by linear quadrat.pdf:application/pdf}
}
@article{abramova_combining_nodate,
	title        = {Combining {Reinforcement} {Learning} {And} {Optimal} {Control} {For} {The} {Control} {Of} {Nonlinear} {Dynamical} {Systems}},
	author       = {Abramova, Ekaterina},
	pages        = 182,
	language     = {en},
	file         = {Abramova - Combining Reinforcement Learning And Optimal Contr.pdf:/Users/sgillen/Textbooks/Papers/Abramova - Combining Reinforcement Learning And Optimal Contr.pdf:application/pdf}
}
@article{leonessa_nonlinear_2001,
	title        = {Nonlinear system stabilization via hierarchical switching control},
	author       = {Leonessa, A. and Haddad, W.M. and Chellaboina, V.S.},
	year         = 2001,
	month        = jan,
	journal      = {IEEE Transactions on Automatic Control},
	volume       = 46,
	number       = 1,
	pages        = {17--28},
	doi          = {10.1109/9.898692},
	issn         = {00189286},
	url          = {http://ieeexplore.ieee.org/document/898692/},
	urldate      = {2019-08-01},
	abstract     = {In this paper, a nonlinear control-system design framework predicated on a hierarchical switching controller architecture parameterized over a set of moving system equilibria is developed. Specifically, using equilibria-dependent Lyapunov functions, a hierarchical nonlinear control strategy is developed that stabilizes a given nonlinear system by stabilizing a collection of nonlinear controlled subsystems. The switching nonlinear controller architecture is designed based on a generalized lower semicontinuous Lyapunov function obtained by minimizing a potential function over a given switching set induced by the parameterized system equilibria. The !!! proposed framework provides a rigorous alternative to designing gain-scheduled feedback controllers and guarantees local and global closed-loop system stability for general nonlinear systems.},
	language     = {en},
	file         = {00898692.pdf:/Users/sgillen/Textbooks/Papers/00898692.pdf:application/pdf}
}
@article{lee_robust_2019,
	title        = {Robust {Recovery} {Controller} for a {Quadrupedal} {Robot} using {Deep} {Reinforcement} {Learning}},
	author       = {Lee, Joonho and Hwangbo, Jemin and Hutter, Marco},
	year         = 2019,
	month        = jan,
	journal      = {arXiv:1901.07517 [cs]},
	url          = {http://arxiv.org/abs/1901.07517},
	urldate      = {2019-08-09},
	note         = {arXiv: 1901.07517},
	abstract     = {The ability to recover from a fall is an essential feature for a legged robot to navigate in challenging environments robustly. Until today, there has been very little progress on this topic. Current solutions mostly build upon (heuristically) predeï¬ned trajectories, resulting in unnatural behaviors and requiring considerable effort in engineering system-speciï¬c components. In this paper, we present an approach based on model-free Deep Reinforcement Learning (RL) to control recovery maneuvers of quadrupedal robots using a hierarchical behavior-based controller. The controller consists of four neural network policies including three behaviors and one behavior selector to coordinate them. Each of them is trained individually in simulation and deployed directly on a real system. We experimentally validate our approach on the quadrupedal robot ANYmal, which is a dog-sized quadrupedal system with 12 degrees of freedom. With our method, ANYmal manifests dynamic and reactive recovery behaviors to recover from an arbitrary fall conï¬guration within less than 5 seconds. We tested the recovery maneuver more than 100 times, and the success rate was higher than 97 \%.},
	language     = {en},
	keywords     = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
	file         = {1901.07517.pdf:/Users/sgillen/Textbooks/Papers/1901.07517.pdf:application/pdf}
}
@article{schulman_high-dimensional_2015,
	title        = {High-{Dimensional} {Continuous} {Control} {Using} {Generalized} {Advantage} {Estimation}},
	author       = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
	year         = 2015,
	month        = jun,
	journal      = {arXiv:1506.02438 [cs]},
	url          = {http://arxiv.org/abs/1506.02438},
	urldate      = {2019-09-07},
	note         = {arXiv: 1506.02438},
	abstract     = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difï¬culty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the ï¬rst challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(Î»). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks.},
	language     = {en},
	keywords     = {Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
	file         = {1506.02438.pdf:/Users/sgillen/Textbooks/Papers/1506.02438.pdf:application/pdf}
}
@article{kingma_adam:_2014,
	title        = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle   = {Adam},
	author       = {Kingma, Diederik P. and Ba, Jimmy},
	year         = 2014,
	month        = dec,
	journal      = {arXiv:1412.6980 [cs]},
	url          = {http://arxiv.org/abs/1412.6980},
	urldate      = {2019-09-08},
	note         = {arXiv: 1412.6980},
	abstract     = {We introduce Adam, an algorithm for ï¬rst-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efï¬cient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the inï¬nity norm.},
	language     = {en},
	keywords     = {Computer Science - Machine Learning},
	file         = {1412.6980.pdf:/Users/sgillen/Textbooks/Papers/1412.6980.pdf:application/pdf}
}
@article{parr_reinforcement_1998,
	title        = {Reinforcement {Learning} with {Hierarchies} of {Machines}},
	author       = {Parr, Ronald and Russell, Stuart J},
	year         = 1998,
	journal      = {NIPS},
	pages        = 7,
	abstract     = {We present a new approach to reinforcement learning in which the policies considered by the learning process are constrained by hierarchies of partially specified machines. This allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems. Our approach can be seen as providing a link between reinforcement learning and "behavior-based" or "teleo-reactive" approaches to control. We present provably convergent algorithms for problem-solving and learning with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states.},
	language     = {en},
	file         = {1384-reinforcement-learning-with-hierarchies-of-machines.pdf:/Users/sgillen/Textbooks/Papers/1384-reinforcement-learning-with-hierarchies-of-machines.pdf:application/pdf}
}
@article{mnih_asynchronous_2016,
	title        = {Asynchronous {Methods} for {Deep} {Reinforcement} {Learning}},
	author       = {Mnih, Volodymyr and Badia, AdriÃ  PuigdomÃšnech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	year         = 2016,
	month        = feb,
	journal      = {arXiv:1602.01783 [cs]},
	url          = {http://arxiv.org/abs/1602.01783},
	urldate      = {2019-09-21},
	note         = {arXiv: 1602.01783},
	abstract     = {We !! propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
	language     = {en},
	keywords     = {Computer Science - Machine Learning},
	file         = {1602.01783.pdf:/Users/sgillen/Textbooks/Papers/1602.01783.pdf:application/pdf}
}
@book{tedrake_drake:_2019,
	title        = {Drake: {Model}-based design and verification for robotics},
	author       = {Tedrake, Russ and Team, the Drake Development},
	year         = 2019,
	url          = {https://drake.mit.edu}
}
@article{williams_simple_1992,
	title        = {Simple {Statistical} {Gradient}-{Following} {Algorithms} for {Connectionist} {Reinforcement} {Learning}},
	author       = {Williams, Ronald J},
	year         = 1992,
	journal      = {Machine Learning},
	pages        = 27,
	abstract     = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Speci c examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
	language     = {en},
	file         = {10.1.1.31.2545.pdf:/Users/sgillen/Textbooks/Papers/10.1.1.31.2545.pdf:application/pdf}
}
@inproceedings{randlov_combining_2000,
	title        = {Combining {Reinforcement} {Learning} with a {Local} {Control} {Algorithm}},
	author       = {RandlÃžv, Jette and Barto, Andrew G. and Rosenstein, Michael T.},
	year         = 2000,
	booktitle    = {Proceedings of the {Seventeenth} {International} {Conference} on {Machine} {Learning}},
	publisher    = {Morgan Kaufmann Publishers Inc.},
	address      = {San Francisco, CA, USA},
	series       = {{ICML} '00},
	pages        = {775--782},
	isbn         = {1-55860-707-2},
	url          = {http://dl.acm.org/citation.cfm?id=645529.657804},
	file         = {RandlÃžv et al. - Combining Reinforcement Learning with a Local Cont.pdf:/Users/sgillen/Zotero/storage/IANDQ6VV/RandlÃžv et al. - Combining Reinforcement Learning with a Local Cont.pdf:application/pdf}
}
@incollection{dayan_feudal_1993,
	title        = {Feudal {Reinforcement} {Learning}},
	author       = {Dayan, Peter and Hinton, Geoffrey E},
	year         = 1993,
	booktitle    = {Advances in {Neural} {Information} {Processing} {Systems} 5},
	publisher    = {Morgan-Kaufmann},
	pages        = {271--278},
	url          = {http://papers.nips.cc/paper/714-feudal-reinforcement-learning.pdf},
	editor       = {Hanson, S. J. and Cowan, J. D. and Giles, C. L.},
	file         = {Dayan - Feudal Reinforcement Learning.pdf:/Users/sgillen/Zotero/storage/TAETI6QL/Dayan - Feudal Reinforcement Learning.pdf:application/pdf}
}
@article{doya_multiple_2002,
	title        = {Multiple {Model}-{Based} {Reinforcement} {Learning}},
	author       = {Doya, Kenji and Samejima, Kazuyuki and Katagiri, Ken-ichi and Kawato, Mitsuo},
	year         = 2002,
	month        = jun,
	journal      = {Neural Computation},
	volume       = 14,
	number       = 6,
	pages        = {1347--1369},
	doi          = {10.1162/089976602753712972},
	issn         = {0899-7667, 1530-888X},
	url          = {http://www.mitpressjournals.org/doi/10.1162/089976602753712972},
	urldate      = {2019-09-24},
	language     = {en},
	file         = {10.1.1.5.6184.pdf:/Users/sgillen/Textbooks/Papers/10.1.1.5.6184.pdf:application/pdf}
}
@misc{galloudec2021multigoal,
	title        = {Multi-Goal Reinforcement Learning environments for simulated Franka Emika Panda robot},
	author       = {Quentin Gallou{\'e}dec and Nicolas Cazin and Emmanuel Dellandr{\'e}a and Liming Chen},
	year         = 2021,
	eprint       = {2106.13687},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{doya_multiple_2002-1,
	title        = {Multiple {Model}-{Based} {Reinforcement} {Learning}},
	author       = {Doya, Kenji and Samejima, Kazuyuki and Katagiri, Ken-ichi and Kawato, Mitsuo},
	year         = 2002,
	month        = jun,
	journal      = {Neural Computation},
	volume       = 14,
	number       = 6,
	pages        = {1347--1369},
	doi          = {10.1162/089976602753712972},
	issn         = {0899-7667, 1530-888X},
	url          = {http://www.mitpressjournals.org/doi/10.1162/089976602753712972},
	urldate      = {2019-09-25},
	language     = {en},
	file         = {10.1.1.5.6184.pdf:/Users/sgillen/Textbooks/Papers/10.1.1.5.6184.pdf:application/pdf}
}
@misc{baselines,
	title        = {OpenAI Baselines},
	author       = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai and Zhokhov, Peter},
	year         = 2017,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/openai/baselines}}
}
@article{doya_reinforcement_2000,
	title        = {Reinforcement {Learning} in {Continuous} {Time} and {Space}},
	author       = {Doya, Kenji},
	year         = 2000,
	month        = jan,
	journal      = {Neural Computation},
	volume       = 12,
	number       = 1,
	pages        = {219--245},
	doi          = {10.1162/089976600300015961},
	issn         = {0899-7667, 1530-888X},
	url          = {http://www.mitpressjournals.org/doi/10.1162/089976600300015961},
	urldate      = {2019-09-26},
	language     = {en},
	file         = {Doya00b.pdf:/Users/sgillen/Textbooks/Papers/Doya00b.pdf:application/pdf}
}
@misc{schulman_klimov_wolski_dhariwal_radford_2017,
	title        = {openai.com},
	author       = {Schulman , John and Klimov, Oleg and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec},
	year         = 2017,
	month        = {Jul},
	journal      = {openai.com},
	publisher    = {OpenAI},
	url          = {https://openai.com/blog/openai-baselines-ppo/}
}
@article{Gillen2020CombiningDR,
	title        = {Combining Deep Reinforcement Learning And Local Control For The Acrobot Swing-up And Balance Task},
	author       = {S. Gillen and Marco Molnar and K. Byl},
	year         = 2020,
	journal      = {2020 59th IEEE Conference on Decision and Control (CDC)},
	pages        = {4129--4134}
}
@article{Gillen2020ExplicitFractal,
	title        = {Explicitly Encouraging Low Fractional Dimensional Trajectories Via Reinforcement Learning},
	author       = {S. Gillen and K. Byl},
	year         = 2020,
	journal      = {2020 4th Conference Of Robot Learning (CORL)}
}
@misc{gallouédec2021multigoal,
	title        = {Multi-Goal Reinforcement Learning environments for simulated Franka Emika Panda robot},
	author       = {Quentin Gallou{\'e}dec and Nicolas Cazin and Emmanuel Dellandr{\'e}a and Liming Chen},
	year         = 2021,
	eprint       = {2106.13687},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{benelot2018,
	title        = {PyBullet Gymperium},
	author       = {Benjamin Ellenberger},
	year         = 2019,
	howpublished = {\url{ https://github.com/benelot/pybullet-gym}}
}
@misc{salimans2017evolution,
	title        = {Evolution Strategies as a Scalable Alternative to Reinforcement Learning},
	author       = {Tim Salimans and Jonathan Ho and Xi Chen and Szymon Sidor and Ilya Sutskever},
	year         = 2017,
	eprint       = {1703.03864},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}
@misc{forestier2020intrinsically,
	title        = {Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning},
	author       = {Sébastien Forestier and Rémy Portelas and Yoan Mollard and Pierre-Yves Oudeyer},
	year         = 2020,
	eprint       = {1708.02190},
	archiveprefix = {arXiv},
	primaryclass = {cs.AI}
}
@misc{kuznetsov2020controlling,
	title        = {Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics},
	author       = {Arsenii Kuznetsov and Pavel Shvechikov and Alexander Grishin and Dmitry Vetrov},
	year         = 2020,
	eprint       = {2005.04269},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{mnih2016asynchronous,
	title        = {Asynchronous Methods for Deep Reinforcement Learning},
	author       = {Volodymyr Mnih and Adrià Puigdomènech Badia and Mehdi Mirza and Alex Graves and Timothy P. Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
	year         = 2016,
	eprint       = {1602.01783},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{fujimoto2018addressing,
	title        = {Addressing Function Approximation Error in Actor-Critic Methods},
	author       = {Scott Fujimoto and Herke van Hoof and David Meger},
	year         = 2018,
	eprint       = {1802.09477},
	archiveprefix = {arXiv},
	primaryclass = {cs.AI}
}
@inproceedings{pmlr-v80-colas18a,
	title        = {{GEP}-{PG}: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms},
	author       = {Colas, C{\'e}dric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
	year         = 2018,
	month        = jul,
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 80,
	pages        = {1039--1048},
	url          = {https://proceedings.mlr.press/v80/colas18a.html},
	editor       = {Dy, Jennifer and Krause, Andreas},
	pdf          = {http://proceedings.mlr.press/v80/colas18a/colas18a.pdf},
	abstract     = {In continuous action domains, standard deep reinforcement learning algorithms like DDPG suffer from inefficient exploration when facing sparse or deceptive reward problems. Conversely, evolutionary and developmental methods focusing on exploration like Novelty Search, Quality-Diversity or Goal Exploration Processes explore more robustly but are less efficient at fine-tuning policies using gradient-descent. In this paper, we present the GEP-PG approach, taking the best of both worlds by sequentially combining a Goal Exploration Process and two variants of DDPG . We study the learning performance of these components and their combination on a low dimensional deceptive reward problem and on the larger Half-Cheetah benchmark. We show that DDPG fails on the former and that GEP-PG improves over the best DDPG variant in both environments.}
}
@inproceedings{pmlr-v100-xie20a,
	title        = {Learning Locomotion Skills for Cassie: Iterative Design and Sim-to-Real},
	author       = {Xie, Zhaoming and Clary, Patrick and Dao, Jeremy and Morais, Pedro and Hurst, Jonanthan and van de Panne, Michiel},
	year         = 2020,
	month        = oct,
	booktitle    = {Proceedings of the Conference on Robot Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 100,
	pages        = {317--329},
	url          = {https://proceedings.mlr.press/v100/xie20a.html},
	editor       = {Kaelbling, Leslie Pack and Kragic, Danica and Sugiura, Komei},
	pdf          = {http://proceedings.mlr.press/v100/xie20a/xie20a.pdf},
	abstract     = {Deep reinforcement learning (DRL) is a promising approach for developing legged locomotion skills. However, current work commonly describes DRL as being a one-shot process, where the state, action and reward are assumed to be well defined and are directly used by an RL algorithm to obtain policies. In this paper, we describe and document an iterative design approach, which reflects the multiple design iterations of the reward that are often (if not always) needed in practice. Throughout the process, transfer learning is achieved via Deterministic Action Stochastic State (DASS) tuples, representing the deterministic policy actions associated with states visited by the stochastic policy. We demonstrate the transfer of policies learned in simulation to the physical robot without dynamics randomization. We also identify several key components that are critical for sim-to-real transfer in our setting.}
}
@misc{xie2021policy,
	title        = {Policy Finetuning: Bridging Sample-Efficient Offline and Online Reinforcement Learning},
	author       = {Tengyang Xie and Nan Jiang and Huan Wang and Caiming Xiong and Yu Bai},
	year         = 2021,
	eprint       = {2106.04895},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{siekmann2021blind,
	title        = {Blind Bipedal Stair Traversal via Sim-to-Real Reinforcement Learning},
	author       = {Jonah Siekmann and Kevin Green and John Warila and Alan Fern and Jonathan Hurst},
	year         = 2021,
	eprint       = {2105.08328},
	archiveprefix = {arXiv},
	primaryclass = {cs.RO}
}
@misc{plappert2018multigoal,
	title        = {Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research},
	author       = {Matthias Plappert and Marcin Andrychowicz and Alex Ray and Bob McGrew and Bowen Baker and Glenn Powell and Jonas Schneider and Josh Tobin and Maciek Chociej and Peter Welinder and Vikash Kumar and Wojciech Zaremba},
	year         = 2018,
	eprint       = {1802.09464},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{andrychowicz2018hindsight,
	title        = {Hindsight Experience Replay},
	author       = {Marcin Andrychowicz and Filip Wolski and Alex Ray and Jonas Schneider and Rachel Fong and Peter Welinder and Bob McGrew and Josh Tobin and Pieter Abbeel and Wojciech Zaremba},
	year         = 2018,
	eprint       = {1707.01495},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{Talelepush,
	title        = {{Mesh-based methods for quantifying and improving robustness of a planar biped model to random push disturbances}},
	author       = {Talele, Nihar and Byl, Katie},
	year         = 2019,
	journal      = {Proceedings of the American Control Conference},
	volume       = {2019-July},
	pages        = {1860--1866},
	doi          = {10.23919/acc.2019.8815226},
	isbn         = 9781538679265,
	issn         = {07431619},
	abstract     = {In this paper, we apply meshing tools to improve and analyze the performance of a 5-link planar biped model to random push perturbations. Creating a mesh for a 14-dimensional state space would typically be infeasible. However, as we show in this paper, low level controllers can restrict the reachable space of the system to a much lower dimensional manifold, which makes it possible to apply our tools to improve the performance. To validate the effectiveness of our tools in analyzing, quantifying and improving the performance of a system, we conduct simulations on two different sets of trajectories: one consisting of trajectories having only a single support phase, and a second set consisting of trajectories having both a double support as well as a single support phase.},
	file         = {:Users/sgillen/Textbooks/Papers/acc19{\_}Talele{\_}MeshbasedTrajOptRobustness.pdf:pdf},
	mendeley-groups = {CORL2020}
}
@book{Samet1990,
	title        = {{The design and analysis of spatial data structures.pdf}},
	author       = {Samet, Hanan},
	year         = 1990,
	publisher    = {Addison-Wesley},
	file         = {:Users/sgillen/Downloads/(Addison-Wesley series in computer science) Hanan Samet - The design and analysis of spatial data structures-Addison-Wesley (1990).pdf:pdf}
}
@article{Byl2009,
	title        = {{Metastable walking machines}},
	author       = {Byl, Katie and Tedrake, Russ},
	year         = 2009,
	journal      = {International Journal of Robotics Research},
	volume       = 28,
	number       = 8,
	pages        = {1040--1064},
	doi          = {10.1177/0278364909340446},
	issn         = {02783649},
	abstract     = {Legged robots that operate in the real world are inherently subject to stochasticity in their dynamics and uncertainty about the terrain. Owing to limited energy budgets and limited control authority, these " disturbances" cannot always be canceled out with high-gain feedback. Minimally actuated walking machines subject to stochastic disturbances no longer satisfy strict conditions for limit-cycle stability; however, they can still demonstrate impressively long-living periods of continuous walking. Here, we employ tools from stochastic processes to examine the "stochastic stability" of idealized rimless-wheel and compass-gait walking on randomly generated uneven terrain. Furthermore, we employ tools from numerical stochastic optimal control to design a controller for an actuated compass gait model which maximizes a measure of stochastic stabilityg-the mean first-passage time-and compare its performance with a deterministic counterpart. Our results demonstrate that walking is well characterized as a metastable process, and that the stochastic dynamics of walking should be accounted for during control design in order to improve the stability of our machines.},
	file         = {:Users/sgillen/Textbooks/Papers/Byl08f.pdf:pdf},
	keywords     = {Compass gait,Legged locomotion,Mean first-passage time1 stability metrics,Metastability,Passive dynamic walking,Rimless wheel,Rough terrain}
}
@article{Saglamhzd,
	title        = {{Meshing hybrid zero dynamics for rough terrain walking}},
	author       = {Saglam, Cenk Oguz and Byl, Katie},
	year         = 2015,
	journal      = {Proceedings - IEEE International Conference on Robotics and Automation},
	volume       = {2015-June},
	number       = {June},
	pages        = {5718--5725},
	doi          = {10.1109/ICRA.2015.7140000},
	issn         = 10504729,
	abstract     = {For an underactuated biped on a constant-slope terrain, the hybrid zero dynamics (HZD) controller framework provides exponentially stable walking motions. In this paper, we quantify the stability of such a control system on rough terrain by estimating the expected number of steps before failure. In addition, we show how to switch between multiple HZD controllers (optionally using terrain look-ahead) to increase the stability dramatically, e.g., 10 thousand steps compared to 10. To do this robustly, we make use of the new meshing method proposed in this paper.},
	file         = {:Users/sgillen/Textbooks/Papers/Saglam{\_}HZDmesh{\_}2015.pdf:pdf}
}
@misc{BrendanRyan/Publicdomain2020,
	title        = {{Fractal Dimension Example}},
	author       = {{Brendan Ryan / Public domain}},
	year         = 2020,
	booktitle    = {Wikapedia},
	mendeley-groups = {CORL2020}
}
@article{Mania2018,
	title        = {{Simple random search of static linear policies is competitive for reinforcement learning}},
	author       = {Mania, Horia and Guy, Aurelia and Recht, Benjamin},
	year         = 2018,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = {2018-December},
	number       = {NeurIPS},
	pages        = {1800--1809},
	issn         = 10495258,
	abstract     = {Model-free reinforcement learning aims to offer off-the-shelf solutions for controlling dynamical systems without requiring models of the system dynamics. We introduce a model-free random search algorithm for training static, linear policies for continuous control problems. Common evaluation methodology shows that our method matches state-of-the-art sample efficiency on the benchmark MuJoCo locomotion tasks. Nonetheless, more rigorous evaluation reveals that the assessment of performance on these benchmarks is optimistic. We evaluate the performance of our method over hundreds of random seeds and many different hyperparameter configurations for each benchmark task. This extensive evaluation is possible because of the small computational footprint of our method. Our simulations highlight a high variability in performance in these benchmark tasks, indicating that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms. Our results stress the need for new baselines, benchmarks and evaluation methodology for RL algorithms.},
	file         = {:Users/sgillen/Textbooks/Papers/7451-simple-random-search-of-static-linear-policies-is-competitive-for-reinforcement-learning.pdf:pdf},
	mendeley-groups = {CORL2020}
}
@inproceedings{OguzSaglam2015,
	title        = {{Robust Policies via Meshing for Metastable Rough Terrain Walking}},
	author       = {{Oguz Saglam}, Cenk and Byl, Katie},
	year         = 2015,
	booktitle    = {Robotics Science and Systems},
	doi          = {10.15607/rss.2014.x.049},
	abstract     = {—In this paper, we present and verify methods for developing robust, high-level policies for metastable (i.e., rarely falling) rough-terrain robot walking. We focus on simultaneously addressing the important, real-world challenges of (1) use of a tractable mesh, to avoid the curse of dimensionality and (2) maintaining near-optimal performance that is robust to uncertainties. Toward our first goal, we present an improved meshing technique, which captures the step-to-step dynamics of robot walking as a discrete-time Markov chain with a small number of points. We keep our methods and analysis generic, and illustrate robustness by quantifying the stability of resulting control policies derived through our methods. To demonstrate our approach, we focus on the challenge of optimally switching among a finite set of low-level controllers for underactuated, rough-terrain walking. Via appropriate meshing techniques, we see that even terrain-blind switching between multiple controllers increases the stability of the robot, while lookahead (terrain information) makes this improvement dramatic. We deal with both noise on the lookahead information and on the state of the robot. These two robustness requirements are essential for our methods to be applicable to real high-DOF robots, which is the primary motivation of the authors.},
	file         = {:Users/sgillen/Textbooks/Papers/Saglam{\_}RSS{\_}2014.pdf:pdf},
	mendeley-groups = {CORL2020}
}
@misc{engstrom2020implementation,
	title        = {Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO},
	author       = {Logan Engstrom and Andrew Ilyas and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},
	year         = 2020,
	eprint       = {2005.12729},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{Gneiting2012,
	title        = {{Estimators of fractal dimension: Assessing the roughness of time series and spatial data}},
	author       = {Gneiting, Tilmann and {\v{S}}ev{\v{c}}{\'{i}}kov{\'{a}}, Hana and Percival, Donald B.},
	year         = 2012,
	journal      = {Statistical Science},
	volume       = 27,
	number       = 2,
	pages        = {247--277},
	doi          = {10.1214/11-STS370},
	issn         = {08834237},
	abstract     = {The fractal or Hausdorff dimension is a measure of roughness (or smoothness) for time series and spatial data. The graph of a smooth, differentiable surface indexed in Rd has topological and fractal dimension d. If the surface is nondifferentiable and rough, the fractal dimension takes values between the topological dimension, d, and d + 1. We review and assess estimators of fractal dimension by their large sample behavior under infill asymptotics, in extensive finite sample simulation studies, and in a data example on arctic sea-ice profiles. For time series or line transect data, boxcount, Hall-Wood, semi-periodogram, discrete cosine transform and wavelet estimators are studied along with variation estimators with power indices 2 (variogram) and 1 (madogram), all implemented in the R package fractaldim. Considering both efficiency and robustness, we recommend the use of the madogram estimator, which can be interpreted as a statistically more efficient version of the Hall-Wood estimator. For two-dimensional lattice data, we propose robust transect estimators that use the median of variation estimates along rows and columns. Generally, the link between power variations of index p{\textgreater}0 for stochastic processes, and the Hausdorff dimension of their sample paths, appears to be particularly robust and inclusive when p = 1. {\textcopyright} Institute of Mathematical Statistics, 2012.},
	archiveprefix = {arXiv},
	arxivid      = {1101.1444},
	eprint       = {1101.1444},
	file         = {:Users/sgillen/Textbooks/Papers/1101.1444.pdf:pdf},
	keywords     = {Box-count,Gaussian process,Hausdorff dimension,Madogram,Power variation,Robustness,Sea-ice thickness,Smoothness,Variation estimator,Variogram},
	mendeley-groups = {CORL2020}
}
@article{Emery2005,
	title        = {{Variograms of order $\omega$: A tool to validate a bivariate distribution model}},
	author       = {Emery, Xavier},
	year         = 2005,
	journal      = {Mathematical Geology},
	volume       = 37,
	number       = 2,
	pages        = {163--181},
	doi          = {10.1007/s11004-005-1307-4},
	issn         = {08828121},
	abstract     = {The multigaussian model is used in mining geostatistics to simulate the spatial distribution of grades or to estimate the recoverable reserves of an ore deposit. Checking the suitability of such model to the available data often constitutes a critical step of the geostatistical study. In general, the marginal distribution is not a problem because the data can be transformed to normal scores, so the check is usually restricted to the bivariate distributions. In this work, several tests for diagnosing the two-point normality of a set of Gaussian data are reviewed and commented. An additional criterion is proposed, based on the comparison between the usual variogram and the variograms of lower order: the latter are defined as half the mean absolute increments of the attribute raised to a power between 0 and 2. This criterion is then extended to other bivariate models, namely the bigamma, Hermitian and Laguerrian models. The concepts are illustrated on two real data-sets. Finally, some conditions to ensure the internal consistency of the variogram under a given model are given. {\textcopyright} International Association for Mathematical Geology 2005.},
	file         = {:Users/sgillen/Textbooks/Papers/Emery2005{\_}Article{\_}VariogramsOfOrder{\^{I}}{\textcopyright}AToolToValid.pdf:pdf},
	keywords     = {Bigaussian distribution,Diffusion-type random functions,Isofactorial models,Madogram},
	mendeley-groups = {CORL2020}
}
@misc{schulman2017proximal,
	title        = {Proximal Policy Optimization Algorithms},
	author       = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
	year         = 2017,
	eprint       = {1707.06347},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{haarnoja2018soft,
	title        = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
	author       = {Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
	year         = 2018,
	eprint       = {1801.01290},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{mnih2013playing,
	title        = {Playing Atari with Deep Reinforcement Learning},
	author       = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
	year         = 2013,
	eprint       = {1312.5602},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{mnih2015humanlevel,
	title        = {Human-level control through deep reinforcement learning},
	author       = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	year         = 2015,
	month        = feb,
	journal      = {Nature},
	publisher    = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	volume       = 518,
	number       = 7540,
	pages        = {529--533},
	issn         = {00280836},
	url          = {http://dx.doi.org/10.1038/nature14236},
	added-at     = {2015-08-26T14:46:40.000+0200},
	biburl       = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
	description  = {Human-level control through deep reinforcement learning - nature14236.pdf},
	interhash    = {eac59980357d99db87b341b61ef6645f},
	intrahash    = {fb15f4471c81dc2b9edf2304cb2f7083},
	keywords     = {deep learning toread},
	timestamp    = {2015-08-26T14:46:40.000+0200}
}
@inproceedings{Saglam-RSS-14,
	title        = {Robust Policies via Meshing for Metastable Rough Terrain Walking},
	author       = {Cenk Oguz Saglam AND Katie Byl},
	year         = 2014,
	month        = {July},
	booktitle    = {Proceedings of Robotics: Science and Systems},
	address      = {Berkeley, USA},
	doi          = {10.15607/RSS.2014.X.049}
}

@ARTICLE{linear-walking-2022,
  author={Krishna, Lokesh and Castillo, Guillermo A. and Mishra, Utkarsh A. and Hereid, Ayonga and Kolathaya, Shishir},
  journal={IEEE Robotics and Automation Letters}, 
  title={Linear Policies are Sufficient to Realize Robust Bipedal Walking on Challenging Terrains}, 
  year={2022},
  volume={7},
  number={2},
  pages={2047-2054},
  doi={10.1109/LRA.2022.3143227}}


@inproceedings{Rajeswaran-NIPS-17,
	title        = {{Towards Generalization and Simplicity in Continuous Control}},
	author       = {Aravind Rajeswaran and Kendall Lowrey and Emanuel Todorov and Sham Kakade},
	year         = 2017,
	booktitle    = {NIPS}
}
@article{2018-TOG-deepMimic,
	title        = {DeepMimic: Example-guided Deep Reinforcement Learning of Physics-based Character Skills},
	author       = {Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and van de Panne, Michiel},
	year         = 2018,
	month        = jul,
	journal      = {ACM Trans. Graph.},
	publisher    = {ACM},
	address      = {New York, NY, USA},
	volume       = 37,
	number       = 4,
	pages        = {143:1--143:14},
	doi          = {10.1145/3197517.3201311},
	issn         = {0730-0301},
	url          = {http://doi.acm.org/10.1145/3197517.3201311},
	issue_date   = {August 2018},
	articleno    = 143,
	numpages     = 14,
	acmid        = 3201311,
	keywords     = {motion control, physics-based character animation, reinforcement learning}
}
@misc{rl-zoo3,
	title        = {RL Baselines3 Zoo},
	author       = {Raffin, Antonin},
	year         = 2020,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/DLR-RM/rl-baselines3-zoo}}
}
@misc{stable-baselines3,
	title        = {Stable Baselines3},
	author       = {Raffin, Antonin and Hill, Ashley and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Dormann, Noah},
	year         = 2019,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/DLR-RM/stable-baselines3}}
}
@misc{coumans2020,
	title        = {PyBullet, a Python module for physics simulation for games, robotics and machine learning},
	author       = {Erwin Coumans and Yunfei Bai},
	year         = {2016--2020},
	howpublished = {\url{http://pybullet.org}}
}
@article{Kaygisiz2001,
	title        = {{Smoothing stability roughness of a robot arm under dynamic load using reinforcement learning}},
	author       = {Kaygisiz, Burak H. and Erkmen, Aydan M. and Erkmen, Ismet},
	year         = 2001,
	journal      = {IEEE International Conference on Intelligent Robots and Systems},
	volume       = 4,
	pages        = {2392--2397},
	doi          = {10.1109/IROS.2001.976427},
	isbn         = {0780366123},
	abstract     = {We introduce in this paper a new fractal/rough set modeling approach to the domains of attraction of nonlinear systems obtained by cell mapping. The state space is partitioned into cells and the stability regions found using cell to cell mapping. Our new approach gives a fractal/rough set identity to the domains of attraction where cells are identified according to their fractal dimension as fully stable, possibly stable and unstable. There the stability domain is a rough set where fully stable cells determine the lower approximation of the domain, and possibly stable cells its rough boundary. Consequently, the totality of these cells forms an upper approximation to the rough stability domain. The boundary of this domain which is a rough set of cells having a fractal dimension as an attribute of roughness is smoothed, minimising the inherent stability uncertainty of the region, using a reinforcement learning technique which takes into account the stability history of each fractal/rough cell. This new approach intended to reinforce the performance of a controller under stability uncertainty is applied for illustrative purpose to a two-axis robot arm under dynamic load.},
	file         = {:Users/sgillen/Textbooks/Papers/fractal{\_}robot{\_}arm2.pdf:pdf},
	keywords     = {Cell to cell mapping,Fractal dimension,Rough sets,Rough stability boundary},
	mendeley-groups = {CORL2020}
}
@article{Taleledeep,
	title        = {{Mesh-based Tools to Analyze Deep Reinforcement Learning Policies for Underactuated Biped Locomotion}},
	author       = {Talele, Nihar and Byl, Katie},
	year         = 2019,
	url          = {http://arxiv.org/abs/1903.12311},
	abstract     = {In this paper, we present a mesh-based approach to analyze stability and robustness of the policies obtained via deep reinforcement learning for various biped gaits of a five-link planar model. Intuitively, one would expect that including perturbations and/or other types of noise during training would likely result in more robustness of the resulting control policy. However, one would also like to have a quantitative and computationally-efficient means of evaluating the degree to which this might be so. Rather than relying on Monte Carlo simulations, which can become quite computationally burdensome in quantifying performance metrics, our goal is to provide more sophisticated tools to assess robustness properties of such policies. Our work is motivated by the twin hypotheses that contraction of dynamics, when achievable, can simplify the required complexity of a control policy and that control policies obtained via deep learning may therefore exhibit tendency to contract to lower-dimensional manifolds within the full state space, as a result. The tractability of our mesh-based tools in this work provides some evidence that this may be so.},
	archiveprefix = {arXiv},
	arxivid      = {1903.12311},
	eprint       = {1903.12311},
	file         = {:Users/sgillen/Textbooks/Papers/1903.12311.pdf:pdf}
}
@article{Byl2017,
	title        = {{Mesh-based switching control for robust and agile dynamic gaits}},
	author       = {Byl, Katie and Strizic, Tom and Pusey, Jason},
	year         = 2017,
	journal      = {Proceedings of the American Control Conference},
	pages        = {5449--5455},
	doi          = {10.23919/ACC.2017.7963802},
	isbn         = 9781509059928,
	issn         = {07431619},
	abstract     = {In this paper, we describe and analyze mesh-based tools to control bounding motions of an 8 degree-of-freedom planar quadruped model with limited footholds on terrain. There are two complementary goals in our presentation. First, we aim to clarify potential advantages and disadvantages of our mesh-based approach in planning agile motions for a legged system. A key advantage is the ability to map the reachable states and their feasible transitions, given a relatively high-dimensional nonlinear dynamic system for which traditional meshing techniques would be impractical. A suspected disadvantage is that meshing has finite resolution, and robustness of mesh-based results should correspondingly be considered. Our second goal is to discuss appropriate frameworks for optimizing agility. Unlike typical locomotion optimization studies, in which control is designed for a limit cycle behavior that minimizes energy use or improves robustness to perturbations, here we focus on quantifying the performance of sets of controllers that together enhance reachability of the controlled system. In planning agile motions for our legged system model, we find that our mesh-based policies predict future dynamics robustly for plans up to about a 5-step horizon, and in quantifying controller sets, we emphasize that both the number of and parameterizations for such controllers should be considered in tandem during optimization.},
	file         = {:Users/sgillen/Textbooks/Papers/DRAFT{\_}strizic{\_}ACC17.pdf:pdf}
}
@article{Saglam2015,
	title        = {{Meshing hybrid zero dynamics for rough terrain walking}},
	author       = {Saglam, Cenk Oguz and Byl, Katie},
	year         = 2015,
	journal      = {Proceedings - IEEE International Conference on Robotics and Automation},
	volume       = {2015-June},
	number       = {June},
	pages        = {5718--5725},
	doi          = {10.1109/ICRA.2015.7140000},
	issn         = 10504729,
	abstract     = {For an underactuated biped on a constant-slope terrain, the hybrid zero dynamics (HZD) controller framework provides exponentially stable walking motions. In this paper, we quantify the stability of such a control system on rough terrain by estimating the expected number of steps before failure. In addition, we show how to switch between multiple HZD controllers (optionally using terrain look-ahead) to increase the stability dramatically, e.g., 10 thousand steps compared to 10. To do this robustly, we make use of the new meshing method proposed in this paper.},
	file         = {:Users/sgillen/Textbooks/Papers/Saglam{\_}HZDmesh{\_}2015.pdf:pdf}
}
@inproceedings{deisenroth2011pilco,
	title        = {PILCO: A model-based and data-efficient approach to policy search},
	author       = {Deisenroth, Marc and Rasmussen, Carl E},
	year         = 2011,
	booktitle    = {Proceedings of the 28th International Conference on machine learning (ICML-11)},
	pages        = {465--472},
	organization = {Citeseer}
}
@inproceedings{todorov2012mujoco,
	title        = {Mujoco: A physics engine for model-based control},
	author       = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
	year         = 2012,
	booktitle    = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
	pages        = {5026--5033},
	organization = {IEEE}
}
@article{metz2021gradients,
	title        = {Gradients are Not All You Need},
	author       = {Metz, Luke and Freeman, C Daniel and Schoenholz, Samuel S and Kachman, Tal},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2111.05803}
}
@article{RUBINSTEIN199789,
	title        = {Optimization of computer simulation models with rare events},
	author       = {Reuven Y. Rubinstein},
	year         = 1997,
	journal      = {European Journal of Operational Research},
	volume       = 99,
	number       = 1,
	pages        = {89--112},
	doi          = {https://doi.org/10.1016/S0377-2217(96)00385-2},
	issn         = {0377-2217},
	url          = {https://www.sciencedirect.com/science/article/pii/S0377221796003852}
}
@inproceedings{8053243,
	title        = {Gate-variants of Gated Recurrent Unit (GRU) neural networks},
	author       = {Dey, Rahul and Salem, Fathi M.},
	year         = 2017,
	booktitle    = {2017 IEEE 60th International Midwest Symposium on Circuits and Systems (MWSCAS)},
	volume       = {},
	number       = {},
	pages        = {1597--1600},
	doi          = {10.1109/MWSCAS.2017.8053243}
}
@article{doi:10.1021/jp970984n,
	title        = {Global Optimization by Basin-Hopping and the Lowest Energy Structures of Lennard-Jones Clusters Containing up to 110 Atoms},
	author       = {Wales, David J. and Doye, Jonathan P. K.},
	year         = 1997,
	journal      = {The Journal of Physical Chemistry A},
	volume       = 101,
	number       = 28,
	pages        = {5111--5116},
	doi          = {10.1021/jp970984n}
}
@article{10.3389/fnbot.2019.00006,
	title        = {A Differentiable Physics Engine for Deep Learning in Robotics},
	author       = {Degrave, Jonas and Hermans, Michiel and Dambre, Joni and wyffels, Francis},
	year         = 2019,
	journal      = {Frontiers in Neurorobotics},
	volume       = 13,
	doi          = {10.3389/fnbot.2019.00006},
	issn         = {1662-5218},
	url          = {https://www.frontiersin.org/article/10.3389/fnbot.2019.00006},
	abstract     = {An important field in robotics is the optimization of controllers. Currently, robots are often treated as a black box in this optimization process, which is the reason why derivative-free optimization methods such as evolutionary algorithms or reinforcement learning are omnipresent. When gradient-based methods are used, models are kept small or rely on finite difference approximations for the Jacobian. This method quickly grows expensive with increasing numbers of parameters, such as found in deep learning. We propose the implementation of a modern physics engine, which can differentiate control parameters. This engine is implemented for both CPU and GPU. Firstly, this paper shows how such an engine speeds up the optimization process, even for small problems. Furthermore, it explains why this is an alternative approach to deep Q-learning, for using deep learning in robotics. Finally, we argue that this is a big step for deep learning in robotics, as it opens up new possibilities to optimize robots, both in hardware and software.}
}
@inbook{doi:10.2514/6.2018-1452,
	title        = {Parallel Monotonic Basin Hopping for Low Thrust Trajectory Optimization},
	author       = {Steven L. McCarty and Laura M. Burke and Melissa McGuire},
	booktitle    = {2018 Space Flight Mechanics Meeting},
	pages        = {},
	doi          = {10.2514/6.2018-1452},
	url          = {https://arc.aiaa.org/doi/abs/10.2514/6.2018-1452},
	chapter      = {},
	eprint       = {https://arc.aiaa.org/doi/pdf/10.2514/6.2018-1452}
}
@misc{qiao2021efficient,
	title        = {Efficient Differentiable Simulation of Articulated Bodies},
	author       = {Yi-Ling Qiao and Junbang Liang and Vladlen Koltun and Ming C. Lin},
	year         = 2021,
	eprint       = {2109.07719},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{pmlr-v139-mora21a,
	title        = {PODS: Policy Optimization via Differentiable Simulation},
	author       = {Mora, Miguel Angel Zamora and Peychev, Momchil and Ha, Sehoon and Vechev, Martin and Coros, Stelian},
	year         = 2021,
	month        = {18--24 Jul},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 139,
	pages        = {7805--7817},
	url          = {https://proceedings.mlr.press/v139/mora21a.html},
	editor       = {Meila, Marina and Zhang, Tong},
	pdf          = {http://proceedings.mlr.press/v139/mora21a/mora21a.pdf},
	abstract     = {Current reinforcement learning (RL) methods use simulation models as simple black-box oracles. In this paper, with the goal of improving the performance exhibited by RL algorithms, we explore a systematic way of leveraging the additional information provided by an emerging class of differentiable simulators. Building on concepts established by Deterministic Policy Gradients (DPG) methods, the neural network policies learned with our approach represent deterministic actions. In a departure from standard methodologies, however, learning these policies does not hinge on approximations of the value function that must be learned concurrently in an actor-critic fashion. Instead, we exploit differentiable simulators to directly compute the analytic gradient of a policy’s value function with respect to the actions it outputs. This, in turn, allows us to efficiently perform locally optimal policy improvement iterations. Compared against other state-of-the-art RL methods, we show that with minimal hyper-parameter tuning our approach consistently leads to better asymptotic behavior across a set of payload manipulation tasks that demand a high degree of accuracy and precision.}
}
@misc{huang2021cemgd,
	title        = {CEM-GD: Cross-Entropy Method with Gradient Descent Planner for Model-Based Reinforcement Learning},
	author       = {Kevin Huang and Sahin Lale and Ugo Rosolia and Yuanyuan Shi and Anima Anandkumar},
	year         = 2021,
	eprint       = {2112.07746},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{pourchot2019cemrl,
	title        = {CEM-RL: Combining evolutionary and gradient-based methods for policy search},
	author       = {Aloïs Pourchot and Olivier Sigaud},
	year         = 2019,
	eprint       = {1810.01222},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{bharadhwaj2020modelpredictive,
	title        = {Model-Predictive Control via Cross-Entropy and Gradient-Based Optimization},
	author       = {Homanga Bharadhwaj and Kevin Xie and Florian Shkurti},
	year         = 2020,
	eprint       = {2004.08763},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{hu2020difftaichi,
	title        = {DiffTaichi: Differentiable Programming for Physical Simulation},
	author       = {Yuanming Hu and Luke Anderson and Tzu-Mao Li and Qi Sun and Nathan Carr and Jonathan Ragan-Kelley and Frédo Durand},
	year         = 2020,
	eprint       = {1910.00935},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{heiden2021neuralsim,
	title        = {Neural{S}im: Augmenting Differentiable Simulators with Neural Networks},
	author       = {Heiden, Eric and Millard, David and Coumans, Erwin and Sheng, Yizhou and Sukhatme, Gaurav S},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
	url          = {https://github.com/google-research/tiny-differentiable-simulator}
}
@software{brax2021github,
	title        = {Brax - A Differentiable Physics Engine for Large Scale Rigid Body Simulation},
	author       = {C. Daniel Freeman and Erik Frey and Anton Raichuk and Sertan Girgin and Igor Mordatch and Olivier Bachem},
	year         = 2021,
	url          = {http://github.com/google/brax},
	version      = {0.0.10}
}
@article{williams1992simple,
	title        = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	author       = {Williams, Ronald J},
	year         = 1992,
	journal      = {Machine learning},
	publisher    = {Springer},
	volume       = 8,
	number       = 3,
	pages        = {229--256}
}
@article{279181,
	title        = {Learning long-term dependencies with gradient descent is difficult},
	author       = {Bengio, Y. and Simard, P. and Frasconi, P.},
	year         = 1994,
	journal      = {IEEE Transactions on Neural Networks},
	volume       = 5,
	number       = 2,
	pages        = {157--166},
	doi          = {10.1109/72.279181}
}
@article{Carpanese2022,
author = {Carpanese, Francesco and Ewalds, Timo and Hafner, Roland and Abdolmaleki, Abbas and Casas, Diego De and Donner, Craig and Fritz, Leslie and Galperti, Cristian and Huber, Andrea and Keeling, James and Tsimpoukelli, Maria and Kay, Jackie and Merle, Antoine and Moret, Jean-marc and Noury, Seb and Pesamosca, Federico and Pfau, David and Sauter, Olivier and Sommariva, Cristian and Coda, Stefano and Duval, Basil and Fasoli, Ambrogio and Kohli, Pushmeet and Kavukcuoglu, Koray},
doi = {10.1038/s41586-021-04301-9},
file = {:Users/sgillen/Books/Papers/s41586-021-04301-9.pdf:pdf},
number = {February},
publisher = {Springer US},
title = {{Magnetic control of tokamak plasmas through deep reinforcement learning}},
volume = {602},
year = {2022}
}


