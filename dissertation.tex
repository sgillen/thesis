%%%---PREAMBLE---%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[oneside,12pt,final]{sty/ucthesis-CA2012}
\pdfoutput=1

%--- Packages ---------------------------------------------------------

\makeatletter
%\let\normalsize\relax
\let\@currsize\normalsize
\makeatother

\usepackage[utf8]{inputenc}
\usepackage{multirow}

\usepackage{graphicx} % Required to insert images
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\usepackage{csquotes}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\renewcommand{\vec}[1]{\mathbf{#1}}
\usepackage{float}
\usepackage{commath}
\usepackage{booktabs}
\usepackage{hyperref}
% in preamble
\usepackage{array}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{braket}
\usepackage{color}
\usepackage{setspace}

\renewcommand{\vec}[1]{\mathbf{#1}}

\usepackage{tabu}
\usepackage{multirow}


\usepackage{wrapfig}
\usepackage{lipsum}

\usepackage{breqn}
\usepackage{pdfpages}

\usepackage[english]{babel}

\usepackage{caption}
\usepackage{subcaption}





%---New Definitions and Commands------------------------------------------------------
\def\p{\partial}
\def\im{\mrm{im}}
\def\Tr{\mrm{Tr}}
\def\Z{\mbb{Z}}
\def\R{\mbb{R}}
\def\C{\mbb{C}}
\def\half{\frac{1}{2}}
\def\filler{\phantom{fillerfillerfiller}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\mrm}[1]{\mathrm{#1}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\ph}[1]{\phantom{#1}}
\newcommand{\udten}[3]{#1^{#2}_{\ph{#2}#3}}
\newcommand{\duten}[3]{#1^{\ph{#2}#3}_{#2}}
%\newcommand{\pd}[2]{\frac{\p#1}{\p#2}}
\newcommand{\D}[2]{\frac{d#1}{d#2}}

\newcommand{\todo}[1]{\color{red}#1\color{black}}

%---Set Margins ------------------------------------------------------
\setlength\oddsidemargin{0.25 in} \setlength\evensidemargin{0.25 in} \setlength\textwidth{6.25 in} \setlength\textheight{8.50 in}
\setlength\footskip{0.25 in} \setlength\topmargin{0 in} \setlength\headheight{0.25 in} \setlength\headsep{0.25 in}



%%%---DOCUMENT---%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%=== Preliminary Pages ============================================
\begin{frontmatter}
	\input{tex/title}
	\maketitle
	\approvalpage
	\copyrightpage
	\input{tex/dedicate}%comment out if you don't want a dedication
	\input{tex/acknowledgements}
	%\input{tex/vitae}
    \includepdf[pages=-]{sgillen_CV.pdf}
	\input{tex/abstract}
	\tableofcontents
\end{frontmatter}

\begin{mainmatter}

%---Set Headers and Footers ------------------------------------------------------
\pagestyle{fancy}
\renewcommand{\chaptermark}[1]{\markboth{{\sf #1 \hspace*{\fill} Chapter~\thechapter}}{} }
\renewcommand{\sectionmark}[1]{\markright{ {\sf Section~\thesection \hspace*{\fill} #1 }}}
\fancyhf{}

\makeatletter \if@twoside \fancyhead[LO]{\small \rightmark} \fancyhead[RE]{\small\leftmark} \else \fancyhead[LO]{\small\leftmark}
\fancyhead[RE]{\small\rightmark} \fi

\def\cleardoublepage{\clearpage\if@openright \ifodd\c@page\else
  \hbox{}
  \vspace*{\fill}
  \begin{center}
    This page intentionally left blank
  \end{center}
  \vspace{\fill}
  \thispagestyle{plain}
  \newpage
  \fi \fi}
\makeatother
\fancyfoot[c]{\textrm{\textup{\thepage}}} % page number
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

\fancypagestyle{plain} { \fancyhf{} \fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}

%=== Introduction ============================================
\chapter{Introduction and Literature Review}

Reinforcement learning has a long history, which I will tell you about at length.

Now in recent years research into deep reinforcement has exploded, with many impressive results advancing the state of the art in many fields.


- Super human video game AI (starcraft, dota, atari) \\ 
    - Board Games (Go, Chess) (State of the art here ... \todo{Is it} \\ 
    - Object Manipulation (Rubics cube) \\ 
    
    
However there are a number of glaring issues I see with the current state of the art in reinforcement learning.
    - Sample Inefficient \\
    - Sim 2 real must be solved, for robotics \\ 
    - Can't trust \\ 
    - Quantifying Performance is Hard \\ 
    - It is extremely Brittle  \\ 
    - Sensitive To Initial Conditions \\ 
    - Does Terribly in Some Kinds of Problems
    
    
I take some steps to solve this

    I solved the Acrobot, Twice ... Very Cool
    
    I did some reward post processing on a Markov chain thing
    Uhh anyway ... 
    
    
    Only a few years later, we saw a big breakthrough, a team was able to train neural networks to play Atari games at a superhuman level. The input was the raw pixel values from the game, and the reward is the score from the game. This is remarkable for a number of reasons, but it's cool that the same algorithm was able to work successfully across a wide variety of games, which you might think require very different strategies. \cite{mnih2015humanlevel}


% \begin{figure}[!htb]
%     \centering
%     \includegraphics[width=.2\linewidth]{fig/dissertation/Atari.png}
%     \label{fig:atari}
% \end{figure}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{fig/dissertation/Go.png}
    \caption{\cite{2017Natur.550..354S}}
    \label{fig:alphago}
\end{figure}


\begin{figure}[!htb]
    \centering
    \includegraphics[width=.6\linewidth]{fig/dissertation/humanoid_running.png}
    \caption{\cite{heess_emergence_2017}}
    \label{fig:deepmind_running}
\end{figure}


\begin{figure}[!htb]
    \centering
    \includegraphics[width=.6\linewidth]{fig/dissertation/rubics_cube.png}
    \caption{\cite{openai_learning_2018}}
    \label{fig:rubics_cube}
\end{figure}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=.6\linewidth]{fig/dissertation/anymal_grid.png}
    \caption{\cite{anymal2022}}
    \label{fig:anymal_grid}
\end{figure}


\cite{Carpanese2022} Nucluear fusion


Probably the most impressive and famous examples have been in the realm of games. An early breakthrough was in 2015, when Mnih1 et. al. showed that DRL could play a wide range of Atari video games using deep reinforcement learning \cite{mnih2015humanlevel}. This is significant for two reasons, this first is that this is a very challenging problem, the agent is tasked to play the game using only pixles as input, and only the score of the game as output. Imagine you are the agent, you are handed an array of 12288 numbers (a 64x64 RGB image), asked to pick an action, and then given a score of zero. This repeats maybe 100 times and then you get a reward of one. Was is the action you picked at timestep 62? was it because entry 10456 had a large value at timestep 3? Figuring this out on it's own is impressive, and using the same algorithm to solve dozens of different games was even more impressive. In 2016 Alpha Go achieved super human performance in the game of Go, which no other approach has been able to do \cite{2017Natur.550..354S}. The same team later released Mu Zero, which surpassed Alpha Go and can also play shogi, chess, and a suite of Atari games, again at superhuman levels \cite{muzero2020}. 

in which DRL has been used for physics-based character animation~\cite{2018-TOG-deepMimic}, and a wide variety of complex robotic tasks. This paper focuses primarily on applications in robotics. Continuous control problems in the context of robotics include controlling a 47 degree-of-freedom (DOF) humanoid to navigate various obstacles~\cite{heess_emergence_2017}, dexterously manipulating objects with a 24 DOF robotic hand~\cite{openai_learning_2018}, training the quadrupedal ANYmal robot to recover from falls~\cite{lee_robust_2019} \cite{hwangbo_learning_2019}, and teaching the bipedal Cassie robot to navigate stairs blindly~\cite{siekmann2021blind}. Recently I might even argue that DRL has taken over Boston dynamics in terms of robust walking with the recent work by ETH Zurich \cite{anymal2022}.


    


%=== Chapter  ============================================
\chapter{Background}
%---  Section -------------------------

\input{tex/background}


%=== Chapter ==============================================
\chapter{Switching}

\input{tex/p1.tex}

%=== Chapter   ============================================
\chapter{Reward Post Processing and Mesh Dimensions}

\input{tex/p2.tex}

\input{tex/p3.tex}

\input{tex/p4.tex}

\chapter{Differentiable Simulators}

\input{tex/p5.tex}

\chapter{Conclusions}



%=== Appendix ============================================
\appendix

\dsp

\chapter{Appendix Title }{\label{appendix:a}}
\begin{section}{Section Title}

Appendicitis

\end{section}
\end{mainmatter}

%----- Bibliography ----------------
\ssp
\bibliographystyle{JHEP3}
\bibliography{dissertation}

\end{document} 
