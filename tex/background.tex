\section{Reinforcement Learning}
\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{fig/dissertation/RL_sutton.png}
    \caption{The classic picture from Sutton and Barto that everyone seems to steal without attribution}
    \label{fig:r}
\end{figure}

What is reinforcement learning? Let's introduce the basic terminology first, we will formalize this later. An agent takes actions in an environment in order to maximize a reward. The environment is described by a state, which can be observed (sometimes only partially) by the agent. One example might be a chess playing program, the state is the position of the pieces on the board, the action the their move on the current term, and their reward is a one when they win the game and a zero otherwise. Or maybe the agent is a robot, whose states are joint angles and velocities, whos actions are motor commands, and whos reward function is their forward velocity. Another example might be a motor inside that very robot, it's state might include a motor command and a measurement of the current motor position, and it's output might be a voltage signal.

As these examples hopefully illustrate, reinforcement learning is extremely general, and can and has been applied to a myriad of problems across a number of domains. 

A history of reinforcement learning... Do I need this? 
I can just present the state of the art here to compare to what I did. And I can do that in chronological order

Definition and Problem Statement, maybe some early victories
\todo{Also need to indtroduce Q function and Value functions}

In reinforcement learning, the goal is to train an agent, acting in an environment, to maximize some reward function. The environment is a discrete time dynamical system described by state $s_{t} \in \mathbb{R}^{n}$ and the current action $a_{t} \in \mathbb{R}^{b}$. an evolution function $f: \mathbb{R}^{n} \times \mathbb{R}^{b} \rightarrow \mathbb{R}^{n}$ takes as input the current state and action, and outputs the state at time t+1:


\begin{equation}
s_{t+1}= f(s_{t},a_{t})
\end{equation} 

The controller is the function we are learning $g: \mathbb{R}^{n} \times \mathbb{R}^{\norm{\theta}} \rightarrow \mathbb{R}^{m}$ such that:

\begin{equation}
a_{t} = g(s_{t}, \theta)
\end{equation} 

The goal is to maximize a reward function $r : \mathbb{R}^{n} \times \mathbb{R}^{m} \times \mathbb{R}^{n} \rightarrow \mathbb{R}$. We consider the finite time undiscounted reward case. The objective function then is:

\begin{equation} 
R(\theta) =  \sum_{t=0}^{T}r(s_{t}, a_{t}, s_{t+1}) 
\end{equation}


Then REINFORCE

REINFORCE is the OG policy gradient algorithm, it's pretty old and influential. \todo{I need to introduce policy gradients here I guess}

Then some stuff I don't care about

\section{Parameterized Functions and Neural Networks}

It's worth taking a second here and talking about so called function approximators and neural networks, because otherwise the rest of this paper won't make any sense. A function approximator is really just a function which has tunable parameter in addition to inputs and outputs. In controls or reinforcement learning, we usually just call these "parameterized policies". Let's look at a simple example, third order polynomial with coefficients $a_{0}$, $a_{1}$, and $a_{2}$

\begin{equation}
\label{eq:poly}
    y = a_{0} + a_{1}x + a_{2}x^{2}
\end{equation}

At risk of belaboring the point, the equation above has an input (x), and output(y) but also parameters($a$). In function approximation, we typically have data from an unknown function, often sampled from a physical system, and we wish to solve for parameters such that our function approximator best matches the output of the target function. 

One of the most successful and popular function approximators, especially in recent years, have been neural networks. These come in many flavors, multilayers perecptrons (MLPs), convolutional neural networks (CNNs), long short term memories (LSTM) and Gated Recurrent Units (GRUs).

Let's examine the MLP first in particular, which is the most common especially for control applications. We can see a diagram for an MLP in figure \ref{fig:mlp}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=.8\linewidth]{fig/dissertation/mlp.png}
    \caption{A very simple MLP}
    \label{fig:mlp}
\end{figure}

The outut is computed as follows: \todo{blah blah}

So essentially, we do a big matrix multiply of the weights, a vectorized nonlinearity on the result, a new matrix multiply with the next layer weights, repeat. 

Cool, another thing worth noting is that for most modern DRL, the thing we are parameterizing is actually a probability over actions rather than directly mapping to actions. So if we have an action space of size three then our network may have 6 outputs, 3 means and 3 standard deviations for normal distributions over the actions. When it is time to select an action, we sample from this distribution. 

\section{Deep Learning and Supervised Learning}

Supervised learning is when you learn by example. For example classifying images as either dogs or cats. This is done with backpropogation and a certain loss

o train the gating network we minimize the binary cross entropy loss:

\begin{dmath}L^{\text{G}} =  \mathop{\mathbb{E}}_{\gamma} -\left[ c_{w} y_{i}\log(G_{\gamma}(s_{i})) + (1 - y_{i})\log(1 - G_{\gamma}(s_{i})) \right]\end{dmath} 

Where $y_{i}$ is the class label for the ith sample, $c_{w}$ is a class weight for positive examples. we set $c_{w} = \frac{n_{t}}{n_{p}}w $ where $n_{t}$ is the total number of samples, $n_{p}$ is the number of positive examples, and $w$ is a manually chosen weighting parameter to encourage learning a conservative basin of attraction.

As discussed in the introduction, deep supervised learning really took off starting around 2012. 


\section{Modern Reinforcement Learning}
\todo{Might do one example each? Policy Gradient, SAC, then ARS?}

\subsection{Off Policy RL}    
    
We can further divide up DRL into on policy and off policy algorithms. On policy algorithms use only data obtained with the under the current policy to make policy updates. After an update it essentially throws out the data obtained so far and starts afresh. Examples of this include the classic REINFORCE, A2C, TPRO and PPO. Off policy algorithms by contrast use data obtained by previous policies to continue updating. This usually takes the form of a "replay buffer" which stores state, action, and reward tuples obtained so far during training. These tuples can then be used to update the current policy. Off policy algorithms are often a variant of Q learning, since the Q function in reinforcement learning  does not require the current action, it is well suited to off policy learning. 

\subsubsection{Soft Actor Critic}

Soft actor critic (SAC) is an off policy deep reinforcement learning algorithm shown to do well on control tasks with continuous actions spaces \cite{haarnoja_soft_2018}. To aid in exploration, rather than directly optimize the discounted sum of future rewards, SAC attempt to find a policy that optimizes a surrogate objective: 


\begin{dmath}J^{\mathrm{soft}} = \mathbb{E}\left[ \sum_{t=0}^{\infty}\gamma^{t}\bigg(R_{t} + \alpha H(\pi(\cdot|s_{t}))\bigg) \right]  \label{eq:thresh} \end{dmath}
 
 Where $H$ is the entropy of the policy.
 
 %\[ V^{\pi}(s) = \mathbb{E}\left[ \sum_{t=0}^{\infty}\gamma^{t}\bigg(R_{t} + \alpha H(\pi(\cdot|s_{t}))\bigg)  \bigg\rvert s_{0} = s\right] \]

%We are also going to learn a Q function which satisfies:

%\[ Q^{\pi}(s,a) = \mathbb{E}\left[ \sum_{t=0}^{\infty}\gamma^{t}\bigg(R_{t}+ \alpha \sum_{t=1}^{\infty} \gamma^{t} H(\pi(\cdot|s_{t}))\bigg)  \bigg\rvert s_{0} = s, a_{0} = a\right] \]

 
SAC introduces several neural networks for the training. We define a soft value function $V_{\phi}(s_{t})$, a neural network defined by weights $\phi$, which approximate $J^{soft}$ given the current state. Next we define two soft Q functions $Q_{\rho_{1}}(s_{t}, a_{t})$ and $Q_{\rho_{2}}(s_{t}, a_{t})$ which approximate $J^{soft}$ given both the current state and the current action. Using two Q networks is a trick that aids the training by avoiding overestimating the Q function. We must also define a target soft value function $V_{\overline \phi}(s_{t})$, which follows the value function via polyak averaging:

\begin{dmath}
V_{\overline \phi^{+}}(s_{t}) = c_{py}V_{\overline \phi}(s_{t}) +  (1 - c_{py})V_{\phi}
\end{dmath}

With $c_{py}$ a fixed hyper parameter. We also define $\Pi_{\theta}$, a neural network that outputs $\mu_{\theta}(s_{t})$ and $\log(\sigma_{\theta}(s_{t}))$ which define the probability distribution of our policy $\pi_{\theta}$. The action is given by:

\begin{dmath}
a_{t} = \tanh(\mu_{\theta}(s_{t}) + \sigma_{\theta}(s_{t}) \epsilon_{t})
\label{act}
\end{dmath}

where $\epsilon_{t}$ is drawn from $N(0,1)$. 
 
SAC also make use of a replay buffer $D$ which stores the tuple $(s_{t}, a_{t}, r_{t})$ after policy rollouts. When it is time to update we sample randomly from this buffer, and use those samples to compute our losses and update our weights. 
 
 
With this we can define the losses for each of these networks (originated from \cite{haarnoja_soft_2018})
 
The loss for our two Q functions is:
 
\begin{dmath} L^{Q} = \mathbb{E}_{s_{t}, a_{t} \sim D}\left[\frac{1}{2}\left( Q_{\rho}(s_{t},a_{t}) - \hat{Q}(s_{t}, a_{t}) \right) ^{2}  \right] \end{dmath}
where
\begin{dmath} \hat{Q}(s_{t}, a_{t}) = r(s_{t}, a_{t}) + \gamma\mathbb{E}_{s_{t+1}}\left[V_{\overline{\phi}}(s_{t+1} ) \right]  \end{dmath}

Our policy seeks to minimize:

\begin{dmath} 
L^{\pi} = \mathbb{E}_{s_{t} \sim D, \epsilon_{t} \sim N(0,1)}\left[ \log \pi_{\theta}(f_{\theta}(\epsilon_{t}, s_{t}) | s_{t}) - Q_{\rho_{1}}(s_{t}, f_{\theta}(\epsilon_{t}, s_{t}) \right]
\end{dmath}

And our value function:

\begin{dmath}
L^{\text{V}} = \mathop{\mathbb{E}}_{s_{t} \sim D} \left[ \frac{1}{2} \left( V_{\phi}(s_{t}) - \hat  V_{\phi}(s_{t}) \right)^{2}  \right] 
\end{dmath} 

Where

\begin{dmath}
\hat V_{\phi} = \mathbb{E}_{a_{t} \sim \pi_{\theta}} \left[Q^{min}(s_{t}, a_{t}) - log \pi_{\theta}(a_{t} | s_{t}) \right]
\end{dmath}

And $Q^{min} = \min(Q_{\rho_{1}}(s_{t}, a_{t}), Q_{\rho_{2}}(s_{t}, a_{t}))$

 SAC starts by doing policy roll outs, recording the state, action, reward, and the active controller at each time step. It stores these experiences in the replay buffer. After enough trials have been run, we run our update step. We sample from the replay buffer, and use these sampled states to compute the losses above. We then run one step of Adam \cite{kingma_adam:_2014} to update our network weights. We repeat this update $n_{u}$ times with different samples. Finally we  copy our weights to our target network and repeat until convergence (or some other stopping metric). 

\subsection{On Policy RL}

In general, off policy algorithms tend to be more sample efficient, requiring less samples to obtain a similar reward level. On policy algorithms are less sample efficient, but do tend to be more stable, getting more consistent rewards across random seeds.

\subsubsection{Policy Gradients}


\begin{equation}
    \label{eq:vapg}
    \theta^{+} = \theta + \alpha \nabla_\theta R(\theta)
\end{equation}

\begin{equation}
\nabla_{\theta}R(\theta) = \mathop{\mathbb{E}}_{\eta}\left[
\sum_{t=0}^{T} \nabla_{\theta}\log \pi_\theta(a_{t}|s_{t})r_{t}
\right]
\end{equation}


\subsection{Gradient Free Algorithms}


\subsubsection{Augmented Random Search}
    In contrast to other modern RL algorithms, Augmented Random Search (ARS) represents a return to simplicity, it is perhaps a testament to how far increased computational power alone can take us. in \cite{Mania2018}, the authors showed that linear policies and a simple direct random search over policy parameters was sufficient to obtain state of the art results for the Mujoco locomotion tasks. In addition to this, work by \cite{Rajeswaran-NIPS-17} also showed that simpler policy classes, linear and RBF policies, combined with a simple natural policy gradient algorithm could also compete with state of the art RL. There is also \cite{linear-walking-2022} which is just more proof.
    
    

\begin{algorithm}
\caption{Direct Policy Search}\label{algo:brs}
\begin{algorithmic}[1]
\Require Policy $\pi$ with trainable parameters $\theta$
\Require Hyper-parameters - $\alpha$ $\sigma$ $n$
\State Sample $\bm{\delta} = [\delta_{1}, ..., \delta_{n}]$ from $\mathcal{N}(0, \sigma)^{\text{n x }\abs{\theta}}$
\State $\theta^{*}  = [\theta - \delta_{1}, ..., \theta - \delta_{n}, \theta + \delta_{1}, ..., \theta + \delta_{n}] $
\For{$\theta_{i}$ in $\theta^{*}$}
    \State Do rollout with policy $\pi_{\theta_{i}}$, using the MLA
    \State Collect sum of rewards $R_{i}$. 
\EndFor
\State $ \theta^{+} = \theta + \frac{\alpha}{n \sigma_{R}}\sum_{i=0}^{n} (R_{i} - R_{i+n})\delta_{i} $ 
\end{algorithmic}
\end{algorithm}



