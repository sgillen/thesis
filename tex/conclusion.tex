
To conclude, let us revisit the limitations of reinforcement learning that we outlined in chapter one, and examine how we have addressed them in this thesis. We claimed that modern RL does not have good ways to incorporate domain knowledge into their learning. To address this, we introduced two methods to do just this. In chapter \ref{chap:p1} we introduced a method to learn a switching controller that can combine hand designed and learned controllers to solve a difficult nonlinear control problem. Later, in chapter \ref{chap:p5} we introduced a method to allow reinforcement learning agents to make use of gradient information from a new class of differentiable simulator.

We also claimed that RL is difficult to trust and analyze, to address this we turned to previously developed mesh based tools. These tools allow us to analyze the robustness of locomotion policies under a given set of disturbances, but they suffer from the curse of dimensionality, which limits their use to lower dimensional systems. We introduced a method to train RL policies in such a way that lowers their mesh dimensionality, and analyzed these policies. We then showed that this reduced dimensionality also reduced the size of meshes for the reachable states of these systems, which is what is required to use meshing tools to analyze robustness. Finally we extended these results to fine tuning of arbitrary neural network policies. We show that a simple policy search can be used to shrink the mesh dimension of DNN policies, and that this sort of policy refinement is useful for increasing reward and lowering variance in policy rollouts across a wide variety of problems, even without the mesh dimension reward. 

Reinforcement learning shows great promise for robotic control, and this work represents a step toward more reliable, safe, and performant controllers for the robotic systems of tomorrow.