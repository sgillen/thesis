\section{Introduction}
   
    

    As we have discussed, modern RL algorithms have a serious draw back in that they are mostly black boxes. It is an open challenge to figure out what exactly it is that your RL agent has learned. If all you know is that one of your agents achieved very high reward, it is not clear how to verify that this system is safe and sensible in all the regions of state space it will visit during its life. Nor can we necessarily say anything about the stability or robustness properties of the system. Recent work ~\cite{Taleledeep} has used so-called mesh-based tools to examine precisely these questions. 
    
    However, utility of any mesh-based tool to accurately discretize a state-space is limited, due to the curse of dimensionality. In practice, these methods are only able to work on relatively high dimensional systems if the reachable space grows at a rate that is much smaller than the exponential growth of the full state space the system within which it is embedded. To expand these methods to higher dimensional systems. We will need to find ways to keep the volume of visited states from expanding commensurately. One way to quantify this rate of growth is by using one of the several notions of "fractional dimensions" from fractal geometry.
    
    In this chapter, we discuss an efficient meshing algorithm, which we call box meshing. We show that this approach makes calculating the so called mesh dimension feasible in the context of reinforcement learning. We also propose using other notions of fractional dimension from the literature as a proxy for the property we care about. We then show that reinforcement learning agents can be trained to shrink these measures by post processing their reward function. We present the results of this training, and finally present some brief analysis of the resulting structure for select policies.  


    

\section{Meshing \& Fractional Dimensions}


\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/acc_fractal/Fractaldimensionexamplebw.png}
    \caption{Scaling in different dimensions}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/acc_fractal/Snow_Mesh_Example.png}
    \caption{A non uniform mesh}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/acc_fractal/Snow_LineFitAnn.png}
    \caption{Calculating the mesh dimension}
  \end{subfigure}
  \caption{Image credit for sub-figure a: \cite{BrendanRyan/Publicdomain2020}, image credit for sub-figures b and c: \cite{Talelepush}}
  \label{fig:fracdim}
\end{figure}

% \begin{wrapfigure}{L}{0.3\textwidth}
% \centering
% \includegraphics[width=0.3\textwidth]{fig/corl2020/HalfCheetahRews.png}
% \caption{\label{fig:frog2} Half Cheetah Reward Curves.}
% \end{wrapfigure}

%(??) is there other work I should be citing here?
%(??) I feel like this section is already too long, but also if I don't explain the pertubation style meshing people will misunderstand what we are tying to do and what "meshing" is?
Let's say we have a continuous set S that we want to approximate by selecting a discrete set M  composed of regions in S. We will call this set M a mesh of our space. Figure \ref{fig:fracdim}(a) shows some examples of this: a line is broken into segments, a square into grid spaces, and so on. The question is: as we increase the resolution of these regions, how many more regions N do we need? Again, figure \ref{fig:fracdim}(a) shows us some very simple examples. For a D dimensional system, if we go from regions of size d to d/k, then we would expect the number of mesh points to scale as $N \propto k^{D}$. But not all systems will scale like this, as \ref{fig:fracdim}(b) and \ref{fig:fracdim}(c) illustrate. Figure \ref{fig:fracdim}(b) is an example of a curve embedded in a two dimensional space. The question of how many mesh points are required must be answered empirically. Going backwards, we can use this relationship to assign a notion of "dimension" to the curve. 

\begin{equation}
    D_{f} = -\lim_{k \rightarrow 0}\frac{\log N(k)}{\log k} 
\end{equation}

What we are talking about is called the Minkowskiâ€“Bouligand dimension, also known as the box counting dimension. This dimension need not be an integer, hence the name "fractional dimension". As a practical matter, we use the slope of the log-log plot of mesh sizes over d to calculate this, rather than taking a limit. This is one of many measures of "fractional dimension" that that emerged from the study of fractal geometry. Although these measures were invented to study fractals, they can still be usefully applied to non-fractal sets.

In \cite{OguzSaglam2015}, Saglam and Byl introduced a technique that is able to simultaneously build a non-uniform mesh of a reachable state space while developing robust policies for a bipedal walker on rough terrain. Having a discrete mesh allows for value iteration over several candidate controllers, which found a robust control policy. In addition this mesh allows for the construction of a state transition matrix, which was used to calculate the mean first passage time \cite{Byl2009}, a metric that quantifies the expected number of steps a meta-stable system can take before falling. 

Since its introduction, meshing in this fashion has been used for designing walking controllers robust to push disturbances \cite{Talelepush}, to design agile gaits for a quadruped \cite{Byl2017}, and to analyze hybrid zero dynamics (HZD) controllers \cite{Saglam2015}. There has also been recent work to use these tools to analyze policies trained by deep reinforcement learning \cite{Taleledeep}. A long term goal and motivation for this work is to take a high performance controller obtained via reinforcement learning, and extract from it a mesh-based policy that is both explainable and amenable to analysis.

\todo{Can add some more intro, explanation, use figures from the GSCubed talk}


\subsection{Box Meshing}
\label{sec:boxmsh}
Our primary improvement to the prior work on meshing is to introduce something we call box meshing. Prior, a new mesh point could take any value in the state space. To determine if a new state is already in the mesh, we would compute a distance metric to every point in the mesh, and check if the minimum was below our threshold. Thus, building the mesh was an $O(n)^{2}$ algorithm. By contrast, in box meshing we apriori divide the space uniformly into boxes with side length $d$. We identify any state s with a key obtained by: $\text{key} = \text{round}(\frac{s}{d})d$, where round performs an element-wise rounding to the nearest integer. We can then use these keys to store mesh points in a hash table. Using this data structure, we can still store the mesh compactly, only keeping the points we come across. However, insertion and search are now $O(1)$, and so building the mesh is $O(n)$. This is very similar to non-hierarchical bucket methods, which are well studied spatial data structure \cite{Samet1990}, although we are using them for data compression here. In the prior meshing work, this sort of speedup would be minor, the run-time is dominated by the simulator or robot. However, this speedup does open some new possibilities: most poignantly, it makes calculating the mesh dimension during reinforcement learning plausible. 


\subsection{Algorithmic Box Mesh Dimension}
\label{sec:boxdim}
The "mesh dimension" is the quantity extracted from the slope of the log log plot of mesh sizes vs d values. For this paper, it is assumed that the mesh algorithm being used for this calculation is the box mesh. Automatically computing the mesh dimension of a data set generated from learning agent with speed, accuracy, and robustness is very challenging. A single trajectory provides only a small amount of data, which adds significant noise to the mesh sizes. Agents might do things like fall over and generate extremely short trajectories, or learn a trajectory that "stands in place", which can lead to numerical errors. Finally, every decision is a trade-off between accuracy and speed. Model-free RL is predicated on having a huge number of rollouts to learn from, and we would like for any mesh-dimension quantification algorithm to be fast enough so as to not dominate the total learning time. With these factors in mind, we introduce two box mesh dimensions. The lower mesh dimension does the linear fit, but intentionally errs on the side of including flat parts of the graph, and therefore tends to underestimate the true mesh dimension. We then have the upper mesh dimension, which takes the largest slope in the log log relationship, thus tending to overestimate the true mesh dimension. Neither of these measures are correct, but taken together they can bound the mesh dimension, and as we will see they can be useful on their own.


\subsection{Mesh Dimension Examples}

Figure \ref{fig:curves} illustrates two examples of the curves used to compute the mesh dimension. Recall that to compute the mesh dimension, we choose several values for d, the box length, and for each d construct a mesh using that box size. The x axis of these plots represents the log of the box length used, the y axis represents the log of size of the mesh created. For each curve, we display the lower bound and upper bound for the dimension as computed by algorithm 2, as well as several hand fits of the data. We hope that figure \ref{fig:curves}a makes clear what a close to ideal situation looks like, and provide intuition as to why the upper and lower mesh dimension bound the quantity we are trying to measure. Figure \ref{fig:curves}b serves to illustrate some of the problems with making an algorithmic measure of the dimension. There is much less data to work with due to performance constraints, which causes a large amount of noise on the estimate of the mesh dimension. Indeed even fitting this data by hand becomes a challenge and we provide two fits which can both be argued to be "correct". Which of these two represents the quantity we care about depends on the exact system being used and the purpose of the meshes we want to build with the resulting policy.


\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/meshdim.png}
    \caption{High resolution curve of a well behaved policy}
  \end{subfigure}
%   \begin{subfigure}[b]{0.32\textwidth}
%     \includegraphics[width=\textwidth]{fig/corl2020/meshdimhand.png}
%     \caption{Hopper-v2}
%   \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/meshdimreal.png}
    \caption{Run time resolution curve of a typical policy}
  \end{subfigure}
  \caption{Mesh curve and mesh dimension examples}
  \label{fig:curves}
\end{figure}



% !!! may or may not get included
% \subsection{PCA}

% In higher dimensions, the number of mesh points needed to represent data becomes unacceptably large, especially data that has low dimensional structure. To combat this we do a principle component decomposition of a nominal trajectory, while meshing the states, we project each state into the top l PCA values, and crucially, we then scale each projected dimension by the associated normalized singular values. This in effect gives our mesh more resolution in the important dimensions, while allowing less important dimensions to take on a coarser mesh. more formally if the singular value decomposition is given by:


% \begin{equation}
% X = U \Sigma W^{T}
% \end{equation}
% Then our projected coordinates are:

% \begin{equation}
% T = X W \frac{\Sigma}{|\Sigma|}
% \end{equation}


\begin{algorithm}
\begin{algorithmic}[1]
%\KwResult{Mesh Table, Deterministic State Transition Matrix}
\State \textbf{Input:} State set $S$, box size d.
\State \textbf{Output:} Mesh size m.
\State \textbf{Initialize:} Empty hash table M.
 \For{s $\in$ S}
    \State $\bar s  = \text{Normalize(s)} $
    \State key = round($\bar s$ / d)d
    \If{s $\in$ M}
       \State  M[key]++
    \Else
        \State M[key]=1
    \EndIf
\EndFor
\State \textbf{Return:} M
\end{algorithmic}
\caption{Create Box Mesh, see section \ref{sec:boxmsh}}
\end{algorithm}


\begin{algorithm}
\begin{algorithmic}[1]
%\KwResult{Mesh Table, Deterministic State Transition Matrix}
\State \textbf{Input:} State set $S$ 
\State \textbf{Output:} Mesh M.
\State \textbf{Hyperparameters:} scaling factor f,  initial box size $d_{0}$.
\State \textbf{Initialize:} Empty list of mesh sizes H, empty list of d values D.
\State m = Size(CreateBoxMesh(S, $d_{0}$)) 
\State d = $d_{0}$
\State Append m to H, append d to D.
\While{m $<$ size(S)}
    \State d = d/f 
    \State m = Size(CreateBoxMesh(S, d)) 
    \State Prepend m to H, prepend d to D. 
\EndWhile
\While{m $\neq$ 1}
    \State d = d*f  
    \State m = Size(CreateBoxMesh(S,d)) 
    \State Append m to H, append d to D. 
\EndWhile

\State X =  $\log$ d 
\State Y =  -$\log$ m

\textbf{Lower Mesh Dim:} fit Y = gX + b, \textbf{Return:} g \\
\textbf{Upper Mesh Dim:} w = greatest slope in Y over X \textbf{Return:} w

\caption{Compute Box Mesh Dimension, see section \ref{sec:boxdim}}
\label{algo:mesh_dim}
\end{algorithmic}
\end{algorithm}


\subsection{Variation Estimators}
\label{sec:var}
As discussed, computing the mesh dimension automatically is fraught with peril, in many practical scenarios. But there are also many other, different metrics one might consider, to give various approximations to the fractional dimension we seek to estimate. Gneiting et al.~\cite{Gneiting2012} compare a number of these estimators, and submit that the variation estimator \cite{Emery2005} offers a very good trade off between speed and robustness. To obtain this estimator, first define the power variation of order p as:

\begin{equation}
P_{p}(X, l) = \frac{1}{(2n-l)}\sum_{i=l}^{n} | X_{i} - X_{i-l}|^{p}
\end{equation}

Then, we define the variation estimator of order p as:

\begin{equation}
Dv_{p}(X) = 2 - \frac{\log P_{p}(X,2) - \log P_{p}(X, 1)}{p\log 2}
\label{eq:var}
\end{equation}

The \textbf{madogram} estimator is the special case of \eqref{eq:var} where p = 1, and the \textbf{variogram} is where p = 2.





% Why not use the thing we actually want to reduce? As we've mentioned, measuring the mesh dimension automatically is tricky (again see implementation details in the appendix). But with the faster meshing, it becomes feasible to use the mesh dimension directly, in the same manner we used the variation dimensions before. Here we introduce a second measure of mesh dimesion, called the "conservative mesh dimension". Recall that the normal mesh dimension is the least squares fit of a line to the log log plot of the mesh size vs the box width. The conservative mesh dimension is defined as the steepest slope found on this graph, simple as! This can be seen as an upper bound on the increases that we can expect while decreasing our mesh size. Table \ref{tab:mesh} shows the dimensions and reward after training.

\subsection{Post Processing Rewards}

In order to influence the dimensionality of the resulting policies, we introduce various postprocessors, which act on the reward signals before passing them to the agent. These obviously modify the problem: in some sense the postprocessed environment is a completely different problem from the original. However our meta-goal is to train agents that achieve reasonable rewards in the base environment, while simultaneously exhibiting reduced dimensionality we are looking for. These postprocessors take the form:
\begin{equation}
R_{*}(\vec{s}, \vec{a}) = \frac{1}{D_{*}(\vec{s})}\sum_{t=0}^{T} r(s_{t}, a_{t}, s_{t+1})
\end{equation}

Where $\vec{s}, \vec{a}$ are understood to be an entire trajectory of state action pairs, and $D_{*}$ is some measure of fractional dimension. Some measures of dimensionality can be inserted here directly (See \ref{sec:var}). However the mesh dimensions computed by algorithm \ref{algo:mesh_dim} require a little more care. We must first define a clipped dimension:

\begin{equation}
D^{c}_{*} = \text{clip}[D_{*}(\vec{s}_{t > Tr}), 1, D_{t}/2)] 
\end{equation}

where $D_{t}$ is the topological dimension, equal to the number of states in the system. $Tr$ is a fixed timestep chosen to exclude the initial transients resulting from a system moving from rest to into a quasi-cyclical ``gait''. In this paper we set $Tr$ = 200 for all experiments. For comparison, the nominal episode length is 1000. The clipping is to ensure that the pathological trajectories that and RL agent sometimes generates don't interfere with the training. It will also clip trajectories that terminate early, to prevent agents learning to fall over immediately to ``game the system''. Half of the topological dimension proved to be a decent upper bound for the worst case dimensionality of each system in practive. The \textbf{mesh dimension postprocessors} use the clipped dimension. Finally, when $D_{*} = 1$ is used, we call the result is the \textbf{identity post processor}, since in this case the total reward is completely unchanged.

\subsection{Environments}
We examine a subset of the popular OpenAI Mujoco locomotion environments introduced in \cite{1606.01540}. In particular, we evaluate our work on HalfCheetah-v2, Hopper-v2, and Walker2d-v2. These environments were chosen because they have a relatively high dimensionality (11-17 DOF), yet we believe can be made feasible for meshing based approaches. The state space consists of all joint / base positions and velocities, with the x (the "forward") position being held out, because we want a policy that is invariant along that dimension.


\begin{figure}
  \centering
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/cheetah_crop.png}
    \caption{HalfCheetah-v2}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/hopper_crop.png}
    \caption{Hopper-v2}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/walker2d_crop.png}
    \caption{Walker2d-v2}
  \end{subfigure}
  \caption{The Mujoco locomotion environments}
  \label{fig:envs}
\end{figure}


\subsection{Augmented Random Search}

\todo{may move this to intro or background}

 In \cite{Mania2018} Mania et al introduce Augmented Random Search (ARS) which proved to be efficient and effective on the locomotion tasks. Rather than a neural network, ARS used static linear policies, and compared to most modern reinforcement learning, the algorithm is very straightforward. The algorithm operates directly on the policy weights, each epoch the agent perturbs it's current policy N times, and collects 2N rollouts using the modified policies. The rewards from these rollouts are used to update the current policy weights, repeat until completion. The algorithm is known to have high variance; not all seeds obtain high rewards, but to our knowledge their work in many ways represents the state of the art on these benchmarks. Mania et al introduce several small modifications of the algorithm in their paper, our implementation corresponds to the version they call ARS-V2t.


% \begin{wrapfigure}{R}{0.3\textwidth}
% \centering
% \includegraphics[width=0.25\textwidth]{fig/corl2020/linearz.png}
% \caption{\label{fig:frog1}This is a figure caption.}
% \end{wrapfigure}

% Linearz. We made up a small toy system that exhibited some of the properties we were looking for in our policies. The dynamics are:

% \begin{equation}
%     \dot x = u_{1} , \dot y = u_{2}, \dot z = x
% \end{equation}

% What are the challenges of this environment? it has a dummy dimension, it blows up... 

% \begin{figure}[h!]
%   \centering
%     \includegraphics[width=.25\textwidth]{fig/corl2020/linearz.png}
%     \caption{HalfCheetah}
% \end{figure}

\subsection{Training}
\label{sec:training}

 We hadn chose parameters that were found to work well across all environments, with the parameters reported in Table 9 from \cite{Mania2018} as a starting point. We tuned until our unprocessed learning achieved satisfactory results across all tasks. Again, ARS is known to have high variance between random seeds, and some seeds never learn to gather a large reward. The parameters we found are able to consistently solve the cheetah and walker; for the hopper, the algorithm learns a policy with high reward around half the time. This seems consistent with the performance reported in \cite{Mania2018}. We train each postprocessor on 10 random seeds, the evaluation metrics are averages over 5 rollouts from each seed, and for the dimension metrics we use extended episodes of length 10,000 to get a more accurate measurement. The reported returns, and the training, both use the normal 1,000 step episodes. We found that the mesh postprocessors were getting very poor performance when trained from a random policy. However, we found that we saw good results when these trials were initialized with a working policy. Therefore we trained agents for 750 epochs without post processing, and used that to initialize the mesh dimension policies. The mesh policies were then trained for an additional 250 epochs and the results reported.
%===============================================================================



\section{Results}
\subsection{Mesh Dimension Postprocessors}

\begin{figure}[!htb]
  \centering
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/mesh4_curves/cheetah.png}
    \caption{HalfCheetah}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/mesh4_curves/hopper.png}
    \caption{Hopper}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/mesh4_curves/walker.png}
    \caption{Walker}
  \end{subfigure}
  \caption{Reward curves for mesh dimension postproccesor runs.}
  \label{fig:mesh_rews}
\end{figure}

\begin{table}[!htb]
\begin{tabular}{ l|l|l|l|l }
\hline
Environment & Postprocessor 
                  & Lower Mesh Dim.         & Upper Mesh Dim.   & Return \\ 
\hline
\multirow{3}{2.6cm}{HalfCheetah-v2} 
& Identity        & 2.31 $\pm$ 0.71   & 7.34 $\pm$  1.56 &  5469 $\pm$ 823 \\
& Lower Mesh Dim        &  \textbf{0.66 $\pm$ 0.51}   & \textbf{2.55 $\pm$  1.52} &  4962 $\pm$ 598 \\
& Upper Mesh Dim.  & \textbf{1.06 $\pm$ 1.13}  & \textbf{2.83  $\pm$ 1.27} &  4432 $\pm$ 539 \\
\hline
\multirow{3}{2.6cm}{Hopper-v2} 
& Madogram$^{*}$        & 1.62 $\pm$ .27   & 4.68  $\pm$  0.82   &  3461 $\pm$ 119 \\
& Lower Mesh Dim.        & 1.13 $\pm$ .02   & 3.54 $\pm$  0.96   &  2941 $\pm$ 538 \\
& Upper Mesh Dim.  & 1.27 $\pm$ .50   & 2.98 $\pm$  1.48   &  3020 $\pm$ 337 \\
\hline
\multirow{3}{2.6cm}{Walker2d-v2 \\ (walking seeds)$^{**}$}
& Identity        & 2.13 $\pm$  0.31    & 4.62 $\pm$ 1.03  &  3758 $\pm$ 1037  \\
& Lower Mesh Dim.       & 1.21 $\pm$  0.06    & 4.09 $\pm$ 1.03  &  3339 $\pm$ 887   \\
& Upper Mesh Dim. & 1.89 $\pm$  0.42    & 3.10 $\pm$ 0.93  &  3359 $\pm$ 903 \\
\hline
\multirow{3}{2.6cm}{Walker2d-v2 \\ (all seeds)$^{**}$ }
& Identity        & 2.13 $\pm$ 0.31   & 4.62 $\pm$ 1.03    &  3758 $\pm$ 1037 \\
& Lower Mesh Dim.       & 1.04 $\pm$ 0.53   & 4.45 $\pm$ 1.19    &  3034 $\pm$ 1086 \\
& Upper Mesh Dim. & 1.48 $\pm$ 0.67   & 2.27 $\pm$ 0.95    &  2556 $\pm$ 1378 \\
\hline
\end{tabular}
\caption{\label{tab:mesh} Mesh dimensions and returns for trajectories after training. See \ref{sec:training} for details\\
%% \footnotesize{*  Because ARS with our chosen hyper parameters does not consistently produce 10 seeds that perform well on the hopper, we instead use madodiv (see the \ref{sec:var}) for the seed policies.  \\
%% **  See \ref{sec:msh}}
}
\end{table}

For all environments the mesh post processors had a significant impact on the mesh dimensions. It's important to remember here, the dimensions reported represent lower and upper bounds for the actual mesh dimensions. There was also a corresponding and significant decrease in the unprocessed rewards. However with our meta goal of training agents that have acceptable reward but which are more amenable to meshing, this is a more than acceptable trade. In the case of walker, several seeds (4 for the upper dim., 3 for the lower mesh dim.), "forget how to walk", and learn a policy that stands in place. This certainly has a low dimensionality, but is not very useful, to be complete we include statistics from the seeds that learned a gait, and for all 10 seeds, including the standing policies.


\subsection{Variational Postprocessors}


\begin{figure}[!htb]
  \centering
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/data17_rews/cheetah.png}
    \caption{HalfCheetah}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/data17_rews/hopper.png}
    \caption{Hopper}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/data17_rews/walker.png}
    \caption{Walker}
  \end{subfigure}
  \caption{Reward curves for the variation postprocessors}
  \label{fig:rews}

\end{figure}
\begin{table}
\begin{tabular}{ l|l|l|l|l|l }
\hline
Environment & Postprocessor 
                    & Variogram        & Madogram       & Lower Mesh Dim.   & Return \\ \hline
\multirow{3}{*}{HalfCheetah-v2} 
& Identity          & 1.71 $\pm$ .03   & 1.42 $\pm$ .05 &  2.36 $\pm$ .61 & 5545 $\pm$ 593  \\
& Variogram   & 1.68 $\pm$ .01   & 1.36 $\pm$ .02 &  2.06 $\pm$ .60 & 5136 $\pm$ 851\\
& Madogram    & 1.65 $\pm$ .02   & 1.31 $\pm$ .04 &  2.09 $\pm$ .64 & 5234 $\pm$ 950\\

\hline
\multirow{3}{*}{Hopper-v2} 
& Identity$^{*}$           & 1.61 $\pm$ .14  & 1.22 $\pm$ .28               &  1.03$^{*}$ $\pm$ .71 & 2063 $\pm$ 1052 \\
& Variogram   &  \textbf{1.51 $\pm$ .02}  & \textbf{1.03 $\pm$ .04}   &  1.58 $\pm$ .54 & 3299 $\pm$ 711 \\
& Madogram     & \textbf{1.51 $\pm$ .002}  & \textbf{1.02 $\pm$ .004} &  1.57 $\pm$ .36 & 3449 $\pm$ 146\\
\hline
\multirow{3}{*}{Walker2d-v2} 
& Identity           & 1.68 $\pm$ .35  & 1.36 $\pm$ .71 &              2.14 $\pm$ .29 & 3742 $\pm$ 1038\\
& Variogram    & \textbf{1.54 $\pm$ .07}  & \textbf{1.07 $\pm$ .01} &  1.85 $\pm$ .54 & 3779 $\pm$ 894 \\
& Madogram     & \textbf{1.53 $\pm$ .01}  & \textbf{1.06 $\pm$ .02} &  1.99 $\pm$ .53 & 3414 $\pm$ 1025\\
\hline
\end{tabular}
\caption{\label{tab:dims} Mesh dimensions and returns for trajectories after training. See \ref{sec:training} for details\\
\footnotesize{* This includes policies which learned to "stand still", which lowers the average mesh dimension considerably see discussion}
}
\end{table}

It seems that the variational postprocessors had a modest effect the variational dimension, but that does not seem to correlate to a smaller mesh dimension, despite what our preliminary tests had led us to believe. The hopper and walker did have remarkable consistency in the variation dimensions they found; possibly this could be used to lower the variance in ARS. The fact that the variogram and madogram also got higher performance on the hopper task could support this claim. However without running many more trials and hyper parameter sweeps, that's not a claim that can be substantiated. These experiments show that 1) measures for fractional dimension can be influenced without adversely effecting the reward, and 2) that it is possible for an agent to shrink it's variogram and madogram dimensions without a large impact on its mesh dimension. 






\label{sec:msh}

\section{Analysis}

We now examine the learned behavior for one of the more notable policies. By far the most dramatic effect from the tables above was the mesh dimension postprocessors on the cheetah. Both measures of dimension shrunk by 2-4 times.
Figure 7 presents data for this case.

% \begin{figure} (!! there must be a better way to display this figure)
% \centering
% \includegraphics[width=0.4\textwidth]{fig/corl2020/cheetah_analysis/meshes.png}
% \caption{\label{fig:frog2} Half Cheetah Reward Curves.}
% \end{figure}

% \begin{wrapfigure}{L}{0.4\textwidth}
% \centering
% \includegraphics[width=0.4\textwidth]{fig/corl2020/cheetah_analysis/meshes.png}
% \caption{\label{fig:frog2} Half Cheetah Reward Curves.}
% \end{wrapfigure}


\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{fig/corl2020/cheetah_mesh_cor.png}        
    \caption{Top: mesh sizes vs log of the box size for the cheetah environment. Lower Left: Every five frames overlaid for the an identity policy on the cheetah. Lower Right: Every five frames of cheetah after the lower mesh dimension training.}
    \label{fig:mesh_anal}
\end{figure}


% \begin{figure}[!htb]
%   \begin{subfigure}[b]{0.88\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{fig/corl2020/cheetah_mesh_cmp.png}
%     \end{subfigure}
%   \begin{subfigure}[b]{0.4\textwidth}
%     \includegraphics[width=\textwidth]{fig/corl2020/ident_cheetah_crop.png}
%   \end{subfigure}
%  \centering
%   \begin{subfigure}[b]{0.4\textwidth}
%     \includegraphics[width=\textwidth]{fig/corl2020/mdim_cheetah_crop.png}
%   \end{subfigure}
%   \caption{Left: mesh size comparisons across the post processors. middle: Trajectory after training normal ARS for 750 epochs. Right: Trajectory after training 250 additional epochs with the mesh dimension post processor}
% \end{figure}

Toward more intuitively understanding this data, a few comments are worth making, first. We have discussed the mesh dimension rather abstractly so far. In visualizing what this really means, imagine two different gait cycles. In one case, there is a general pattern to the motion, but it wanders in a noisy-looking way, like a ``signature'' that does not quite match up, cycle after cycle.  As motions become closer to being exact limit cycles, there is a more clear pattern of repetition, exactly analogous to re-tracing the same path, again and again, within the state space. Such a more tightly-structured limit cycle nature in turn results in a significantly lower-dimensional set of states being visited. 

We can see from the mesh size curves in Figure 7 that there is an overwhelming difference in the mesh sizes between the lower mesh dimension post processor and the other two. To put this in perspective, before the extra 250 epochs of training, if given a box size of .01, the agent would need a unique mesh point for every single state in the 10,000 state trajectory. After the additional training, the agent can represent all 10,000 points with just 5 mesh points! In this case it appears both agents learned a quasi periodic gait with period 5. In figure \ref{fig:mesh_anal} we present an overlay of the agents rendered every 5 steps. The results show us that the mesh agent has learned an extremely tight limit cycle. It's a bit of a strange limit cycle, being only 5 timesteps long, but nonetheless we think this is interesting and surprising behavior.


% action std = .01
% HalfCheetah-v2:
% |          | mesh dimension | cmesh dimension |
% | identity |           2.33 |            6.78 |
% | mdim_div |           1.50 |            3.69 |

% Hopper-v2:
% |          | mesh dimension | cmesh dimension |
% | madodiv  |           1.64 |            4.52 |
% | mdim_div |           1.44 |            3.60 |

% Walker2d-v2:
% |          | mesh dimension | cmesh dimension |
% | identity |           2.06 |            4.42 |
% | mdim_div |           1.50 |            4.15 |



% State std = .005
% HalfCheetah-v2:
% |          | mesh dimension | cmesh dimension |
% | identity |           2.39 |            6.78 |
% | mdim_div |           1.76 |            3.69 |

% Hopper-v2:
% |          | mesh dimension | cmesh dimension |
% | madodiv  |           2.25 |            4.52 |
% | mdim_div |           1.89 |            4.18 |

% Walker2d-v2:
% |          | mesh dimension | cmesh dimension |
% | identity |           2.29 |            4.59 |
% | mdim_div |           1.89 |            4.36 |



The behavior displayed in figure \ref{fig:mesh_anal} is clearly something that can only happen in a noiseless simulation, so we also measured the mesh dimensions of our policies when subjected to noise during rollouts using the noise values from the robustness experiments (see table \ref{tab:robust})to inform parameter values. The difference in fractal dimension is less pronounced than the no noise case but is still a clear improvement. Furthermore we expect that if we were to add noise at training time, that the learning may be able to find ways to lower the mesh dimension that is more robust to noise.

\begin{table}
\begin{tabular}{ l|l|l|l|l }
\hline
Environment & Postprocessor 
                  & Lower Mesh Dim.         & Upper Mesh Dim.   & Return \\ 
\hline
\multirow{3}{2.6cm}{HalfCheetah-v2} 
& Identity              &  2.38   $\pm$ 0.43   & 6.65 $\pm$  1.90 &  5404 $\pm$ 1015 \\
& Lower Mesh Dim        &  1.51   $\pm$ 0.13   & 3.03  $\pm$  1.09 &  4952 $\pm$ 572 \\
& Upper Mesh Dim.       & 1.76    $\pm$ 0.53      & 3.54  $\pm$  1.27 &  4222 $\pm$ 803 \\
\hline
\multirow{3}{2.6cm}{Hopper-v2} 
& Madogram$^{*}$         & 1.63 $\pm$ .14   & 4.49 $\pm$  0.75   &  3438 $\pm$ 185 \\
& Lower Mesh Dim.        & 1.67 $\pm$ .22   & 3.71 $\pm$  0.89   &  2943 $\pm$ 535 \\
& Upper Mesh Dim.        & 1.64 $\pm$ .16   & 3.01 $\pm$  1.36   &  3019 $\pm$ 337 \\
\hline
\multirow{3}{2.6cm}{Walker2d-v2 \\ (walking seeds)$^{**}$}
& Identity        & 2.13 $\pm$  0.31    & 4.62 $\pm$ 1.03  &  3758 $\pm$ 1037  \\
& Lower Mesh Dim. & 1.83 $\pm$  0.34    & 2.73 $\pm$ 0.75  &  3511 $\pm$ 872   \\
& Upper Mesh Dim. & 1.60 $\pm$  0.33    & 4.01 $\pm$ 1.18  &  3384 $\pm$ 903 \\
\hline
\multirow{3}{2.6cm}{Walker2d-v2 \\ (all seeds)$^{**}$ }
& Identity        & 2.10 $\pm$ 0.34   & 4.42 $\pm$ 1.00    &  3743 $\pm$ 1034 \\
& Lower Mesh Dim. & 1.68 $\pm$ 0.70   & 4.19 $\pm$ 1.25    &  3048 $\pm$ 1071 \\
& Upper Mesh Dim. & 1.48 $\pm$ 0.38   & 2.98 $\pm$ 0.86    &  2558 $\pm$ 1373 \\
\hline
\end{tabular}
\caption{\label{tab:mesh} Mesh dimensions and returns for trajectories subject to zero mean Guassian noise. Standard deviation of .001 and .01 was added to all actions and observations respectively. See \ref{sec:training} for details\\
\footnotesize{Because ARS with our chosen hyper parameters does not consistently produce 10 seeds that perform well on the hopper, we instead use madodiv (see the \ref{sec:var}) for the seed policies.  \\
**  See \ref{sec:msh}}
}
\end{table}

We also performed analysis on the robustness properties of the resulting polices. We tested the failure rate for agents in the presence of noise and disturbances, figure \ref{tab:robust} shows the results. It's worth noting at this point that in practice the lower mesh dimension seems to work better in practice than the upper one. We found that when computing the mesh dimension by hand (by hand fitting a line to a set of carefully obtained mesh size data) that the hand picked value was generally much closer to the lower mesh dimension, at least for the three systems we studied. Training with the lower mesh dimension also resulted in agents that were more robust and achieved higher reward compared to the upper dimension.  

We tested three different cases, adding zero mean Gaussian noise to the actions and observations, and adding a force disturbance to the center of mass of the agents during their rollouts. For the push disturbances we have two parameters, the rate of disturbances, and the mangitude of the force applied. At every step we sample uniformly from [0,1], if the result is less than the rate parameter, then a force is applied at that timestep. The force is applied at a random angle in the xz plane, with the fixed magnitude from the magnitude parameter. For each type of disturbance we did a grid search over the parameters and report the parameter for which the identity post processor failed in around 20 percent of cases. Failure is defined as an early termination of an episode. 


\begin{table}
\centering
\begin{tabular}{ l|l|l|l }

\hline
            & Identity      & Lower mesh dim. & Action std  \\ 
\hline
Cheetah     &          0.24  &           0.05 & .05 \\
Hopper      &          0.19  &           0.10 & .05 \\ 
Walker      &          0.28  &           0.03 & .15 \\
\hline
\hline
            & Identity       & Lower mesh dim. & Observation std  \\ 
\hline
Cheetah     &          0.20  &           0.02 & .005 \\
Hopper      &          0.20  &           0.25 & .02 \\ 
Walker      &          0.18  &           0.10 & .03 \\
\hline
\hline
            & Identity       & Lower mesh dim. & Magnitude, Rate  \\ 
\hline
Cheetah     &          0.21  &           0.03 & 3, .2 \\
Hopper      &          0.17  &           0.10 & 1, .2 \\ 
Walker      &          0.20  &           0.00 & 1, .2 \\
\hline
\end{tabular}
\caption{\label{tab:robust} Failure rates for agents under various noise and push disturbances}
\end{table}


% \section{Related Work}
    
%     !! I think related work belongs at the end, we are mostly comparing to our labs work, and the distinction will be much clearer once the reader knows wtf we are talking about. I think I've seen other papers do this...
%     Outside our lab there really has not been that much work analysing the effects of fractal dimension on reinforcement learning agents. This \cite{Kaygisiz2001} work from back in 2001 does something similar to our meshing approach, and they examine the fractal microstructure of their mesh equivalent. But they make no attempt to actually modulate the fractal structure of the resulting policy. 
    
%     I think I need to mostly talking about our own labs work here? How this differs from it perhaps?


%===============================================================================

% \section{Future Work}

% The obvious next step is to use this to recreate some of the earlier work from our lab. Ideally these techniques will allow our mesh based methods to scale to even higher dimensional systems. There are also improvements that can be made to the meshing algorithm, it's unclear if using a more accurate measure of this during training will aid performance in any way.
    


\section{Conclusion}

In this work, we introduced a technique to influence the fractional dimension of the closed-loop dynamics of a system through the use of novel, dimensionality-based modifications to  the cost functions for reinforcement learning policies. We demonstrate this technique on several benchmark tasks, and we briefly analyze a resulting policy to verify the outcome, demonstrating a much smaller mesh dimension without a large loss in reward or function.

\label{sec:conclusion}
% \begin{center}
% \begin{tabu}{ X[1,l] | X[1,l] | X[1,l] | X[1,l] | X[1,l] }
% Environment & $\alpha$ & $\nu$ & $N$ & $b$ \\
%  \hline
%  \text{HalfCheetah}-v2 & 0.02 & 0.025 & 60 & 20 \\
%  \hline
% \end{tabu}
% \end{center}

%/





\subsection*{Hyper Parameters}

\textbf{ARS:} For all environments $\alpha = .02$, $\sigma = .025$, $N=50$, $b=20$.  \\
\textbf{MeshDim:} f = 1.5, $d_{0}$ = 1e-2



\subsection{Implementation Details}

For performance reasons, the mesh dimension algorithm does not actually create meshes until the mesh size equals the total data size, but rather until the mesh size is 4/5 the total data size. Figure 8a shows a typical mesh curve, and we can see the long tail of values with mesh sizes close to the maximum value. Not much useful information is gained from this and it is wasting time, so we stop early. We do not place the same limitation on the lower size of the mesh, since typically the mesh size hits one much more rapidly, again figure 8a illustrates this. In addition implement a minimum size for d, set to 1e-9 in this work to avoid numerical errors. 

The normalization done during box creation uses a running mean and standard deviation of all states seen so far during training. These stats are saved and used for evaluation as well, we found that the upper mesh dimension is very sensitive to the normalization used, but that the other metrics where not.



\section{Reachable State Space Mesh for the Hopper}


%is this work improving the control of legged systems or is it moving towards more trusted RL policies
% clearly both, but which to emphasize? should open with whichever one that is.

% Might go with the same "RL is growing in importance because of computers, but how can we trust it" intro that I used for both CDC and CORL

% Not sure we even need to talk about wheeled vehicles 

We have already discussed some prior work has using mesh based techniques to analayze control policies for robotic systems \cite{Taleledeep}. Broadly, these techniques take a continuous system and approximate it with a discrete set of states. This allows us to model the system as a Markov chain, these systems are arguably easier to reason about, and it opens up a new box of tools we can bring to bear on the problem. For example we can use value iteration to switch between several controllers to improve the robustness \cite{Talelepush} or agility \cite{Byl2017} of the system. We can also perform eigen-analysis on the Markov chain's transition matrix, which provides us insights on the stability of the system \cite{Byl2009}. These techniques could both be used for policy refinement, and/or for verification and analysis of existing policies. 

Unlike the previous section, where we considered meshes of a single trajectory, this prior work that studies a systems reachable state space mesh. That is, the set of states that can be reached with a fixed controller, and a known set of disturbances. Although in the previous section, each trajectory was encouraged to have a smaller fractal dimension, this does not obviously extend to properties of the entire reachable state space for the system, which is what was used for the previous mesh based analysis of RL policies. 

In this work we take the next step and construct reachable state space meshes of agents trained with and without our modified reward. Our primary contribution is showing that these modified policies result in significantly smaller reachable meshes for a given box size, and in smaller fractal dimensions for the reachable state space. We then use the modified policies to construct a much finer mesh than would be possible otherwise. We use this mesh to compute a quantity called the mean first passage time (MFPT), and validate the obtained MFPT with Monte Carlo trials. Finally we use our mesh to produce interesting visualizations of failure states, which motivates future work.

\section{The Hopper Benchmark System}

Our model system is openAI gym's Hopper-v2 environment introduced in \cite{1606.01540}. This environment is part of a popular and standardized set of benchmarking tasks for reinforcement learning algorithms. The system is a 4 link, 6 DOF hopper constrained to travel in the XZ plane, seen in Figure \ref{fig:hopper}. The observation space for the agent has 11 states, the position in the direction of motion is held out, since we seek a policy that is invariant to forward progress. The actions in this case are commanded joint torques. The reward function for this environment is simply forward velocity minus a small penalty to actions. Successful controllers in this environment must execute a dynamic hopping motion to move robot along the x axis as quickly as possible. This is clearly a toy problem, but it captures many of the challenges of legged locomotion. The system is highly non-linear, under-actuated, and must interact with friction and ground contacts to maximize it's reward.  


%Our meta goal here is to construct a controller that can successfully hop, but which is also stable to disturbances and amenable to meshing.

\begin{figure}
\centering
\includegraphics[width=.5\linewidth]{fig/icra2021//hopper_crop.png}
\caption{A render of the hopper system studied in this work.}
\label{fig:hopper}
\end{figure}


% \subsection{Membership functions}
% In all form of meshing, the goal is to represent a continuous set with a discrete approximation. To do this we need two things, one is a function which determines membership of a continuous state in our discrete set. The second is the method with which we obtain the continuous states that we wish to mesh. In most previous work \cite{previous lab work} membership was determined by a nearest neighbor lookup of all the points in the current mesh. 

% \begin{algorithm}
% \SetAlgoLined
% \textbf{Input:} New state $s_{i}$, box size $d_{thr}$, current mesh $\mathbb{M}$. \\
%     $d(s_{i}, M) := \min_{s_{j} \in \mathbb{M}} \sqrt{\sum_{k=0}^{N} (s_{i}(k) - s_{j}(k))^{2}}$ \\
%     \eIf{ $d(s_{i, M}) < d_{th}$}
%         {add $s_{i}$ to set of points represented by argmin !!}
%     {add new mesh point centered at $s_{i}$}
% \caption{Nearest Neighbor Mesh}
% \end{algorithm}

% \begin{algorithm}
% \SetAlgoLined
% \textbf{Input:} New state $s_{i}$, box size $d_{thr}$, current mesh $\mathbb{M}$. \\
%     $ \text{key}_{i} = \text{round}(s_{i}/d_{thr})d_{thr}$ \\
%     \eIf{ $\text{key}_{i} \in M$}
%         {add $s_{i}$ to mesh point argmin !!}
%     {$M = key_{i} \bigcup M$}
% \caption{Hash-Box Mesh}
% \end{algorithm}


\section{Meshing the Reachable State Space}

We are interested in the set of states that our system can transition to with a fixed policy and a given set of push disturbances. We first introduce a failure state to the mesh. The failure state is assumed to be absorbing, once the robot falls it is assumed to stay that way. For our hopper, any state where the COM falls below .7m is considered to have failed, which works well in practice. This is also the failure condition of the environment during training, and therefore the agent is never trained in regions of the state space that satisfy the failure condition. 

In addition to the reachable set of states, we want to construct a state to state transition map. That is, for a given initial state, we wish to know which state we transition to for every disturbance in our disturbance set. It's worth emphasizing that this map is completely deterministic.

To make this concrete, recall that we manifest our mesh as a hash table. The key for any given state is obtained by \ref{eq:key}. When we insert a new key into our hash table, the value we place is a pair with a unique state ID (which is simply the number of keys in the table at the time of insertion), and an initially empty list of all mesh states which are reachable after one step from the key state. This data structure will provide both the reachable set, and the transition mapping.

%Figure \ref{fig:traj} shows an example of this.

For the hopper in particular, the system transitions from its initial standing position to a stable long term hopping gait. After letting the system enter its gait, we start detecting states on the  Poincar\'e section by selecting the state corresponding to the peak of the base link's height in every ballistic phase. These states are then collected as the initial states to seed the mesh with. Throughout this paper, we seed the mesh with trajectories from 10 initial conditions. 

% \begin{figure}[!h]
% \centering
% \includegraphics[width=.8\linewidth]{fig/icra2021//initial_trajectory.png}
% \caption{First 300 time steps of a typical hopper trajectory. Vertical red lines indicate the location of Poincar\'e snapshots.}
% \label{fig:traj}
% \end{figure}


For each snapshot, we initialize the system in the snap-shotted state. For each disturbance in our fixed disturbance set, we simulate the system forward subject to that disturbance. If the system does not fail, then the next Poincar\'e snapshot is captured, this state is then checked for membership in our mesh. If the new state is already in our mesh, then we simply append the new state to the list of states that the initial state can transition to. If the new state is not already in our mesh, then we expand our mesh to include the new state, and append this new state to the transition list of the initial state. If the system does fail, then we simply append the failure state to the transition list of the initial state, and no new state is added to the mesh. 

For every new state added to the mesh, we repeat this process until every state has been explored. Algorithm \ref{algo:createMesh} details this process in pseudo code.

\begin{algorithm}
\begin{algorithmic}[1]
%\KwResult{Mesh Table, Deterministic State Transition Matrix}
\State \textbf{Input:} Initial states $S_{i}$, Disturbance set $D$
\State \textbf{Output:} Mesh M. 
\State Q $\leftarrow S_{i}$ (excluding the failure state) 
\While{Q not empty}
    \State pop q from Q 
    \For{d $\in$ D}
        \State Initialize system in state q 
        \State Run system for one step subject to disturbance d 
        \State Obtain final state x 
        \If{x $\notin$ M}
            \State M[x] = List()  
            \State Push x onto Q
        \EndIf
        \State Append x to M[q]
    \EndFor
\EndWhile
\State \textbf{Return:} M 
\end{algorithmic}
\caption{createMesh}
\label{algo:createMesh}
\end{algorithm}

\subsection{Stochastic Transition Matrix}

The stochastic transition matrix $\mathbf{T}$ is defined as follows:

\begin{equation}
 \mathbf{T}_{ij} = \text{Pr}(id[n+1] = j \ | \ id[n] = i)
\end{equation}

where $id[n]$ is the index in our mesh data structure of the state at step n. For some intuition, consider the transition matrix as the adjacency matrix for a graph. There is one row/column for every state in our mesh, for a given row i, each entry j is the probability of transitioning from state i to state j. Every row will sum to one, but the sum for each column has no such constraint. After constructing a mesh using algorithm \ref{algo:createMesh}, it is straightforward to create the stochastic transition matrix by iterating through every transition list in our mesh. 

\subsection{Mean First Passage Time}

We wish to use our mesh based methods to quantify the stability of our system. To do this we estimate the average number of steps the agent will take before falling, subject to a given distribution of disturbances. To do this we will use the so called Mean First Passage Time (MFPT) which in this case will describe expected number of footsteps, rather than the number of timesteps to failure. First recall that our assumption is that our failure state is an absorbing state in our Markov chain approximation, and this implies that the largest eigenvalue of \textbf{T} will always be $\lambda_{1} = 1$. In \cite{Byl2009} Byl showed that when the second largest eigenvalue $\lambda_{2}$ is close to unity, the MFPT is approximately equal to:

\begin{equation}
    MFPT \approx \frac{1}{(1-\lambda_{2}).}
    \label{eq:mfpt}
\end{equation}

% \subsection{Mesh Dimension}

% The question is: as we increase the resolution of these regions, how many more regions N do we need? Again, Figure \ref{fig:fracdim}(a) shows us some very simple examples. For a D dimensional system, if we go from regions of size d to d/k, then we would expect the number of mesh points to scale as $N \propto k^{D}$. But not all systems will scale like this, as \ref{fig:fracdim}(b) and \ref{fig:fracdim}(c) illustrate. Figure \ref{fig:fracdim}(b) is an example of a (rather famous) curve embedded in a two dimensional space. The question of how many mesh points are required must be answered empirically. Going backwards, we can use this relationship to assign a notion of "dimension" to the curve. 

% \begin{equation}
%     D_{f} = -\lim_{k \rightarrow 0}\frac{\log N(k)}{\log k} 
% \end{equation}




%\subsection{Reinforcement Learning}

% Not convinced we need this at all?


\section{Training the Hopper}

In \cite{Mania2018} Mania et al introduce Augmented Random Search (ARS) which proved to be efficient and effective on the locomotion tasks. Rather than a neural network, ARS used static linear policies, and compared to most modern reinforcement learning, the algorithm is very straightforward. The algorithm is known to have high variance; not all seeds obtain high rewards, but to our knowledge their work in many ways represents the state of the art on the Mujoco benchmarks. Mania et al introduce several small modifications of the algorithm in their paper, our implementation corresponds to the version they call ARS-V2t, hyper parameters are provided in the appendix.

The training process is done in episodes, each episode corresponds to 1000 policy evaluations played out in the simulator. At the start of each episode, the system is initialized in a nominal initial condition offset by a small amount of noise added to each state. During each episode we fix a static policy to let the the system evolve under, we collect the observed state, the resulting action, and the resulting reward at each timestep. This information is then used to update the policy for the next episode.

We compare four different sets of agents trained in different conditions, for each training condition we use training runs across 10 different random seeds. As mentioned ARS is a very high variance algorithm, so a common practice is to run many seeds in parallel and choose the highest performing one. The standard environment has two sources of randomness which are set by the random seed. The first is a small amount of noise added to a the nominal initial condition at the beginning of each episode. The second is noise added to the policy parameters as part of the normal ARS training procedure. Using ARS in the unmodified Hopper-v2 environment will be called the \textbf{standard} training procedure. In addition to this, we have a second set of agents which are initialized with the standard training, and then trained for another 250 epochs with the fractal reward function used in equation \ref{eq:frac_reward}, these are called the \textbf{fractal agents}. Using the standard training agents as the initial policies for the fractal reward was also used in \cite{Gillen2020ExplicitFractal}, please see that manuscript for more details. 

In addition to standard training, we repeat this standard / fractal setup but with the addition of a small amount of zero mean Gaussian noise added to both the states and actions at training time. For brevity we will call these the \textbf{Standard noise} and \textbf{Fractal noise} scenarios. Hyper parameters for ARS and noise values are reported in the appendix. 
\section{Results}

\subsection{Mesh Sizes Across All Seeds}
First we wish to compare the reachable state space mesh sizes obtained for these four different training regiments. For this we assume a disturbance profile consisting of 25 pushes equally spaced between -15 and 15 Newtons, applied for 0.01 seconds along the x axis at the apex of each jump. The goal for this particular exercise is to get an idea of the relative mesh sizes among the different agents across box sizes. Table \ref{table:mesh_sizes} shows these results. We can see that across all box sizes, adding noise at run time decreases the mesh sizes slightly, and that adding the fractal reward training decreases the mesh size even further. The combination of adding noise and the fractal reward seems to perform best at reducing the mesh size. 



\begin{table}[!htb]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|l|l|l|l|l|}
\hline 
Training         & $d_{thr} = .4$ & $d_{thr} = .3$ & $d_{thr} = .2$ & $d_{thr} = .1$ \\ \hline \hline
Standard         &    64.9       &     129.0      &   289.2        &  2975.2         \\  \hline
Standard Noise   &    40.7       &     73.3       &   231.6        &  2133.3         \\  \hline
Fractal          &    26.0       &     41.8       &   67.7         &  684.4          \\ \hline
Fractal Noise    &    15.1       &     24.6       &   45.1         &  297.2          \\ 
\hline
\end{tabular}

\caption{Mesh sizes across all seeds for a disturbance profile of 25 pushes. All values are the average mesh size across 10 agents trained with different seeds.}
\label{table:mesh_sizes}
\end{table}



% \begin{table}
% \centering

% \begin{tabular}{|l|l|l|l|l| }
% \hline
% Seed & Iden & Mdim       & Iden Noise & Mdim Noise \\ \hline
% 0    &      &   97319    &            &    1216         \\ 
% 1    &      &   3014     &            &     9774       \\ 
% 2    &      &   479919   &            &     4778       \\ 
% 3    &      &   4041     &            &     1434       \\ 
% 4    &      &   849      &            &     2445       \\ 
% 5    &      &   93067    &            &     3567       \\ 
% 6    &      &   25533    &            &     4667       \\ 
% 7    &      &   15428    &            &     1605       \\ 



% \hline
% \end{tabular}
% \end{table}


\subsection{Larger Meshes}

With the general trend established, we now take the best performing seed from the noisy training for further study. We chose the seed that had the smallest mesh size from both the standard noise and fractal noise agents. 

For this next experiment, we consider a richer distribution of 100 randomly generated push disturbances. These disturbances have a magnitude drawn from a uniform distribution between 5-15 Newtons. This force is applied in the xz plane with an angle drawn from a uniform distribution between 0 and 2$\pi$. The number of forces was chosen by increasing the number of forces sampled until the mesh sizes between two random sets did not change. The magnitude of the pushes was chosen arbitrarily, in principle one can use these methods for any distribution of disturbance they expect their robot to encounter during operation. 
% \begin{figure}[!h]
% \centering
% \includegraphics[width=.4\linewidth]{fig/icra2021//noise.png}
% \caption{Noise profile for detailed experiments on single seeds, magnitudes range between 5 and 15 newton meters}
% \label{fig:noise}
% \end{figure}

We then construct meshes for different box sizes. For each agent we construct 10 meshes. We vary the box size between 0.1 and 0.01 for the fractal noise agent. For the standard noise agent we instead vary the box size between 0.1 and 0.02 because the mesh sizes for the standard agent were proving to be too large at the smaller box sizes. Figure \ref{fig:mesh_cmp} shows the comparison, We can see clearly that at the very least, the exponential blowup in mesh size starts at much more accurate mesh resolutions for the fractal agent.

\begin{figure}[!htb]
\centering
\includegraphics[width=.7\linewidth]{fig/icra2021//mesh_size_cmp.png}
\caption{Mesh sizes for the top performing standard noise and fractal noise agents.}
\label{fig:mesh_cmp}
\end{figure}

We are also interested in the exponential scaling factor in the mesh size as the box gets smaller, which is captured by the fractal dimension discussed in section \ref{sec:fracdim}. As mentioned before, in previous work our modified reward signal resulted in agents with a smaller fractal dimension with respect to individual trajectories. We now ask if this carries over to meshes of the reachable state space obtained by the procedure from algorithm \ref{algo:createMesh}. Table \ref{table:mesh_dims} shows the results, we can see that indeed, the fractal training does seem to reduce the mesh dimensionality for the reachable state space meshes. 



\begin{table}[!h]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|l|l|l|}
\hline 
Training              & Trajectory Mesh Dim.    &  Reachable Mesh Dim. \\ \hline \hline
Standard Noise        &    1.38                 &      3.83     \\  \hline
Fractal Noise         &    1.16                 &      3.16     \\  \hline
\end{tabular}

\caption{Mesh dimensions for the best performing seed from the standard with noise training, and the fractal with noise training, given the same disturbance profile of 100 pushes. For reference the state space for our system has 12 dimensions.}
\label{table:mesh_dims}
\end{table}

\subsection{Validating the Mean First Passage Time}

We emphasize that the reward function for the hopper environment is simply to move forward with the highest velocity possible, no attempts were made to make the system robust to disturbances. Perhaps because of this, the mean first passage time for these systems are relatively small, on the order of 100 foot steps. For this small number of steps, we can validate the mean first passage time with Monte Carlo trials. It's worth noting that the eigen estimate of the mean first passage time is much more valuable for more robust systems. This is because this estimate becomes more accurate as the system becomes more stable, and because the cost of calculating the MFPT with Monte Carlo trials grows much more expensive for more stable systems. In previous works \cite{OguzSaglam2015} it was used to quantify robustness for systems with a MFPT as high as $10^{15}$. 

To do this, we compare the mean first passage time as estimated by equation \ref{eq:mfpt} to the value computed by looking at many Monte Carlo rollouts. For the rollouts we apply a random action drawn from the same distribution described above. Instead of sampling 100 pushes though we sample a new push every time we need a new disturbance. During the rollouts we still apply the push at the apex height of the ballistic phase.

Figure \ref{fig:mdim_monte} shows the convergence of the MFPT as we expand the size of the mesh, and compares it to the mean steps to failure obtained with Monte Carlo trials. We can see that it does look like the MFPT is converging to the Monte Carlo result. Although at the largest mesh we tried, the eigen analysis gives an estimate of 110.2 steps to failure, while the Monte Carlo trials tell us that an average of 85 steps are taken before failure. It's worth noting that the distribution of failure times has a large variance with a standard deviation of 80 steps.


\begin{figure}[!h]
\centering
\includegraphics[width=.75\linewidth]{fig/icra2021//mdim_mfpt.png}
\caption{Estimated mean first passage time computed from \ref{eq:mfpt} compared to a Monte Carlo estimate. The blue dashed line and shaded region are the mean and standard deviation of the steps to failure for 2500 Monte Carlo rollouts.}
\label{fig:mdim_monte}
\end{figure}


% \begin{figure}[!h]
% \centering
% \includegraphics[width=.65\linewidth]{fig/icra2021//iden_mfpt.png}
% \caption{Estimated mean first passage time from eigenvalues of the stochastic transition matrix for different box sizes. The blue dashed line is the mean steps to failure for 2500 Monte Carlo rollouts,
% The shaded region indicates the standard deviation of the Monte Carlo trials}
% \label{fig:iden_monte}
% \end{figure}


\subsection{High Resolution Mesh}

We now use the fractal agent and construct an even more accurate mesh. Figure \ref{fig:mdimT} show the sparsity pattern for the state transition matrix for the fractal noise agent with a box size of 0.005. Recall that in the process for creating the mesh, we start with a small number initial seed states. After that every new state that we add is added in order we find them to the mesh. So if we are expanding state \# 2, and there are currently 100 states in the mesh, if we transition to an unseen state, that state will be labeled \# 101. So although it may seem like it is not possible for states in the top right quadrant to visit states later in the mesh, this is really an artifact of how we construct our mesh and label our points. 

% \begin{figure}
% \centering
% \includegraphics[width=.65\linewidth]{fig/icra2021//iden_noise_25.png}
% \caption{Sparsity pattern for an agent trained with the with the standard reward and noise added at training time. All non zero values are shown with equal size and coloration. The matrix can be interpreted as follows, each row represents the state that you are starting from at time t, and each collum with an entry represents a probability that you will transition to the corresponding collumn at time t+1}
% \label{fig:idenT}
% \end{figure}


\begin{figure}[!htb]
\centering
\includegraphics[width=.7\linewidth]{fig/icra2021//sT2.png}
\caption{Visualization of the stochastic transition matrix for the top performing fractal noise agent. All non zero values are shown with equal size and coloration. Recall that each entry in $T_{ij}$ tell us the probability of transitioning to state j after one step if we start in state i.}
\label{fig:mdimT}
\end{figure}

%(!!! this whole paragraph needs some work, might remove)
We note that there are a smaller set of states that make up most of the transitions. In fact we can see from Figure \ref{fig:mass_clump} that 20\% of the states in our mesh account for about 90\% of all transitions seen during the mesh construction.

\begin{figure}[!htb]
\centering
\includegraphics[width=.7\linewidth]{fig/icra2021//pmass.png}
\caption{Cumulative sum of probability mass excluding the failure state. We take the sum of each column of T, and sort it in descending order, then report the cumulative sum of probability. Each point on the curve tells us that x\% of states make up y\% of all state transitions.}
\label{fig:mass_clump}
\end{figure}

One of the advantages of having a discrete set of states is that it opens up new tools and visualizations, for example we can apply Principle Component Analysis (PCA). Figure \ref{fig:pca_side} shows a projection of our mesh states on the top 3 principle components. We note that these three states account for more than 97\% of the variance, we also note that our analysis reveals that states in red are where 99\% of all failures occur. The visualization reveals that at least in PCA space, all the trouble states are clustered in one spot. A promising direction for future work is to introduce a policy refinement step that attempts to avoid these states. Additionally, if we were designing a real robot this may give us insights into design changes that could be made. 


\begin{figure}
\centering
\includegraphics[width=\linewidth]{fig/icra2021//mdim_pca_side.png}
\caption{View of the first 3 principle components of the mesh for a fractal noise policy.}
\label{fig:pca_side}
\end{figure}

% \begin{figure}
% \centering
% \includegraphics[width=1\linewidth]{fig/icra2021//mdim_pca_top.png}
% \caption{}
% \label{fig:pca_top}
% \end{figure}


% \begin{figure}
% \centering
% \includegraphics[width=.65\linewidth]{fig/icra2021//iden_mfpt.png}
% \caption{}
% \label{fig:mdimT}
% \end{figure}

% Can still add these... it does maybe speak to some of the strengths meshing can provide. 

% \begin{figure}
% \centering
% \includegraphics[width=\linewidth]{fig/icra2021//mdim1_fail.png}
% \caption{Placeholder}
% \label{fig:mass_clump}
% \end{figure}





\section{Conclusions From The Hopper Mesh}

In this work, we apply previously developed tools that create discrete meshes for the reachable state space of a system. These tools were applied to policies obtained with a modified reinforcement learning reward function which was previously shown to encourage small mesh dimensions for individual trajectories not subject to any disturbances. We showed that these modified policies have a smaller average reachable mesh size across all random seeds for coarse meshes and a small number of disturbances. We then showed a clear difference in mesh sizes and mesh dimensions for the top performing seeds on a richer set of disturbances and finer mesh sizes. We also validated our use of the MFPT as a tool by comparing it to Monte Carlo trials. Finally, we constructed a high fidelity mesh at a resolution that would not have been feasible with standard ARS policies. In addition, we created visualizations with this mesh that revealed insights about the contracting nature of the policy, and which point to future applications of this approach. Taken together, these results show two things. First, it further validates the utility of the fractal dimension reward, which we have shown transfers it's desirable quality of having a more compact state space to a setting with external disturbances. These results are also a credit to the mesh based tools, because it shows that the fractal training can be used to extend the reach of these tools to higher dimensional systems or higher resolution meshes than would have otherwise been possible.  


\subsection*{Noise During Training}
Zero mean Gaussian noise with std = 0.01 added to policy actions before being passed to the environment, for reference all actions from the policy are between -1 and 1.  Zero mean Gaussian noise with std = 0.001 added to observations before being passed to the policy.  \\ 

% \subsection*{Implementation Details} Recall that our mesh uses a hash table with a key constructed by \ref{eq:key}. During the meshing process we must reconstruct the original state s. One option is simply to invert equation \ref{eq:key}. Geometrically, this can be viewed as taking the centroid of the box of continuous space that the mesh state represents. For small mesh sizes this is an unimportant detail, however for some of the larger boxes used in constructing table \ref{table:mesh_sizes}, we found that taking the centroid resulted in states with significant ground penetration, which caused instability in the simulator. To alleviate this, we store the first state which resulted in adding a new box to the mesh as the representative for that box. This representative is then used when initializing the system to explore the mesh point.



%\section*{ACKNOWLEDGMENT}

