\section{Introduction}
   
    

    As we have discussed, modern RL algorithms have a serious draw back in that they are mostly black boxes. It is an open challenge to figure out what exactly it is that your RL agent has learned. If all you know is that one of your agents achieved very a high reward, it is not clear how to verify that this system is safe and sensible in all the regions of state space it will visit during its life. Nor can we necessarily say anything about the stability or robustness properties of the system. Recent work~\cite{Taleledeep} has used so-called mesh-based tools to examine precisely these questions by approximating the nonlinear dynamics within state space with a mesh of discrete points and a set of mappings between them over time.
    
    However, utility of any mesh-based tools to accurately discretize a state space is limited, due to the curse of dimensionality. In practice, these methods are only able to work on relatively high dimensional systems if the \textit{reachable} state space grows at a rate that is much smaller than the exponential growth of the full state space the system within which it is embedded. To expand these methods to higher dimensional systems, we will need to find ways to keep the ``volume'' of visited states from expanding commensurately. One way to quantify this rate of growth is by using one of the several notions of ``fractional dimensionality" from fractal geometry.
    
    In this chapter, we discuss an efficient meshing algorithm, which we call box meshing. We show that this approach makes calculating the so called mesh dimension feasible in the context of reinforcement learning. We also propose using other notions of fractional dimension from the literature as a proxy for the property we care about. We then show that reinforcement learning agents can be trained to shrink these measures by post-processing their reward function. We present the results of this training, and also present some brief analysis of the resulting structure for select policies.  


    

\section{Meshing \& Fractional Dimensions}


\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/acc_fractal/Fractaldimensionexamplebw.png}
    \caption{Mesh scaling in different dimensions}
  \end{subfigure}~~~
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/acc_fractal/Snow_Mesh_Example.png}
    \caption{A non-uniform mesh of a fractal structure}
  \end{subfigure}~~~
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/acc_fractal/Snow_LineFitAnn.png}
    \caption{Estimation of the mesh dimension}
  \end{subfigure}
  \caption{Image credit for sub-figure a: \cite{BrendanRyan/Publicdomain2020}, image credit for sub-figures b and c: \cite{Talelepush}}
  \label{fig:fracdim}
\end{figure}

% \begin{wrapfigure}{L}{0.3\textwidth}
% \centering
% \includegraphics[width=0.3\textwidth]{fig/corl2020/HalfCheetahRews.png}
% \caption{\label{fig:frog2} Half Cheetah Reward Curves.}
% \end{wrapfigure}

%(??) is there other work I should be citing here?
%(??) I feel like this section is already too long, but also if I don't explain the pertubation style meshing people will misunderstand what we are tying to do and what "meshing" is?
Let's say we have a continuous set $S$ that we want to approximate by selecting a discrete set $M$  composed of regions in $S$. We will call this set $M$ a mesh of our space. Figure~\ref{fig:fracdim}(a) shows some examples of this: a line is broken into (1D) segments, a square into (2D) grid spaces, and so on. The question is: as we increase the resolution of these regions, how many more regions $N$ do we need? Again, Figure~\ref{fig:fracdim}(a) shows us some very simple examples. For a $D$ dimensional system, if we go from regions of size $d$ to $d/k$, then we would expect the number of mesh points to scale as $N \propto k^{D}$. But not all systems will scale like this, as \ref{fig:fracdim}(b) and \ref{fig:fracdim}(c) illustrate. Figure \ref{fig:fracdim}(b) is an example of a curve embedded in a two dimensional space"; namely, this depicts part of the Koch snowflake fractal pattern. The question of how many mesh points are required must be answered empirically. Going backwards, we can use this relationship to assign a notion of "dimension" to the curve. 

\begin{equation}
    D_{f} = -\lim_{k \rightarrow 0}\frac{\log N(k)}{\log k} 
\end{equation}

What we are talking about is called the Minkowskiâ€“Bouligand dimension, also known as the box counting dimension. This dimension need not be an integer, hence the name "fractional dimension". As a practical matter, we can use the slope of the log-log plot of mesh sizes over $d$ to estimate this value. This is one of many measures of "fractional dimension" that that emerged from the study of fractal geometry. Although these measures were invented to study fractals, they can still be usefully applied to non-fractal sets.

In \cite{OguzSaglam2015}, Saglam and Byl introduced a technique that is able to both build a non-uniform mesh of a reachable state space and develop robust policies for a bipedal walker on rough terrain. Having a discrete mesh allows for the use of value iteration, to select among a set of candidate controllers, toward finding a robust switching control policy. In addition, this mesh allows for the construction of a state transition matrix, which can be used to calculate the \textbf{mean first passage time} \cite{Byl2009}, a.k.a the mean time to failure, as a metric that quantifies the expected number of steps a metastable system can take before falling. 

Since its introduction, in improving and quantifying robustness of biped walking to terrain height variation, meshing in this fashion has also been used for designing walking controllers robust to push disturbances \cite{Talelepush}, to design agile gaits for a quadruped \cite{Byl2017}, and to analyze hybrid zero dynamics (HZD) controllers \cite{Saglam2015}. There has also been recent work to use these tools to analyze policies trained by deep reinforcement learning \cite{Taleledeep}. A long term goal and motivation for this work is to take a high performance controller obtained via reinforcement learning, and to extract from it a mesh-based policy that is both explainable and amenable to robustness analyses.

\subsection{Box Meshing}
\label{sec:boxmsh}
Our primary improvement to the prior work on meshing itself is to introduce something we call \textbf{box meshing}. Prior, a new mesh point could take any, arbitrary value in the state space, based on detection of any new reachable state that was not currently ``close enough'' to existing mesh points, given tolerance settings. To determine if a new state is already in the mesh, we would compute a distance metric to every point in the mesh, and check if the minimum was below our threshold. Thus, building the mesh was an $O(n^{2})$ algorithm. By contrast, in box meshing we a priori divide the space uniformly into boxes with side length $d$. We identify any state $s$ with an indexing key. This key is obtained by first normalizing each element of $s$ by the standard deviation of that dimension, based on all data points, to create $\bar{s}$. Then: $\text{key} = \text{round}(\frac{\bar{s}}{d})d$, where round performs an element-wise rounding to the nearest integer. (See Algorithm~\ref{algo:createboxmesh}.) We can then use these keys to store mesh points in a hash table. Using this data structure, we can still store the mesh compactly, only keeping the points we come across. However, insertion and search are now $O(1)$, and so building the mesh is $O(n)$. This is very similar to non-hierarchical bucket methods, which are well-studied spatial data structures~\cite{Samet1990}, although we are using them for data compression here. In the prior meshing work, this sort of speedup would be minor, as the run-time is dominated by the simulator or robot. However, this speedup does open some new possibilities: most poignantly, it makes calculating the mesh dimension during reinforcement learning plausible. 


\begin{algorithm}
\begin{algorithmic}[1]
%\KwResult{Mesh Table, Deterministic State Transition Matrix}
\State \textbf{Input:} State set $S$, box size d.
\State \textbf{Output:} Mesh size m.
\State \textbf{Initialize:} Empty hash table M.
 \For{s $\in$ S}
    \State $\bar s  = \text{Normalize(s)} $
    \State key = round($\bar s$ / d)d
    \If{s $\in$ M}
       \State  M[key]++
    \Else
        \State M[key]=1
    \EndIf
\EndFor
\State \textbf{Return:} M
\end{algorithmic}
\caption{Create Box Mesh, see Sections \ref{sec:boxmsh}-\ref{ss:meshdimexamples}}
\label{algo:createboxmesh}
\end{algorithm}

\subsection{Algorithmic Box Mesh Dimension}
\label{sec:boxdim}
The ``box mesh dimension" is the quantity extracted from the slope of a log-log plot of mesh size vs $d$ value, as depicted in Fig.~\ref{fig:fracdim}(c). In our work, we estimate this slope based on analyzing a large but finite number of states visited over time by the agent during learning. The log-log plot for a very large data set tends to take on a particular form, illustrated in Figure~\ref{fig:curves}(a). As the box edge size $d$ gets very small, nearly every point falls into its own, unique box. This results is the flat, saturated portion of the curve at the upper left of the subplot. As $d$ gets very large, the curve tends to flatten out at the bottom right, as well. We hypothesize this second saturation can happen when, for a learned gait-like locomotion behavior, all the points at a particular time step in the gait tend to fall within the same box with high probability, yielding nearly the same number of mesh boxes across a range of values of $d$. Correspondingly, these two flat areas are expected numerical artifacts of the curve that we hope to ``ignore'', and the mesh dimension should ideally be based only on the well-behaved flat region near the middle of the curve. This has been done by hand in Figure~\ref{fig:curves}(a) by selected only a subset of the range of $d$ and fitting the log-log data to a line.


%For this paper, it is assumed that the mesh algorithm being used for this calculation is the box mesh. 
As one might expect, automatically computing the mesh dimension of an arbitrary data set generated from a learning agent can be quite challenging in terms of speed and accuracy. A single trajectory provides a relatively small amount of data, essentially making mesh size calculation a noisy estimation process. Agents attempting to learn legged locomotion also might fall over, generating extremely short trajectories, or they might learn a trajectory that "stands in place", which could lead to numerical errors. 

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/meshdim.png}
    \caption{Curve of a well behaved policy}
    \label{fig:curvesa}
  \end{subfigure}
%   \begin{subfigure}[b]{0.32\textwidth}
%     \includegraphics[width=\textwidth]{fig/corl2020/meshdimhand.png}
%     \caption{Hopper-v2}
%   \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/meshdimreal.png}
    \caption{Curve of a typical run-time policy}
    \label{fig:curvesb}
  \end{subfigure}
  \caption{Mesh curve and mesh dimension examples}
  \label{fig:curves}
\end{figure}

Finally, there is a clear trade-off \textit{between} accuracy and speed. Model-free RL is predicated on having a huge number of rollouts to learn from, and we would therefore like for any mesh-dimension quantification algorithm to be fast enough so as to not dominate the total learning time. With these factors in mind, we introduce two box mesh dimensions. The \textit{lower mesh dimension} is based on the linear fit of all of the log-log curve data, so that it intentionally errs on the side of including any flat parts of the graph, therefore tending to underestimate the true mesh dimension. We also calculate the \textit{upper mesh dimension}, which takes the largest slope of any two neighboring points in the log-log relationship, thus tending to overestimate the true mesh dimension. Neither of these measures are correct, but taken together they provide goods bounds on the mesh dimension, and as we will see they can be useful on their own.


\subsection{Mesh Dimension Examples}
\label{ss:meshdimexamples}
Figure~\ref{fig:curves} illustrates two examples of the curves used to compute the mesh dimension. Recall that to compute the mesh dimension, we choose several values for $d$, the box length, and for each $d$ construct a mesh using that box size. The $x$ axis of these plots represents the log of the box length used, the $y$ axis represents the log of size of the mesh created. For each curve, we display the lower bound and upper bound for the dimension as computed by Algorithm~\ref{algo:mesh_dim}, as well as example attempts to ``hand fit'' the data. Subplot~\ref{fig:curvesa} illustrates what a close-to-ideal situation looks like, in addition to providing intuition as to why the upper and lower mesh dimension bound the quantity we are trying to measure. Subplot~\ref{fig:curvesb} serves to illustrate some of the problems with making an algorithmic measure of the dimension. Here, there is much less data to work with, as illustrated by the difference in magnitude of the maximum log(mesh size) at the upper left of each subplot. This performance constraint in turn causes the estimate of the mesh dimension to be noisy. Indeed, even fitting this data by hand becomes a challenge! We provide two fits which can both be argued to be ``plausibly correct". The true value we would have in the limit for a larger data set remains unknown, and the range between upper and lower mesh dimensions also becomes more dramatic here, indicative of a large uncertainty.




% !!! may or may not get included
% \subsection{PCA}

% In higher dimensions, the number of mesh points needed to represent data becomes unacceptably large, especially data that has low dimensional structure. To combat this we do a principle component decomposition of a nominal trajectory, while meshing the states, we project each state into the top l PCA values, and crucially, we then scale each projected dimension by the associated normalized singular values. This in effect gives our mesh more resolution in the important dimensions, while allowing less important dimensions to take on a coarser mesh. more formally if the singular value decomposition is given by:


% \begin{equation}
% X = U \Sigma W^{T}
% \end{equation}
% Then our projected coordinates are:

% \begin{equation}
% T = X W \frac{\Sigma}{|\Sigma|}
% \end{equation}




\begin{algorithm}
\begin{algorithmic}[1]
%\KwResult{Mesh Table, Deterministic State Transition Matrix}
\State \textbf{Input:} State set $S$ 
\State \textbf{Output:} Mesh M.
\State \textbf{Hyperparameters:} scaling factor f,  initial box size $d_{0}$.
\State \textbf{Initialize:} Empty list of mesh sizes H, empty list of d values D.
\State m = Size(CreateBoxMesh(S, $d_{0}$)) 
\State d = $d_{0}$
\State Append m to H, append d to D.
\While{m $<$ size(S)}
    \State d = d/f 
    \State m = Size(CreateBoxMesh(S, d)) 
    \State Prepend m to H, prepend d to D. 
\EndWhile
\While{m $\neq$ 1}
    \State d = d*f  
    \State m = Size(CreateBoxMesh(S,d)) 
    \State Append m to H, append d to D. 
\EndWhile

\State X =  $\log$ d 
\State Y =  -$\log$ m

\textbf{Lower Mesh Dim:} fit Y = gX + b, \textbf{Return:} g \\
\textbf{Upper Mesh Dim:} w = greatest slope in Y over X \textbf{Return:} w

\caption{Compute Box Mesh Dimension, see Sections \ref{sec:boxdim}-\ref{ss:meshdimexamples}}
\label{algo:mesh_dim}
\end{algorithmic}
\end{algorithm}


\subsection{Variation Estimators}
\label{sec:var}
Consider the more general case of a set of data, $X$, collected at equally-spaced moments in time. For example in our work, $X=S$ (i.e., the ``state set''), with each particular element $X_i$ representing the state vector $\vec{s}$ in state space at time stamp $i$, and with a total of $n$ points recorded at equal spacing over time. 
As discussed, computing the mesh dimension (which, as previously discussed, is our hash-table inspired version of the \textit{box dimension}) in an automatic way is fraught with peril, in many practical scenarios. Fortunately, there are also various other metrics one might consider to give various approximations to the fractional dimension we seek to estimate. Gneiting et al.~\cite{Gneiting2012} compare a number of these estimators and propose that the \textbf{variation estimator}~\cite{Emery2005} offers a very good trade off between speed and robustness. To obtain this estimator, first define the \textit{power variation} estimate of order $p$, $\hat{V}_p$ as:
\begin{equation}
\hat{V}_{p}(X, l) = \frac{1}{2}\mathbb{E} \left|X_{i} - X_{i-l} \right|^{p} = \frac{1}{2(n-l)}\sum_{i=l}^{n} \left| X_{i} - X_{i-l} \right|^{p}.
\end{equation}
Then, the \textit{variation estimator} of order p is:
\begin{equation}
Dv_{p}(X) = 2 - \frac{1}{p}\frac{\log V_{p}(X,2) - \log V_{p}(X, 1)}{\log 2},
\label{eq:var_p2}
\end{equation}
The \textbf{madogram} estimator is the special case of \eqref{eq:var_p2} where $p = 1$, and for the \textbf{variogram}, $p = 2$.





% Why not use the thing we actually want to reduce? As we've mentioned, measuring the mesh dimension automatically is tricky (again see implementation details in the appendix). But with the faster meshing, it becomes feasible to use the mesh dimension directly, in the same manner we used the variation dimensions before. Here we introduce a second measure of mesh dimesion, called the "conservative mesh dimension". Recall that the normal mesh dimension is the least squares fit of a line to the log log plot of the mesh size vs the box width. The conservative mesh dimension is defined as the steepest slope found on this graph, simple as! This can be seen as an upper bound on the increases that we can expect while decreasing our mesh size. Table \ref{tab:mesh} shows the dimensions and reward after training.

\subsection{Post-processing Rewards}

In order to influence the dimensionality of the resulting policies, we introduce various post-processors, which act on the reward signals before passing them to the agent. These obviously modify the problem: in some sense the post-processed environment is a completely different problem from the original. However our meta-goal is to train agents that achieve reasonable rewards in the base environment, while simultaneously exhibiting the reduced dimensionality we are looking for. These post-processors take the form:
\begin{equation}
R_{*}(\vec{s}, \vec{a}) = \frac{1}{D_{*}(\vec{s})}\sum_{t=0}^{T} r(s_{t}, a_{t}, s_{t+1}),
\end{equation}
where $(\vec{s}, \vec{a})$ are understood to be an entire trajectory of state action pairs, and $D_{*}$ is some measure of fractional dimension. Various measures of dimensionality can be inserted here directly (e.g., see \ref{sec:var}). 

However, the mesh dimensions computed by algorithm \ref{algo:mesh_dim} require a little more care. We must first define a \textit{clipped dimension}:
\begin{equation}
D^{c}_{*} = \text{clip}[D_{*}(\vec{s}_{t > Tr}), 1, D_{t}/2)], 
\end{equation}
where $D_{t}$ is the topological dimension, equal to the number of states (i.e., for our work, the position and velocity variables) in the system. $T_r$ is a fixed number of timesteps chosen to exclude the initial transients resulting from a system moving from rest to into a quasi-cyclical ``gait''. In this paper we set $T_r$ = 200 for all experiments. For comparison, the nominal episode length is 1000 timesteps. The clipping is intended to minimize the influence of pathological trajectories the RL agent might generate while not interfering dramatically with the overall training. Additionally, it also weeds out trajectories that terminate very early, to prevent agents learning to fall over immediately to ``game the system''. Using half of the topological dimension (i.e., $\frac{1}{2}D_{*}$) proved to be a decent upper bound for the worst case dimensionality of each system in practice. The \textbf{mesh dimension post-processors} use the clipped dimension. Finally, in order to benchmark against a ``no post-processing'' comparison, we additionally train with a fictitious value of $D_{*} = 1$, and we call those non-post-processed results the \textbf{identity post-processor}, since in this case the total reward is completely unchanged.

\subsection{Environments}
\label{sec:ch4env}
We examine a subset of the popular OpenAI Mujoco locomotion environments introduced in \cite{1606.01540}. In particular, we evaluate our work on HalfCheetah-v2, Hopper-v2, and Walker2d-v2. These environments were chosen because they have a relatively high dimensionality, i.e., 11-17 degrees of freedom (DOF), with twice that number of states (including both position and velocity of each DOF), since our goal is to demonstrate that mesh-based approaches are feasible even as dimensionality grows. The state space consists of all joint / base positions and velocities, with the x (the "forward") position being held out, because we want a policy that is invariant along that dimension.


\begin{figure}
  \centering
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/cheetah_crop.png}
    \caption{HalfCheetah-v2}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/hopper_crop.png}
    \caption{Hopper-v2}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/walker2d_crop.png}
    \caption{Walker2d-v2}
  \end{subfigure}
  \caption{The Mujoco locomotion environments}
  \label{fig:envs}
\end{figure}


\subsection{Augmented Random Search}

 In \cite{Mania2018}, Mania et al. introduce Augmented Random Search (ARS), which proved to be efficient and effective on simulated locomotion tasks. Rather than a neural network, ARS uses static linear policies, and compared to most modern reinforcement learning, the algorithm is very straightforward. The algorithm operates directly on the policy weights. In each epoch, the agent perturbs its current policy $N$ times and collects $2N$ rollouts (i.e., using both positive and negative policy deviation by this perturbation vector) of modified policies. The rewards from these rollouts are then used to update the current policy weights, with the process repeating until arriving adequately close to some locally optimal solution. The algorithm is known to have high variance, so that not all seeds obtain high rewards, but to our knowledge this work in many ways represents the state of the art on the benchmark environments described in Sec.\ref{sec:ch4env}. Mania et al. introduce several small modifications of the algorithm in their paper, and our implementation corresponds to the version they call ARS-V2t.


% \begin{wrapfigure}{R}{0.3\textwidth}
% \centering
% \includegraphics[width=0.25\textwidth]{fig/corl2020/linearz.png}
% \caption{\label{fig:frog1}This is a figure caption.}
% \end{wrapfigure}

% Linearz. We made up a small toy system that exhibited some of the properties we were looking for in our policies. The dynamics are:

% \begin{equation}
%     \dot x = u_{1} , \dot y = u_{2}, \dot z = x
% \end{equation}

% What are the challenges of this environment? it has a dummy dimension, it blows up... 

% \begin{figure}[h!]
%   \centering
%     \includegraphics[width=.25\textwidth]{fig/corl2020/linearz.png}
%     \caption{HalfCheetah}
% \end{figure}

\subsection{Training}
\label{sec:training}

 We chose parameters that were found to work well across all environments, using the parameters reported in Table~9 from \cite{Mania2018} as a starting point. We then tuned until our unprocessed learning achieved satisfactory results across all tasks. Again, ARS is known to have high variance between random seeds, indicating high variance among local minima for ARS policies, so that some seeds never learn to gather a large reward. The parameters we found are able to consistently solve the Cheetah and Walker environments. For the hopper, the algorithm learns a policy with high reward\footnote{The MuJoCo environments reward ``forward progress'' over time, with a subtracted penalty for squared torque inputs.} roughly half the time. This seems consistent with the performance reported in~\cite{Mania2018}. We train each post-processor on 10 random seeds. The evaluation metrics are averages over 5 rollouts from each seed, and for the dimension metrics we use extended episodes of length 10,000 to get more accurate estimates of dimensionality. The reported returns, and the training, both use the normal 1,000 timestep episodes. We found that the mesh post-processors were getting very poor performance when trained from a random policy. However, we found that we saw good results when these trials were initialized with a working policy. Therefore we trained agents for 750 epochs without post-processing, and used that to initialize the policies with mesh dimension post-processing. The mesh policies were then trained for an additional 250 epochs, with the results reported below.
%===============================================================================



\section{Results}
\subsection{Mesh Dimension Post-processors}

\begin{figure}[!htb]
  \centering
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/mesh4_curves/cheetah.png}
    \caption{HalfCheetah}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/mesh4_curves/hopper.png}
    \caption{Hopper}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/mesh4_curves/walker.png}
    \caption{Walker}
  \end{subfigure}
  \caption{Reward curves for mesh dimension postproccesor runs.}
  \label{fig:mesh_rews}
\end{figure}

\begin{table}[!htb]
\begin{tabular}{ l|l|l|l|l }
\hline
Environment & Postprocessor 
                  & Lower Mesh Dim.         & Upper Mesh Dim.   & Return \\ 
\hline
\multirow{3}{2.6cm}{HalfCheetah-v2} 
& Identity        & 2.31 $\pm$ 0.71   & 7.34 $\pm$  1.56 &  5469 $\pm$ 823 \\
& Lower Mesh Dim        &  \textbf{0.66 $\pm$ 0.51}   & \textbf{2.55 $\pm$  1.52} &  4962 $\pm$ 598 \\
& Upper Mesh Dim.  & \textbf{1.06 $\pm$ 1.13}  & \textbf{2.83  $\pm$ 1.27} &  4432 $\pm$ 539 \\
\hline
\multirow{3}{2.6cm}{Hopper-v2} 
& Madogram$^{*}$        & 1.62 $\pm$ .27   & 4.68  $\pm$  0.82   &  3461 $\pm$ 119 \\
& Lower Mesh Dim.        & 1.13 $\pm$ .02   & 3.54 $\pm$  0.96   &  2941 $\pm$ 538 \\
& Upper Mesh Dim.  & 1.27 $\pm$ .50   & 2.98 $\pm$  1.48   &  3020 $\pm$ 337 \\
\hline
\multirow{3}{2.6cm}{Walker2d-v2 \\ (walking seeds)$^{**}$}
& Identity        & 2.13 $\pm$  0.31    & 4.62 $\pm$ 1.03  &  3758 $\pm$ 1037  \\
& Lower Mesh Dim.       & 1.21 $\pm$  0.06    & 4.09 $\pm$ 1.03  &  3339 $\pm$ 887   \\
& Upper Mesh Dim. & 1.89 $\pm$  0.42    & 3.10 $\pm$ 0.93  &  3359 $\pm$ 903 \\
\hline
\multirow{3}{2.6cm}{Walker2d-v2 \\ (all seeds)$^{**}$ }
& Identity        & 2.13 $\pm$ 0.31   & 4.62 $\pm$ 1.03    &  3758 $\pm$ 1037 \\
& Lower Mesh Dim.       & 1.04 $\pm$ 0.53   & 4.45 $\pm$ 1.19    &  3034 $\pm$ 1086 \\
& Upper Mesh Dim. & 1.48 $\pm$ 0.67   & 2.27 $\pm$ 0.95    &  2556 $\pm$ 1378 \\
\hline
\end{tabular}
\caption{\label{tab:mesh_p2_lowup} Mesh dimensions and returns for trajectories after training. See \ref{sec:training} for details\\
%% \footnotesize{*  Because ARS with our chosen hyper parameters does not consistently produce 10 seeds that perform well on the hopper, we instead use madodiv (see the \ref{sec:var}) for the seed policies.  \\
%% **  See \ref{sec:msh}}
}
\end{table}

For all environments, the mesh post-processors had a significant impact in reducing the mesh dimensions. It's important to emphasize here that the dimensions reported represent lower- and upper-bound estimates (see Sec.~\ref{sec:boxdim}) for the actual mesh dimensions. Although mesh dimension was successfully reduced, there was also a corresponding and statistically significant decrease in the unprocessed reward returns. 

However, this work stands as a building block for broader, future aims of addressing the ``curse of dimensionality'', lowering dimensionality of the reachable state space for a controlled system toward enabling a variety of other numerical techniques to quantify long-term robustness (which is not measured directly by modern-day reward functions).
Given our primary goal in this work is to train agents to have an acceptable reward while being more amenable to meshing, we argue the trade-offs between mesh dimensionality and reward are quite good here, overall. 

Of note, several seeds (4 for the upper dim., 3 for the lower mesh dim.) for the Walker system ``forget how to walk", instead learning a policy that stands in place. Although this behavior would arguably be likely to have a low dimensionality\footnote{However, this is not guaranteed to result in a lower dimensionality since states are first normalized in performing our ``box meshing''.}, it is certainly not a very useful behavior for locomotion! For completeness, we include the Walker statistics both from the seeds that learned a gait, and for all 10 seeds including the standing policies.


\subsection{Variational Post-processors}


\begin{figure}[!htb]
  \centering
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/data17_rews/cheetah.png}
    \caption{HalfCheetah}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/data17_rews/hopper.png}
    \caption{Hopper}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{fig/corl2020/data17_rews/walker.png}
    \caption{Walker}
  \end{subfigure}
  \caption{Reward curves for the variation postprocessors}
  \label{fig:rews}

\end{figure}
\begin{table}
\begin{tabular}{ l|l|l|l|l|l }
\hline
Env. & Post-proc. 
                    & Variogram        & Madogram       & Lower Mesh Dim.   & Return \\ \hline
\multirow{3}{*}{H-C} 
& Identity          & 1.71 $\pm$ .03   & 1.42 $\pm$ .05 &  2.36 $\pm$ .61 & 5545 $\pm$ 593  \\
& Variogram   & 1.68 $\pm$ .01   & 1.36 $\pm$ .02 &  2.06 $\pm$ .60 & 5136 $\pm$ 851\\
& Madogram    & 1.65 $\pm$ .02   & 1.31 $\pm$ .04 &  2.09 $\pm$ .64 & 5234 $\pm$ 950\\

\hline
\multirow{3}{*}{Hop} 
& Identity$^{*}$           & 1.61 $\pm$ .14  & 1.22 $\pm$ .28               &  1.03$^{*}$ $\pm$ .71 & 2063 $\pm$ 1052 \\
& Variogram   &  \textbf{1.51 $\pm$ .02}  & \textbf{1.03 $\pm$ .04}   &  1.58 $\pm$ .54 & 3299 $\pm$ 711 \\
& Madogram     & \textbf{1.51 $\pm$ .002}  & \textbf{1.02 $\pm$ .004} &  1.57 $\pm$ .36 & 3449 $\pm$ 146\\
\hline
\multirow{3}{*}{Walk} 
& Identity           & 1.68 $\pm$ .35  & 1.36 $\pm$ .71 &              2.14 $\pm$ .29 & 3742 $\pm$ 1038\\
& Variogram    & \textbf{1.54 $\pm$ .07}  & \textbf{1.07 $\pm$ .01} &  1.85 $\pm$ .54 & 3779 $\pm$ 894 \\
& Madogram     & \textbf{1.53 $\pm$ .01}  & \textbf{1.06 $\pm$ .02} &  1.99 $\pm$ .53 & 3414 $\pm$ 1025\\
\hline
\end{tabular}
\caption{\label{tab:dims} Mesh dimensions and returns for trajectories after training. Here, the environments (``Env.'') are H-Ch: HalfCheetah-v2, Hop: Hopper-v2, and Walk: Walker2d-v2. See \ref{sec:training} for details\\
\footnotesize{* This includes policies which learned to "stand still", which lowers the average mesh dimension considerably see discussion}
}
\end{table}

The variational post-processors had a modest effect the variational metrics of dimension, but that did not seem to correlate to a smaller mesh dimension in our experiments, despite what our preliminary tests had led us to hypothesize. The Hopper and Walker had remarkable consistency in the variation dimensions they found. %possibly this could be used to lower the variance in ARS. The fact that the variogram and madogram also got higher performance on the hopper task provides initial support this claim. However without 
Without running many more trials and hyperparameter sweeps, it is challenging to make broad generalizations; however, our experiments show that 1) measures for fractional dimension can be influenced without adversely effecting the reward, and 2) that it is possible for an agent to reduce the variogram and madogram dimensions of observed trajectories without having a significant impact on its box mesh dimension. 






\label{sec:msh}

\section{Analysis}

We now examine the learned behavior for one of the more notable policies. The most dramatic effects in Table~\ref{tab:mesh_p2_lowup} above were for mesh dimension post-processors on the Cheetah. Using either the upper and lower measures of dimension shrunk by 2-4 times.
Figure~\ref{fig:mesh_anal} presents data for this case.

% \begin{figure} (!! there must be a better way to display this figure)
% \centering
% \includegraphics[width=0.4\textwidth]{fig/corl2020/cheetah_analysis/meshes.png}
% \caption{\label{fig:frog2} Half Cheetah Reward Curves.}
% \end{figure}

% \begin{wrapfigure}{L}{0.4\textwidth}
% \centering
% \includegraphics[width=0.4\textwidth]{fig/corl2020/cheetah_analysis/meshes.png}
% \caption{\label{fig:frog2} Half Cheetah Reward Curves.}
% \end{wrapfigure}


\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{fig/corl2020/cheetah_mesh_cor.png}        
    \caption{Top: mesh sizes vs log of the box size for the cheetah environment. Lower Left: Every five frames overlaid for the an identity policy on the cheetah. Lower Right: Every five frames of cheetah after the lower mesh dimension training.}
    \label{fig:mesh_anal}
\end{figure}


% \begin{figure}[!htb]
%   \begin{subfigure}[b]{0.88\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{fig/corl2020/cheetah_mesh_cmp.png}
%     \end{subfigure}
%   \begin{subfigure}[b]{0.4\textwidth}
%     \includegraphics[width=\textwidth]{fig/corl2020/ident_cheetah_crop.png}
%   \end{subfigure}
%  \centering
%   \begin{subfigure}[b]{0.4\textwidth}
%     \includegraphics[width=\textwidth]{fig/corl2020/mdim_cheetah_crop.png}
%   \end{subfigure}
%   \caption{Left: mesh size comparisons across the post processors. middle: Trajectory after training normal ARS for 750 epochs. Right: Trajectory after training 250 additional epochs with the mesh dimension post processor}
% \end{figure}

Toward more intuitively understanding this data, a few comments are worth making, first. We have discussed the mesh dimension rather abstractly so far. In visualizing what this really means, imagine two different gait cycles. In one case, there is a general pattern to the motion, but it wanders in a noisy-looking way, like a ``signature'' that does not quite match up, cycle after cycle.  As motions become closer to being exact limit cycles, there is a more clear pattern of repetition, exactly analogous to re-tracing the same path, again and again, within the state space. Such a more tightly-structured limit cycle nature in turn results in a significantly lower-dimensional set of states being visited. 

We can see from the data in Figure~\ref{fig:mesh_anal} that there is an overwhelming difference in the mesh sizes between the lower mesh dimension post-processor and the other two. Notice that the axes are semilog (in $x$) here. The curve for the lower mesh dimension is furthest to the left, so that overall fewer mesh points are needed for values of $d$. And it transitions more gently in slope, so that the box dimension is also lower. 

To put this in perspective, before the extra 250 epochs of training, if given a box size of $d=0.01$ ($\log(d)\approx-4.6$), the agent would need a unique mesh point for every single point in the 10,000 state trajectory. After the additional training, however, the agent can represent all 10,000 points with just 5 mesh points! In this case it appears both agents learned a quasi-periodic gait that, with the additional training, converged to be almost exactly period-5. In Figure~\ref{fig:mesh_anal}, we present an overlay of the agents rendered every 5 steps. The results show us that the mesh agent has learned an extremely tight limit cycle. It's a bit of a strange limit cycle, being only 5 time steps long, but nonetheless we think this is interesting and surprising behavior.


% action std = .01
% HalfCheetah-v2:
% |          | mesh dimension | cmesh dimension |
% | identity |           2.33 |            6.78 |
% | mdim_div |           1.50 |            3.69 |

% Hopper-v2:
% |          | mesh dimension | cmesh dimension |
% | madodiv  |           1.64 |            4.52 |
% | mdim_div |           1.44 |            3.60 |

% Walker2d-v2:
% |          | mesh dimension | cmesh dimension |
% | identity |           2.06 |            4.42 |
% | mdim_div |           1.50 |            4.15 |



% State std = .005
% HalfCheetah-v2:
% |          | mesh dimension | cmesh dimension |
% | identity |           2.39 |            6.78 |
% | mdim_div |           1.76 |            3.69 |

% Hopper-v2:
% |          | mesh dimension | cmesh dimension |
% | madodiv  |           2.25 |            4.52 |
% | mdim_div |           1.89 |            4.18 |

% Walker2d-v2:
% |          | mesh dimension | cmesh dimension |
% | identity |           2.29 |            4.59 |
% | mdim_div |           1.89 |            4.36 |



The behavior displayed in Figure~\ref{fig:mesh_anal} is clearly something that can only happen in a noiseless simulation, so we also measured the mesh dimensions of our policies when subjected to noise during rollouts. Table~\ref{tab:mesh_p2_noise} shows these results. 
The difference in the fractional dimension is less pronounced than for the ``no noise'' case, but there is still a clear improvement for the post-processor cases. Furthermore, we anticipate (intuitively) that if we were also to add noise at training time, the learned policies might have been able to lower the mesh dimension more significantly for these sorts of post-training trials with noise.


\begin{table}
\begin{tabular}{ l|l|l|l|l }
\hline
Environment & Postprocessor 
                  & Lower Mesh Dim.         & Upper Mesh Dim.   & Return \\ 
\hline
\multirow{3}{2.6cm}{HalfCheetah-v2} 
& Identity              &  2.38   $\pm$ 0.43   & 6.65 $\pm$  1.90 &  5404 $\pm$ 1015 \\
& Lower Mesh Dim        &  1.51   $\pm$ 0.13   & 3.03  $\pm$  1.09 &  4952 $\pm$ 572 \\
& Upper Mesh Dim.       & 1.76    $\pm$ 0.53      & 3.54  $\pm$  1.27 &  4222 $\pm$ 803 \\
\hline
\multirow{3}{2.6cm}{Hopper-v2} 
& Madogram$^{*}$         & 1.63 $\pm$ .14   & 4.49 $\pm$  0.75   &  3438 $\pm$ 185 \\
& Lower Mesh Dim.        & 1.67 $\pm$ .22   & 3.71 $\pm$  0.89   &  2943 $\pm$ 535 \\
& Upper Mesh Dim.        & 1.64 $\pm$ .16   & 3.01 $\pm$  1.36   &  3019 $\pm$ 337 \\
\hline
\multirow{3}{2.6cm}{Walker2d-v2 \\ (walking seeds)$^{**}$}
& Identity        & 2.13 $\pm$  0.31    & 4.62 $\pm$ 1.03  &  3758 $\pm$ 1037  \\
& Lower Mesh Dim. & 1.83 $\pm$  0.34    & 2.73 $\pm$ 0.75  &  3511 $\pm$ 872   \\
& Upper Mesh Dim. & 1.60 $\pm$  0.33    & 4.01 $\pm$ 1.18  &  3384 $\pm$ 903 \\
\hline
\multirow{3}{2.6cm}{Walker2d-v2 \\ (all seeds)$^{**}$ }
& Identity        & 2.10 $\pm$ 0.34   & 4.42 $\pm$ 1.00    &  3743 $\pm$ 1034 \\
& Lower Mesh Dim. & 1.68 $\pm$ 0.70   & 4.19 $\pm$ 1.25    &  3048 $\pm$ 1071 \\
& Upper Mesh Dim. & 1.48 $\pm$ 0.38   & 2.98 $\pm$ 0.86    &  2558 $\pm$ 1373 \\
\hline
\end{tabular}
\caption{\label{tab:mesh_p2_noise} Mesh dimensions and returns for trajectories subject to zero-mean Gaussian noise. Standard deviation of 0.001 and 0.01 was added to all actions and observations respectively. See Sec.~\ref{sec:training} for details.\\
\footnotesize{Because ARS with our chosen hyperparameters does not consistently produce 10 seeds that perform well on the hopper, we instead use madodiv (see the \ref{sec:var}) for the seed policies.  \\
**  See Sec.~\ref{sec:msh}}
}
\end{table}

It's worth noting at this point that in practice the lower mesh dimension seems to work better than the upper one. We found, when computing the mesh dimension by hand (by hand fitting a line to a set of carefully obtained mesh size data), that the hand picked value was generally much closer to the lower mesh dimension, at least for the three systems we studied. Training with the lower mesh dimension also resulted in agents that were more robust and achieved higher reward compared to the upper dimension.  



\begin{table}
\centering
\begin{tabular}{ l|l|l|l|l }

\hline
        Case &  Env.  & Identity      & Lower mesh dim. & Action std  \\ 
\hline
(a) & Cheetah     &          0.24  &           0.05 & .05 \\
 & Hopper      &          0.19  &           0.10 & .05 \\ 
 & Walker      &          0.28  &           0.03 & .15 \\
\hline
\hline
        &    & Identity       & Lower mesh dim. & Observation std  \\ 
\hline
(b) & Cheetah     &          0.20  &           0.02 & .005 \\
 & Hopper      &          0.20  &           0.25 & .02 \\ 
 & Walker      &          0.18  &           0.10 & .03 \\
\hline
\hline
        &    & Identity       & Lower mesh dim. & Magnitude, Rate  \\ 
\hline
(c) & Cheetah     &          0.21  &           0.03 & 3, .2 \\
 & Hopper      &          0.17  &           0.10 & 1, .2 \\ 
 & Walker      &          0.20  &           0.00 & 1, .2 \\
\hline
\end{tabular}
\caption{\label{tab:robust} Failure rates for agents under various noise and push disturbances}
\end{table}


To quantify robust with versus without post-processing, we tested three different cases, adding zero-mean Gaussian noise either (a)~to the actions, (b)~to the observations, or (c)~via an external force disturbance directed at the center of mass of the agents during their rollouts. Table~\ref{tab:mesh_p2_noise} shows the fraction of of runs resulting in a ``failure'', e.g., early termination of a 10,000 time step episode, for example due to tripping and falling. For the push disturbances we have two parameters, the rate of disturbances, and the magnitude of the force applied. At every step we sample uniformly from [0,1], if the result is less than the rate parameter, then a force is applied at that time step. The force is applied at a random angle in the xz plane (each environment is a planar system), with the fixed magnitude from the magnitude parameter. For each type of disturbance, we did a grid search over the parameters and report the parameter for which the identity post processor failed in roughly 20 percent of cases, so that values are close to 0.2, by design. 


% \section{Related Work}
    
%     !! I think related work belongs at the end, we are mostly comparing to our labs work, and the distinction will be much clearer once the reader knows wtf we are talking about. I think I've seen other papers do this...
%     Outside our lab there really has not been that much work analysing the effects of fractal dimension on reinforcement learning agents. This \cite{Kaygisiz2001} work from back in 2001 does something similar to our meshing approach, and they examine the fractal microstructure of their mesh equivalent. But they make no attempt to actually modulate the fractal structure of the resulting policy. 
    
%     I think I need to mostly talking about our own labs work here? How this differs from it perhaps?


%===============================================================================

% \section{Future Work}

% The obvious next step is to use this to recreate some of the earlier work from our lab. Ideally these techniques will allow our mesh based methods to scale to even higher dimensional systems. There are also improvements that can be made to the meshing algorithm, it's unclear if using a more accurate measure of this during training will aid performance in any way.
    


\section{Conclusion}

In this work, we introduced a technique to influence the fractional dimension of the closed-loop dynamics of a system through the use of novel, dimensionality-based modifications to the cost functions for reinforcement learning policies. We demonstrated this technique on several benchmark tasks, and we briefly analyzed a resulting policy to verify the outcome, demonstrating a much smaller mesh dimension without a large loss in reward or function.

\label{sec:conclusion}
% \begin{center}
% \begin{tabu}{ X[1,l] | X[1,l] | X[1,l] | X[1,l] | X[1,l] }
% Environment & $\alpha$ & $\nu$ & $N$ & $b$ \\
%  \hline
%  \text{HalfCheetah}-v2 & 0.02 & 0.025 & 60 & 20 \\
%  \hline
% \end{tabu}
% \end{center}

%/





\subsection*{Hyper Parameters}

\noindent \textbf{ARS:} For all environments $\alpha = .02$, $\sigma = .025$, $N=50$, $b=20$.  \\
\textbf{MeshDim:} f = 1.5, $d_{0}$ = 1e-2



\subsection{Implementation Details}

For performance reasons, the mesh dimension algorithm does not actually create meshes until the mesh size equals the total data size, but rather until the mesh size is 4/5 the total data size. Figure~\ref{fig:mesh_anal} shows a typical mesh curve, and we can see the long tail of values, at the upper left portion of each curve, with mesh sizes close to the maximum value. Not much useful information is gained from this and it wastes time, so we stop early. We do not place the same limitation on the lower size of the mesh, since typically the mesh size hits one much more rapidly. Figure~\ref{fig:mesh_anal} illustrates this, too. In addition, we set a minimum size for $d_{min}=1e-9$ in this work to avoid numerical errors. 

The normalization done during box creation (see Alg.~\ref{algo:createboxmesh}) uses a running mean and standard deviation of all states seen so far during training. These stats are saved and used for evaluation as well. We found that the upper mesh dimension is very sensitive to the normalization used, but that the other metrics where not.

