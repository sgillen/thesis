
Computer simulation has become an indispensable tool for researchers and engineers of robotic systems for design, control, and verification. Recent advances in model free deep reinforcement learning (DRL) have been able to leverage simulation and the continued exponential increase computer resources to solve a variety of challenging control and perception problems. Examples include dexterously manipulating objects with a 24 DOF robotic hand~\cite{openai_learning_2018}, and recent work from the Robotic Systems Lab at ETH Zurich demonstrates rough terrain quadruped robot arguably on part with Boston Dynamics  \cite{anymal2022}. In both cases, training was done in simulation, and then successfully transferred to the real world. 

These simulations have largely been treated as black boxes by DRL algorithms. This is both a strength and a weakness of DRL, and this stems partially from the fact that in commonly used simulators like MuJoco \cite{todorov2012mujoco} or Bullet \cite{coumans2020}, are not differentiable (nor are physical robots, for that matter). That is, we are unable to compute the gradient of the state at time t+1, with the action from time t. Therefore we are also unable to take the gradients of the reward function with respect to controller parameters. Thus, RL must thus rely on various approximations of the true gradients, like finite differences or various policy gradient algorithms, the classic example being REINFORCE \cite{williams1992simple}

However in recent years, fully differentiable physics simulators have started to emerge \cite{hu2020difftaichi} \cite{heiden2021neuralsim} \cite{brax2021github}, which offer analytic gradients using automatic differentiation. These simulators already have a number of interesting applications. For example, it is possible to use data from a physical system as data for a learning algorithm to make simulation to better match. Furthermore the nature of some these simulators allow them to be run on hardware accelerators, which offers some obvious speed advantages for algorithms which can make use of them. And, most relevant to this work, they also provide analytic gradients for any differentiable function of simulation state variable. This means we can use stochastic gradient descent, which is the gold standard for training neural networks, directly using the negative sum of rewards as the loss function. We will call training policies in this way an analytic policy gradient algorithm.

However, in practice, these gradients have proven extremely challenging to use, for a number of reasons. Part of the problem is that to take the gradient through any iterated dynamical system required back propagation through time (BPTT). Long chains of BPTT have long been known to cause exploding or vanishing gradients, which naturally causes to difficulty in learning \cite{279181}. A recent paper by Metz et. al. \cite{metz2021gradients} also offer some exposition on the challenges of using the analytic gradients offered by these new rigid body simulators. They highlight that in addition to problems inherent to BPTT, the naturally chaotic dynamics of many rigid body systems exasperate the problems with diverging gradients significantly.

Another difficulty are severe local maxima. Local maxima are a common problem in all of deep learning, but it is apparent that using APG for reinforcement learning with rigid body systems is especially prone to falling into these extrema. In \cite{metz2021gradients} they also show the reward landscape of the Ant system in Brax. The Ant is a standard benchmark problem in the RL community, it consists of a quadruped robot who's objective is to move forward as fast as possible. They show that reward landscape and reward gradient for this system has extremely high variance, and fraught with local extrema. One may expect that this is due primarily to the fact that the Ant system is relatively high dimensional and subject to contact and frictional forces with the ground. However as we show in figure \ref{fig:reward_landscape}, an Acrobot system, which is a simple two DOF system that is not subject to any contact forces, suffers from many of the same problems. 

% We started with a random neural network policy, and then sampled a random vector from policy space. We then added used this vector to shift the initial policy, and recorded the sum of rewards from the shifted. We use the same initial condition for each trial, the only difference between rollouts is that a very slightly different policy is used. 

\begin{figure}[!htb]
        \centering
        \includegraphics[width=\linewidth]{fig/analytic/reward_landscape2.png}
        \caption{A visualization of the reward landscape for the Acrobot. We started with a random neural network policy, and then sampled a random vector from parameter space. We then added used this vector to shift the initial policy, and record the sum of rewards for a single rollout from the shifted policy . We use the same initial condition for each trial, the only difference between rollouts is that a shifted different policy is used.}
        \label{fig:reward_landscape}
\end{figure}


Given these difficulties, it remains an open question what role, if any, analytic policy gradients have to play in reinforcement learning for robotic control. In this paper we present a novel algorithm, Cross Entropy Analytic Policy Gradients (CE-APG), which we have found is able to outperform both vanilla APG and state of the art DRL algorithms in at least one class of challenging nonlinear control problems. Specifically, we demonstrate that under-actuated planar kinematic chains, like the acrobat or inverted cart pole pendulum can be successfully controlled by this algorithm.

CE-APG uses APG as a local search, combining it with an outer loop cross entropy method to escape from local maxima in the reward landscape. This was inspired the the approach taken by the authors of Tiny Differentiable Simulator \cite{heiden2021neuralsim}, which used an optimization technique called Parallel Basin Hopping combined with a gradient based algorithm for what was essentially system ID.


\section{Background And Related Work}
\subsection{Deep Reinforcement Learning}

Deep Reinforcement Learning has seen a lot of attention and impressive results over the last decade or so, including in the context of continuous control for robotic systems \cite{heess_emergence_2017} \cite{openai_learning_2018} \cite{lee_robust_2019} \cite{siekmann2021blind}. These problems are all high dimensional, nonlinear, underactuated, and they all involve complex contact sequences with the environments, which makes them very challenging for more traditional control design. 

DRL is usually divided into model based and model free control. Model based reinforcement learning learns a model of the system under control, and uses that to do planning, classic dynamic programming is an example, as is PILCO \cite{deisenroth2011pilco}. These approaches are typically much more sample efficient than model free RL, however these methods typically have a hard time scaling to higher dimensional problems, and can require expensive re planning at run time.

Model free RL on the other hand learns a policy directly to maximize a reward function. This implies solving a more difficult optimization problem, but is what has been used to achieve all the results we have discussed in this paper. Examples of model free algorithms include Soft Actor Critic (SAC), Proximal Policy Optimization (PPO), and Twin Delayed Deep Deterministic policy gradient (TD3) \cite{haarnoja2018soft} \cite{schulman2017proximal} \cite{fujimoto2018addressing}.

Although they do not strictly use nueral networks, gradient free methods like Evolutionary Strategies (ES) \cite{salimans2017evolution}, or Augmented Random Search (ARS) \cite{Mania2018} can also be considered modern model free reinforcement learning algorithms.  

Our algorithm can be considered a model free reinforcement learning algorithm, and inherits many of their advantages and disadvantages. For example the algorithm we present should theoretically scale well to high dimensional tasks. However our method currently has sample efficiency on par with other DRL algorithms, and, as it requires a differentiable simulator to function, will require sim2real transfer to be applicable to physical systems. We do note that successful transfer of polices from simulation directly to hardware has been demonstrated many times in the literature \cite{openai_learning_2018} \cite{anymal2022}. 

% Traditional model-based control techniques are still very effective---arguably, Boston Dynamics still represents the state-of-the-art for legged locomotion in robotics, for example. However, these approaches require hundreds of expert person hours to develop each new controller. DRL attempts to automate at least some aspects of this challenging controller development process. There are already examples of learned policies outperforming ones hand-designed by experts~\cite{hwangbo_learning_2019}, and with the ever-continued growth and availability of computational power, there is good reason to believe these le

\subsection{Policy Search in Differentiable Simulations}

Many of the papers which introduce a differentiable simulator also include a basic example using analytic gradients for policy search. Brax \cite{brax2021github} and the unnamed simulator developed by Degrave et. al. \cite{10.3389/fnbot.2019.00006} both implement a basic version of APG. Brax's APG is used to command a fully actuated double pendulum to reach random targets, but is currently unable to solve most of the other problems in their benchmarking suite. Degrave Et. Al. manage to develop a walking gait for a quadruped, though their method requires a fairly significant amount of hand designed components.

In \cite{qiao2021efficient} They introduce a simulator and suggest something called "policy enhancement" whereby they augment a model based RL algorithm with the analytic gradients to control a fully actuated double pendulum.In \cite{pmlr-v139-mora21a} the authors present policy optimization via differentiable simulation. They don't directly use the analytic policy gradient, and instead develop an indirect second order method. They also demonstrate their system on under-actuated systems like the inverted pendulum, the difference is that they are balancing at the stable equilibrium. This is a deceptively difficult problem, however we show that our method works to swing-up and balance the system in their unstable equilibrium, which we would argue is more difficult.

\subsection{Combining Local and Global Search}

As already mentioned, our algorithm was inspired by basin hopping \cite{doi:10.1021/jp970984n} and the extension of parallel basin hopping \cite{doi:10.2514/6.2018-1452}. Tiny Differentiable Simulator \cite{heiden2021neuralsim} uses this method for parameter estimation in their own work to perform system ID. We instead use a cross entropy method, and are obviously tackling a different method.

There are also several methods that combine a zeroth order optimizer with a local gradient based optimizer for robot learning \cite{pourchot2019cemrl} \cite{bharadhwaj2020modelpredictive} \cite{huang2021cemgd} \cite{pourchot2019cemrl}. However none of them are making use of analytic gradients, or doing direct policy optimization.

%\cite{huang2021cemgd} CEM-GD
%     On the face of it, very similar, combining CEM with gradient descent, however they are in the context of trajectory planning, AKA model based RL. Ther
% \cite{bharadhwaj2020modelpredictive} (MPC ... not sure difference between this and CEM-GD)

% What about those that have tried analytic gradeints?



% How does our work fit in? It is similar to model based reinforcement learning (E.G. PILCO \cite{deisenroth2011pilco}) , however we are 

% What about model free?

% In my opinion our algorithms is closer to a model free algorithm, it stands to inherent any advantages of those methods, and can be used to augment them.

% The objective of model free reinforcement learning is to learn a policy directly in order to maximize some reward.. 


% \begin{figure}[!htb]
 
%     \includegraphics[width=.75\linewidth]{fig/analytic/rss_system_diagram.png}
%     \caption{System Diagram \sean{Probably Don't Need this}}
  
% \end{figure}

\subsection{Back Propagation Through Time}



% \begin{equation}
% \frac{dR}{d\theta} = \frac{1}{T}\sum_{t=0}^{T}
% \left[\frac{dr_{t}}{d\theta}\sum_{k=1}^{t} \frac{dr_{t}}{ds_{t}}\left( \prod_{i=k}^{t}\frac{ds_{i}}{ds_{i-1}}\right)\frac{ds_{k}}{d_{\theta}}\right]
% \end{equation}

% \sean{if I want to keep this equation I need to explain it ... }

In order to propagate gradients through an iterated system, we must use a technique called back propagation through time. As we have mentioned, this causes difficulty in exploding or vanishing gradients, especially in long chains of computation. The problem essentially is that the reward at time t depends not only on the state and action at time t-1, but on the state and action from t-2, t-3, back to the initial state, even if system itself is Markovian. For even modest length roll-outs this causes instability in the gradients that make learning with them challenging. 

There are many other contexts that this problem arises, including in natural language processing (NLP). One of the tools used to combat this in the NLP community are specialized recurrent neural networks, which are specifically designed to stop gradients that pass through the network from diverging. In our case we use gated recurrent unit GRU \cite{8053243} as our control policy, we outline this architecture with more detail in the methods section. 

%This lets us avoid using truncated backpropogation through time (TBPTT) which is another tool to diverging gradients. In TBP

% We can also, either instead of or in addition to TBPTT, use specialized recurrent networks. Namely a. or LSTM. These networks were specifically designed with gates that allow for selectively "forgetting" information that flows through them, allowing for longer sequences to be learned from. Although in our case, we cannot replace our entire system with a GRU, only the controller, we have found in practice that using a GRU controller helped immensely in training analytic gradient based algorithms, so much so that we did not need to use truncated back propagation. 

\section{Problem Formulation}

\subsection{Reinforcement Learning}

In reinforcement learning, the goal is to train an agent, acting in an environment, to maximize a scalar reward function. The environment is a discrete time dynamical system described by state $s_{t} \in \mathbb{R}^{n}$ and the current action $a_{t} \in \mathbb{R}^{b}$. An evolution function $f: \mathbb{R}^{n} \times \mathbb{R}^{b} \rightarrow \mathbb{R}^{n}$ takes as input the current state and action, and outputs the state at time t+1:

\begin{equation}
s_{t+1}= f(s_{t},a_{t})
\end{equation} 

The controller is a function parameterized by a vector $\theta \in \mathbb{R}^{\abs{\theta}}$ that maps states to actions $g: \mathbb{R}^{n} \times \mathbb{R}^{\norm{\theta}} \rightarrow \mathbb{R}^{m}$ such that:

\begin{equation}
a_{t} = g(s_{t}, \theta)
\end{equation} 

The goal is to maximize a scalar reward function $r : \mathbb{R}^{n} \times \mathbb{R}^{m} \times \mathbb{R}^{n} \rightarrow \mathbb{R}$. We consider the finite time undiscounted reward case. The objective function then is:

\begin{equation} 
R(\theta) =  \sum_{t=0}^{T}r(s_{t}, a_{t}, s_{t+1}) 
\end{equation}

% \begin{equation}
% \theta^{+} = \theta + \alpha \nabla_{\theta} \frac{1}{M}\sum_{i=0}^{M}\sum_{t=0}^{T}  R_{M}(\theta)
% \end{equation}

% \begin{equation} \argmax_{\theta} \mathop{\mathbb{E}}_{\eta}\left[ \sum_{t=0}^{T}r(s_{t}, a_{t}, s_{t+1}) \right] \end{equation}

% Where $\nu$ is a parameter representing the randomness of the environment or controller. It may come from a randomized initial condition, from sensor noise, or from a stochastic policy. 


\subsection{Kinematic Chains and Simulation}

We consider three systems, a classic cartpole pendulum, a double cartpole pendulum, and an Acrobot \cite{spong_swing_1994}. These are all under-actuated, kinematic chains, and are often used as benchmark problems for both reinforcement learning and nonlinear control. Their dynamics in general can be described with the following:

\begin{equation}
    \textbf{M}(\textbf{q})\Ddot{\textbf{q}} + \textbf{C}(\textbf{q},\dot{\textbf{q}})\dot{\textbf{q}} + T_g(\textbf{q})g = \textbf{B}\textbf{u} \label{manipEqn}
\end{equation}

with M being the inertia matrix, C being the Coriolis matrix, T being a matrix capturing gravitational affects. For these systems u is the torque outputted at each motor, and q is the vector of state variables. In each of these systems, there is a single unstable equilibrium point. Our goal is to swing the system from an initial condition to it's unstable point and the maintain balance there. More details on formulating these environments into an MDP can be found in the appendix. 

It was demonstrated in \cite{Gillen2020CombiningDR} that most DRL struggles with the full version of the Acrobot in particular. It is worth elaborating on that point. Many benchmarking suites (for example, OpenAI's gym \cite{1606.01540}) have underactuated systems like the acrobot or cartpole pendulum, however the tasks are typically either to swing the system up, or to balance it, asking one controller to do both becomes a much more challenging problem. 

%together. In fact the one example of a suite with the swingup and balance task \cite{deepmindcontrolsuite2018} shows that the algorithms they tried on it didn't really work.  

\subsection{The Brax Simulator}

The above systems are simulated using Brax \cite{brax2021github}. Brax is a differentiable physics engine that can simulate systems made up of rigid bodies, joint constraints, and actuators. The simulators primary advantage is that it can run massively parallel simulations very quickly on accelerator hardware, I.E. TPUs and GPUs. By virtue of being written entirely in Jax \cite{jax2018github}, we can also take arbitrary gradients through the simulator using autodiff. 

% However as we've discussed, these analytic gradients suffer from some limitations that have made them difficult to use in practice. They have been found to have huge variance with respect to initial conditions. The appendix of the difftaichi \cite{hu2020difftaichi} paper has some good exposition on the subject. Many systems can have large "flat lands" in the reward space. (!!! offer billiard example here??, als, may move this discussion above when talking abpout gradients), and to large numbers of local minima. Another problem is that these gradients often have discontinuous jumps, imagine dropping a square object on the ground with a reward function equal to the final x position of the square. There will be a discontinuous jump with respect to the initial angle of the sqare (!!! this obviusly needs some work too). Yes another issue arises due to how autodiff handles conditionals. Although the auto-diff framework allows for gradients to propagate through conditionals, the auto-diff is essentially "blind" to these conditionals. For example, let's consider the classic cart-pole pendulum, which receives a reward of one for each step that the pole remains in the top half of the plane. The reward gradient for this system will always be zero. This obviously puts some limitations on the types of rewards suitable for this system. 

% All this is to say, there are problems with the gradients that can make learning hard. 


\section{Methods}

\subsection{Analytic policy gradients}

Stochastic gradient descent and its variants (in our case adam \cite{kingma2017adam}) are the gold standard for training deep neural networks. We seek to train our policy using the gradient of the sum of the reward function for a given episode. 

\begin{equation}
    \label{eq:vapg}
    \theta^{+} = \theta + \alpha \nabla_\theta R(\theta)
\end{equation}

To be more specific, we perform N policy rollouts using the current parameters, take the gradient of the mean of the sum of rewards for each these roll outs, use that gradient to update the current policy, and repeat until convergence. Thus our update step is:

\begin{equation}
    \label{eq:vapg_sum}
    \theta^{+} = \theta + \alpha \nabla_\theta \frac{1}{N}\sum_{i=0}^{N}\sum_{t=0}^{T}r(s_{t}, a_{t}, s_{t+1}) 
\end{equation}

As we've discussed, because we are using a differentiable simulation, the analytic gradient with respect to $\theta$ is available to us.



% \subsection{Reinforcement Learning}

% The goal of reinforcement learning is to train an agent acting in an environment to maximize some reward function. At every timestep $t \in \mathbb{Z}$, the agent receives the current state $s_{t} \in \mathbb{R}^{n}$, uses that to compute an action $a_{t} \in \mathbb{R}^{b}$, and then receives the next state $s_{t+1}$, which is used to calculate a reward $r : \mathbb{R}^{n} \times \mathbb{R}^{m} \times \mathbb{R}^{n} \rightarrow \mathbb{R}$. We consider the finite time undiscounted reward case. The objective then is to find a policy  $\pi_{\theta}: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ such that:

% \begin{equation} \argmax_{\theta} \mathop{\mathbb{E}}_{\eta}\left[ \sum_{t=0}^{T}r(s_{t}, a_{t}, s_{t+1}) \right] \end{equation}
% where $\theta \in \mathbb{R}^{d}$ is a set that parameterizes the policy, and $\eta$ is a parameter representing the randomness in the environment. We will call the sum of rewards obtained during an episode a return. 


\subsection{The Cross Entropy Method}


The Cross Entropy Method (CEM \cite{RUBINSTEIN199789}) is a well established algorithm for importance sampling and optimization. CEM maintains a probability distribution over its decision variables, in this case the decision variables are the parameters for our policy. The most common formulation is to use a normal Distribution, thus we must mantain a vector of means $\mu_{pi}$, and a covariance matrix $\sigma_{\pi}$. $\mathcal{N}(\mu_{\pi}, \sigma_{\pi})$. At each step we sample candidate policies from this distribution, and use the following update rules: 

\begin{equation}
\label{eq:mean}
    \mu_{\pi}^{+} = \frac{1}{K_{e}}\sum_{i=0}^{K_{e}}\mu_{i}
\end{equation}


\begin{equation}
\label{eq:var1}
    \sigma_{\pi}^{2+} = \frac{1}{K_{e}}\sum_{i=0}^{K_{e}}(\mu_{\pi} - \mu_{i})(\mu_{\pi} - \mu_{i})^{T}
\end{equation}

Howeve, the covariance matrix grows quadratically with the number of policy parameters, and neural networks can have thousands of parameters even for small systems. Thus, we make the following simplification to the variance:

\begin{equation}
\label{eq:var}
    \sigma_{\pi}^{2+} = \frac{1}{K_{e}}\sum_{i=0}^{K_{e}}(\mu_{\pi} - \mu_{i})^{2}
\end{equation}

This implicitly ensures that our covariance "matrix" only has entries on the diagonal, and can thus be stored as a vector. This simplification is also made by \cite{pourchot2019cemrl}.


\subsection{Cross Entropy Analytic Policy Gradients}



\begin{algorithm}
\caption{Cross Entropy Analytic Policy Gradients}
\label{algo:ceapg}
\begin{algorithmic}
\Require Policy $\pi$ with trainable parameters $\theta$
\Require Hyper-parameters - $\sigma_{0}$, $K_{a}$, $K_{e}$
\State Sample $\bm{\theta_{c}} = [\theta_{1}, ..., \theta_{n}]$ from $\mathcal{N}(\theta, \sigma^{2})^{K_{a} \times \abs{\theta}}$
\For{$\theta_{i} \text{ in } \bm{\theta_{c}}$}
    \State Run APG with initial policy weights $\theta_{i}$
    \State Collect sum of rewards $R_{i}$ and final policy $\theta^{*}_{i}$. 
\EndFor
\State Sort $\theta^{*}$ values in descending order according to reward
\State $\theta^{+} = \frac{1}{K_{e}}\sum_{i=0}^{K_{e}}\theta^{*}_{i} $
\State $\sigma^{+} = \sqrt{\frac{1}{K_{e}}\sum_{i=0}^{K_{e}}(\theta - \theta^{*}_{i})^{2}}$


%\State $ \theta^{+} = \theta + \frac{\alpha}{n \sigma_{R}}\sum_{i=0}^{n} (R_{i} - R_{i+n})\delta_{i} $ 
\end{algorithmic}
\end{algorithm}


We combine these two algorithms as follows. Start with initial policy weight $\theta$, and an initial parameter variance $\sigma_{0}$. We then generate $K_{a}$ candidate policies by sampling from $\mathcal{N}(\theta, \sigma_{0})$. Using these policies as initial conditions, we run $K_{a}$ analytic policy gradient algorithms in parallel, which gives us new weight vector $ [ \theta_{0} , \theta_{1} ... \theta_{K_{a}} ]$, and the final rewards for these new policy weight, $R_{1}, R_{2} ... R_{K_{a}}$. We then sort the policy weights in descending order based on their associated final return. Finally we select the top $K_{e}$ and use equations \ref{eq:mean} and \ref{eq:var} to update our parameter and variance vector. This is repeated until some stopping criteria, for this paper we simply train for fixed number of steps.

\subsection{Controller Architecture}

As we have already mentioned, we employ a Gated Recurrent Unit (GRU) network as our control policy to help combat the exploding / vanishing gradient problem. For our CE-APG experiments, we used a GRU with two fully connected layers on the output, with ReLU hidden activations and a Tanh non-linearity on the final layer. Network sizes for each experiment can be found in the appendix. We found that by using the GRU we are able to train with episodes lengths of at least 500 steps. 

%\sean{theres a good example in the appendix to the difftaichi paper}.

In addition to this, we use deterministic policies, rather than the stochastic ones usually associated with deep reinforcement learning. Typically in DRL, the policy actually parameters a probability distribution over possible actions. At every time step, one generates a new distribution based on the current state, and then samples from that distribution to select an action. While our algorithm is compatible with stochastic policies of this nature, we instead compute the action directly. We believe this is especially advantageous for systems with unstable and highly sensitive dynamics (like the acrobot, cartpole etc).


\section{Results}

For each environment, we ran trials with 8 random seeds. The seeds affect all the sources of randomness during training, of which there are several. The initial value of the policy parameters, the noise added to the policy at the beginning of each iteration, and the initial condition of the simulator at the beginning of each episode. Figure \ref{fig:reward_curves}. For each of these trials we used the GRU controller architecture discussed above, details on layer sizes etc. can be found in the appendix. We report the resulting rewards obtained at the end of in table \ref{tab:results1}. In addition to our own algorithm, we also run comparisons from PPO, SAC, and Brax's implementation of APG (which has some note-able difference from our own, discussed in the appendix). 


\begin{figure*}[!h]
  \centering
  \begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{fig/analytic/cartpole_reward_curve.png}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\linewidth}
  \includegraphics[width=\linewidth]{fig/analytic/acrobot_reward_curve.png}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\linewidth}
      \includegraphics[width=\linewidth]{fig/analytic/double_cartpole_reward_curve.png}
  \end{subfigure}
  \caption{Reward curves for CE-APG}
  \label{fig:reward_curves}
\end{figure*}

\begin{table}[!htb]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|l|l|l|l|}
\hline 
Environment   & Pendulum              & Acrobot               & Double Pendulum \\ \hline \hline
CE-APG (ours)    &   -355$\pm$ 10  &   3053 $\pm$ 458       &   4295 $\pm$ 75\\  \hline
PPO               &    -464 $\pm$ 223 &    405 $\pm$ 927       &   4249 $\pm$ 549         \\  \hline
SAC               &    -2274  $\pm$ 1000    &    359 $\pm$ 159      &   -3.9e5 $\pm$ 4.9e6        \\ \hline
Brax Apg        &    -3485   $\pm$ 819    &     1949 $\pm$ 814       &   -1982 $\pm$ 4056     \\ 

\hline
\end{tabular}
\caption{Results of the training on our test environments, we report the mean and standard deviation of rewards obtained from training each algorithm with 8 random seeds}
\label{tab:results1}
\end{table}

We can see that across all three environments, our method outperforms the other benchmark algorithms. On the double inverted pendulum we get the same final reward as PPO, exceed it slightly on the cartpole, and get significantly higher reward on the acrobot. In fact for the cart-pole in particular our algorithm significantly outperforms the others. It is worth putting this in context, as it is difficult to understand what a reward of 2000 vs. 3000 really means.

To do this we perform roll-outs with the best performing seeds for both  CE-APG and the best performing benchmark, which was Brax's APG implementation. We perform 10 rollouts with these top performers. For completeness we report the mean and standard deviation of the resulting rewards, CE-APG: 3507 $\pm$ 5, Brax's APG: 2778 $\pm$ 89. However it is much more revealing to visualize one of these roll outs, which we do in figure \ref{fig:rollouts}. We can see that despite comparable total reward, APG does not actually stabilize the system at the equilibrium, likely because it has become stuck in a maxima of the reward landscape. By contrast, our algorithm, augmented by the cross entropy method to avoid such local maxima, manages to find a policy that does stabilize our system. Unsurprisingly given the rewards from table \ref{tab:results1} none of the other systems manage to balance the acrobat either. Of course such stabilization is exactly what we as humans had in mind when defining the environment. 

\begin{figure}[!htb]
        \centering
        \includegraphics[width=\linewidth]{fig/analytic/rollouts.png}
        \caption{Best performing seeds for CE-APG vs APG on the Acrobot. Despite comparable total reward, APG does not actually stabilize the system at the equilibrium, likely because it has become stuck in a maxima of the reward landscape. By contrast our algorithm, augmented by the cross entropy method to avoid such local maxima, manages to find a policy that does stabalize our system}
        \label{fig:rollouts}
\end{figure}
% \begin{figure}[!htb]
 
%     \includegraphics[width=.6\linewidth]{fig/analytic/ce-apg_acrobot.png}
%     \caption{}
  
% \end{figure}

% \begin{figure}[!htb]
 
%     \includegraphics[width=.6\linewidth]{fig/analytic/ppo_acrobot.png}
%     \caption{}
  
% \end{figure}




% What about the double inverted pendulum? which included some reward shaping? Well the story is largely the same, let's look at figure 

% \begin{figure}[!htb]
% \captionsetup[subfigure]{labelformat=empty}
%         \centering
%         \subfloat[]{\label{sublable1}\includegraphics[width=0.6\linewidth]{fig/analytic/ce-apg_acrobot.png}} \\
%         \subfloat[]{\label{sublable2}\includegraphics[width=0.6\linewidth]{fig/analytic/ppo_acrobot.png}}
%         \caption{Best performing seeds for CE-APG vs PPO on the Acrobot. As we can see CE-APG stabalizes the system at the equilibrium, whereas PPO does not}
% \end{figure}





\section{Ablation Studies}

In addition to the comparisons done above, we also conducted several ablation studies, this isolates the effect of our implementation of APG from the results presented above. We conducted experiments using only the gradient free CEM method, with no analytic policy gradients being used. And also compared to our implementation of APG, which we are calling PAPG for parallel APG. This essentially means that we run APG N times in parallel, and picking the best result to return. In both cases we still used the GRU controller architecture as before. In the case of CEM we used 2000 iterations for every environment, which we found to be well past the point of convergence. For APG we used 200*100 iterations, which results in the same number of environment interactions to CE-APG. The results are shown in table \ref{tab:ablation}.

\begin{table}[!htb]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|l|l|l|l|}
\hline 
Environment         & Pendulum            & Acrobot & Double Pendulum \\ \hline \hline
CE-APG (ours)    &   -355$\pm$ 10         &   3053 $\pm$ 458       &   4295 $\pm$ 75\\  \hline
PAPG             &    -1034 $\pm$ 208     &   2456 $\pm$ 647       &  2335 $\pm$ 431  \\ \hline
CEM              &    -3019 $\pm$ 559     &   626 $\pm$ 160        &  -2.7e5 $\pm$ 7.6e5  \\ \hline

\hline
\end{tabular}
\caption{Results of our ablation studies. We report the mean and standard deviation of rewards obtained from training each algorithm with 8 random seeds}
\label{tab:ablation}
\end{table}


As we can see, the performance of either method individually is generally poor, though we do see that PAPG does about as well as the best agent from the APG results reported in table \ref{tab:results1}, however when we perform rollouts the resulting policies exhibit the same behavior shown in figure \ref{fig:rollouts}, that is they continuously spin, spending as much time as they can near the goal state, but never settling there.


\section{Discussion and future work}

    We have shown that analytic policy gradients can be leveraged effectively for at least one class of system, nonlinear underacted systems with unstable target states. This is a limited scope of problems, but it is interesting because other modern DRL algorithms struggle with such systems. However, we are as of yet unable to get our algorithm to perform well in contact rich environments, likely because the contacts introduce huge variance into the gradients. As of writing, there is active work in the community to make Brax friendlier to analytic gradient based algorithms, in particular adding soft contacts, which may very well help a lot. 
    
    In addition, the sample complexity of our algorithm is comparable to other on policy DRL algorithm, and is behind what we might expect from off policy algorithms. However there are many algorithmic improvements could be made, for example importance sampling has been found to improve the sample efficiency of CEM by up to a factor of 10. We are currently unable to effectively implement this due technical limitations in the way we implement parallelization. 
\section{Conclusion} 
\label{sec:conclusion}

In conclusion, we presented Cross Entropy Analytic Policy Gradients, an algorithm that can exploit analytic gradients. We covered some relevant background, introduced our method, and placed it in the broader context. We then presented our algorithm and the environments we used for testing. We presented results of our algorithm compared to state of the art baselines, and performed ablation studies. We then contextualized the rewards obtained on Acrobat in particular. This demonstrated that our algorithm was able to successfully stabilize the system, whereas the baseline algorithms where not. We think this algorithm shows that analytic policy gradients can be leveraged productively in at least context.
%\section*{Acknowledgments}



\section{Appendix}

\subsection{Implementation Details}
There are some notable differences between our implementation of APG (which we call PAGP for parallel apg) and the implementation of APG provided Brax. First, we use different controller architectures, our method uses a deterministic GRU, and theirs  uses a stochastic multi layer perceptron. Furthermore the parellization characteristics are quite different, the Brax implementation of APG was designed for use with a TPU, and thus performs hundreds or thousands of rollouts in parallel for every update step. We note that we found the performance of CE-APG to be significantly faster on CPU compared to GPU/TPU.

\subsection{Network Architecture}

Each policy consists of GRU cell with two fully connected layers on the output. The fully connection network has ReLU hidden activations and a tanh non-linearity on the final layer.


\begin{center}
\begin{tabu}{ l | l | l }
Environment & GRU Size & FC size \\
 \hline
 Cartpole Pendulum & 4  &  4x16x16x1\\ 
 Acrobot & 4  &  4x16x16x1  \\ 
 Cartpole Double Pendulum & 6 & 6x32x32x1 \\ 
\end{tabu}
\end{center}

\subsection{Hyper Paramaters}

In all cases we selected the best hyper parameters we could find using a manual search, using a coarse parameter sweep as a starting point. 


\begin{center}
\begin{tabu}{ X[2,l] | X[1,l] }
CE-APG Hyperparameter & Value \\
 \hline
 APG Epochs & 100 \\ 
 Total Epochs & 200  \\ 
 Total Env Interactions &  1e7 \\
 initial std & 0.05 \\
 learning rate & 1e-3 $\rightarrow$ 1e-6 \\
 batch size (N) & 4 \\
 $K_{a}$ & 24 \\
 $K_{e}$ & 8 \\
\end{tabu}
\end{center}


\begin{center}
\begin{tabu}{ X[2,l] | X[1,l] }
PPO Hyperparameter & Value \\
 \hline
 Total Timesteps & 8e7 \\ 
 Minibatch Size & 32 \\
 Batch Size & 256 \\
 Unroll Length  & 50 \\
 N Update Epochs & 8 \\
 Discounting & 0.99 \\
 Learning Rate & 3e-4 \\
 Entropy Cost & 1e-3 \\
 N Envs & 512 \\
\end{tabu}
\end{center}


\begin{center}
\begin{tabu}{ X[2,l] | X[1,l] }
SAC Hyperparameter  & Value \\
 \hline
 Total Timesteps & 2e6 \\ 
 Discounting & .95 \\
 Learning Rate & 1e-3 \\
 N Envs & 64 \\
 Batch Size & 128 
\end{tabu}
\end{center}



\begin{center}
\begin{tabu}{ X[2,l] | X[1,l] }
Brax-APG Hyperparameter & Value \\
 \hline
 Total Env interactions & 1e7 \\ 
 N Environments & 24  \\
 learning rate & 5e-4 \\
\end{tabu}
\end{center}



\section{Environment Details}

\subsection{Acrobat}
The Acrobat is kept relatively simple, the only state variables are the joint angles and velocities, the reward is just the negative squared distance between the goal state and the current state: 

\[r_{a} = -\phi_{1}^{2} - \phi_{2}^{2} \]

With $\phi_{1} = \phi_{2} = 0$ being the upright balanced position with both links straight up.

\subsection{Cartpole Pendulum}
The cart-pole pendulum is also kept simple, the states are simply the joint angles and velocities, and the reward function is simply the negative square of the pendulum angle, with $\phi_{1}$ = 0 corresponding the the upright position.

\subsection{Inverted Double Cartpole Pendulum}

The double cartpole pendulum is modified from Brax's existing benchmark environments, the only difference is that the initial condition is rotated 180 degrees from the upright. These environments use some reward shaping, adding an an alive bonus as well as some feature extraction. rather than directly feeding in joint angles, both the reward and state variables are fed in as the x,y coordinate for the end of each link. The reward function for the environment is:

\begin{equation} 
r_{dp} = r_{alive} - r_{distance} - r_{velocity}
\end{equation}

Where 

\begin{equation}
r_{alive} = 10 
\end{equation}

\begin{equation}
r_{distance} = 0.05x^{2} + (y - y_{des})^{2}
\end{equation}

\begin{equation}
r_{velocity} = \dot \phi_{1} + \dot \phi_{2}
\end{equation}

and $y_{des}$ co-responds to the height of the second link when in the upright position. Put another way, the reward function is an alive bonus minus the euclidean distance between the end of the second link and the goal state. 

% \begin{equation}
% y = \sin(\phi_{1}) + \sin(\phi_{2})
% \end{equation}

% \begin{equation}
% x = x_{cart} + \cos(\phi_{2}) + \cos(\phi_{2}) 
% \end{equation}



% \[ r_{p} = r_{A} -\sqrt{\sin(\phi_{1})^{2} + \cos(\phi_{1})^{2}}\]

% Or in other words an alive bonus with a penalty for the euclidean distance of the tip of the pendulum to the goal state. The double pendulum likewise has the following reward: 

% \begin{equation} r_{dp} = r_{A} -(\sqrt{(\sin(\phi_{1}) + \sin(\phi_{2}))^{2} + (\cos(\phi_{1}) + \cos(\phi_{2}))^{2}}
% \end{equation}

